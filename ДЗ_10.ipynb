{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "xllyjmbiz6g376jww7cni6",
    "execution_id": "1a616eaa-059e-47e8-afaf-cd8572b78895",
    "id": "KF8DJ2W3qCmr"
   },
   "source": [
    "# Домашнее задание №10.\n",
    "# Машинный перевод. Модель seq2seq и механизм внимания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "2g6tw3lwy753dh7s43ag2s",
    "execution_id": "0a12635b-a79c-4790-94e0-97a01d971529",
    "id": "xnCeUPgdqSSg"
   },
   "source": [
    "## Задание\n",
    "\n",
    "Разобраться с моделькой перевода как она устроена запустить для перевода с русского на английский (при желании можно взять другие пары языков)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellId": "h0tzp10h4idvga383b0ws",
    "id": "578Y-BW9p8xt"
   },
   "outputs": [],
   "source": [
    "#!g2.mig\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "n026srfgufliy2lr7ymvs",
    "execution_id": "5de8b3fa-692d-4a42-96fb-771dc9fa5ae8",
    "id": "TquFrReZrCy8"
   },
   "source": [
    "## Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellId": "zeyh0klmrd8q6aucdz6nqm",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "duBpVcyMqvJp",
    "outputId": "5588e705-b1c3-40b0-df8e-c647e92b554e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-02-20 05:33:06--  http://www.manythings.org/anki/rus-eng.zip\n",
      "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
      "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15011848 (14M) [application/zip]\n",
      "Saving to: ‘rus-eng.zip’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  0%  138K 1m46s\n",
      "    50K .......... .......... .......... .......... ..........  0%  277K 79s\n",
      "   100K .......... .......... .......... .......... ..........  1%  324M 52s\n",
      "   150K .......... .......... .......... .......... ..........  1%  152M 39s\n",
      "   200K .......... .......... .......... .......... ..........  1%  277K 42s\n",
      "   250K .......... .......... .......... .......... ..........  2%  263M 35s\n",
      "   300K .......... .......... .......... .......... ..........  2%  337M 30s\n",
      "   350K .......... .......... .......... .......... ..........  2%  279M 26s\n",
      "   400K .......... .......... .......... .......... ..........  3%  278K 29s\n",
      "   450K .......... .......... .......... .......... ..........  3%  104M 26s\n",
      "   500K .......... .......... .......... .......... ..........  3%  203M 23s\n",
      "   550K .......... .......... .......... .......... ..........  4%  235M 21s\n",
      "   600K .......... .......... .......... .......... ..........  4%  641M 20s\n",
      "   650K .......... .......... .......... .......... ..........  4%  250M 18s\n",
      "   700K .......... .......... .......... .......... ..........  5%  188M 17s\n",
      "   750K .......... .......... .......... .......... ..........  5%  509M 16s\n",
      "   800K .......... .......... .......... .......... ..........  5%  615M 15s\n",
      "   850K .......... .......... .......... .......... ..........  6%  279K 17s\n",
      "   900K .......... .......... .......... .......... ..........  6%  324M 16s\n",
      "   950K .......... .......... .......... .......... ..........  6%  162M 15s\n",
      "  1000K .......... .......... .......... .......... ..........  7%  247M 14s\n",
      "  1050K .......... .......... .......... .......... ..........  7%  204M 13s\n",
      "  1100K .......... .......... .......... .......... ..........  7%  383M 13s\n",
      "  1150K .......... .......... .......... .......... ..........  8%  572M 12s\n",
      "  1200K .......... .......... .......... .......... ..........  8%  239M 12s\n",
      "  1250K .......... .......... .......... .......... ..........  8%  501M 11s\n",
      "  1300K .......... .......... .......... .......... ..........  9%  431M 11s\n",
      "  1350K .......... .......... .......... .......... ..........  9%  494M 10s\n",
      "  1400K .......... .......... .......... .......... ..........  9%  593M 10s\n",
      "  1450K .......... .......... .......... .......... .......... 10%  126M 10s\n",
      "  1500K .......... .......... .......... .......... .......... 10%  438M 9s\n",
      "  1550K .......... .......... .......... .......... .......... 10%  437M 9s\n",
      "  1600K .......... .......... .......... .......... .......... 11%  576M 9s\n",
      "  1650K .......... .......... .......... .......... .......... 11%  517M 8s\n",
      "  1700K .......... .......... .......... .......... .......... 11%  522M 8s\n",
      "  1750K .......... .......... .......... .......... .......... 12%  280K 9s\n",
      "  1800K .......... .......... .......... .......... .......... 12%  255M 9s\n",
      "  1850K .......... .......... .......... .......... .......... 12%  247M 9s\n",
      "  1900K .......... .......... .......... .......... .......... 13%  332M 8s\n",
      "  1950K .......... .......... .......... .......... .......... 13%  269M 8s\n",
      "  2000K .......... .......... .......... .......... .......... 13%  303M 8s\n",
      "  2050K .......... .......... .......... .......... .......... 14%  475M 8s\n",
      "  2100K .......... .......... .......... .......... .......... 14%  479M 7s\n",
      "  2150K .......... .......... .......... .......... .......... 15%  500M 7s\n",
      "  2200K .......... .......... .......... .......... .......... 15%  447M 7s\n",
      "  2250K .......... .......... .......... .......... .......... 15%  531M 7s\n",
      "  2300K .......... .......... .......... .......... .......... 16%  593M 7s\n",
      "  2350K .......... .......... .......... .......... .......... 16%  446M 6s\n",
      "  2400K .......... .......... .......... .......... .......... 16%  566M 6s\n",
      "  2450K .......... .......... .......... .......... .......... 17% 57.5M 6s\n",
      "  2500K .......... .......... .......... .......... .......... 17%  346M 6s\n",
      "  2550K .......... .......... .......... .......... .......... 17%  508M 6s\n",
      "  2600K .......... .......... .......... .......... .......... 18%  448M 6s\n",
      "  2650K .......... .......... .......... .......... .......... 18%  476M 6s\n",
      "  2700K .......... .......... .......... .......... .......... 18%  406M 5s\n",
      "  2750K .......... .......... .......... .......... .......... 19%  473M 5s\n",
      "  2800K .......... .......... .......... .......... .......... 19%  471M 5s\n",
      "  2850K .......... .......... .......... .......... .......... 19%  465M 5s\n",
      "  2900K .......... .......... .......... .......... .......... 20%  441M 5s\n",
      "  2950K .......... .......... .......... .......... .......... 20%  521M 5s\n",
      "  3000K .......... .......... .......... .......... .......... 20%  579M 5s\n",
      "  3050K .......... .......... .......... .......... .......... 21%  431M 5s\n",
      "  3100K .......... .......... .......... .......... .......... 21%  448M 5s\n",
      "  3150K .......... .......... .......... .......... .......... 21%  593M 5s\n",
      "  3200K .......... .......... .......... .......... .......... 22%  562M 4s\n",
      "  3250K .......... .......... .......... .......... .......... 22%  577M 4s\n",
      "  3300K .......... .......... .......... .......... .......... 22%  479M 4s\n",
      "  3350K .......... .......... .......... .......... .......... 23%  573M 4s\n",
      "  3400K .......... .......... .......... .......... .......... 23%  574M 4s\n",
      "  3450K .......... .......... .......... .......... .......... 23%  578M 4s\n",
      "  3500K .......... .......... .......... .......... .......... 24%  282K 5s\n",
      "  3550K .......... .......... .......... .......... .......... 24%  133M 4s\n",
      "  3600K .......... .......... .......... .......... .......... 24%  265M 4s\n",
      "  3650K .......... .......... .......... .......... .......... 25%  512M 4s\n",
      "  3700K .......... .......... .......... .......... .......... 25%  452M 4s\n",
      "  3750K .......... .......... .......... .......... .......... 25%  540M 4s\n",
      "  3800K .......... .......... .......... .......... .......... 26%  527M 4s\n",
      "  3850K .......... .......... .......... .......... .......... 26%  583M 4s\n",
      "  3900K .......... .......... .......... .......... .......... 26%  599M 4s\n",
      "  3950K .......... .......... .......... .......... .......... 27%  565M 4s\n",
      "  4000K .......... .......... .......... .......... .......... 27%  445M 4s\n",
      "  4050K .......... .......... .......... .......... .......... 27%  450M 4s\n",
      "  4100K .......... .......... .......... .......... .......... 28%  472M 4s\n",
      "  4150K .......... .......... .......... .......... .......... 28%  481M 4s\n",
      "  4200K .......... .......... .......... .......... .......... 28%  484M 4s\n",
      "  4250K .......... .......... .......... .......... .......... 29%  558M 3s\n",
      "  4300K .......... .......... .......... .......... .......... 29%  557M 3s\n",
      "  4350K .......... .......... .......... .......... .......... 30%  578M 3s\n",
      "  4400K .......... .......... .......... .......... .......... 30%  492M 3s\n",
      "  4450K .......... .......... .......... .......... .......... 30% 53.4M 3s\n",
      "  4500K .......... .......... .......... .......... .......... 31%  238M 3s\n",
      "  4550K .......... .......... .......... .......... .......... 31%  498M 3s\n",
      "  4600K .......... .......... .......... .......... .......... 31%  395M 3s\n",
      "  4650K .......... .......... .......... .......... .......... 32%  417M 3s\n",
      "  4700K .......... .......... .......... .......... .......... 32%  442M 3s\n",
      "  4750K .......... .......... .......... .......... .......... 32%  482M 3s\n",
      "  4800K .......... .......... .......... .......... .......... 33%  506M 3s\n",
      "  4850K .......... .......... .......... .......... .......... 33%  446M 3s\n",
      "  4900K .......... .......... .......... .......... .......... 33%  564M 3s\n",
      "  4950K .......... .......... .......... .......... .......... 34%  549M 3s\n",
      "  5000K .......... .......... .......... .......... .......... 34%  475M 3s\n",
      "  5050K .......... .......... .......... .......... .......... 34%  432M 3s\n",
      "  5100K .......... .......... .......... .......... .......... 35%  616M 3s\n",
      "  5150K .......... .......... .......... .......... .......... 35%  592M 3s\n",
      "  5200K .......... .......... .......... .......... .......... 35%  591M 3s\n",
      "  5250K .......... .......... .......... .......... .......... 36%  490M 3s\n",
      "  5300K .......... .......... .......... .......... .......... 36%  191M 3s\n",
      "  5350K .......... .......... .......... .......... .......... 36%  393M 2s\n",
      "  5400K .......... .......... .......... .......... .......... 37%  474M 2s\n",
      "  5450K .......... .......... .......... .......... .......... 37%  438M 2s\n",
      "  5500K .......... .......... .......... .......... .......... 37%  433M 2s\n",
      "  5550K .......... .......... .......... .......... .......... 38%  431M 2s\n",
      "  5600K .......... .......... .......... .......... .......... 38%  616M 2s\n",
      "  5650K .......... .......... .......... .......... .......... 38%  454M 2s\n",
      "  5700K .......... .......... .......... .......... .......... 39%  497M 2s\n",
      "  5750K .......... .......... .......... .......... .......... 39%  413M 2s\n",
      "  5800K .......... .......... .......... .......... .......... 39%  480M 2s\n",
      "  5850K .......... .......... .......... .......... .......... 40%  580M 2s\n",
      "  5900K .......... .......... .......... .......... .......... 40%  610M 2s\n",
      "  5950K .......... .......... .......... .......... .......... 40%  500M 2s\n",
      "  6000K .......... .......... .......... .......... .......... 41%  593M 2s\n",
      "  6050K .......... .......... .......... .......... .......... 41%  582M 2s\n",
      "  6100K .......... .......... .......... .......... .......... 41%  607M 2s\n",
      "  6150K .......... .......... .......... .......... .......... 42%  524M 2s\n",
      "  6200K .......... .......... .......... .......... .......... 42%  592M 2s\n",
      "  6250K .......... .......... .......... .......... .......... 42%  286K 2s\n",
      "  6300K .......... .......... .......... .......... .......... 43% 81.7M 2s\n",
      "  6350K .......... .......... .......... .......... .......... 43%  302M 2s\n",
      "  6400K .......... .......... .......... .......... .......... 43%  322M 2s\n",
      "  6450K .......... .......... .......... .......... .......... 44%  458M 2s\n",
      "  6500K .......... .......... .......... .......... .......... 44%  473M 2s\n",
      "  6550K .......... .......... .......... .......... .......... 45%  432M 2s\n",
      "  6600K .......... .......... .......... .......... .......... 45%  566M 2s\n",
      "  6650K .......... .......... .......... .......... .......... 45%  420M 2s\n",
      "  6700K .......... .......... .......... .......... .......... 46%  490M 2s\n",
      "  6750K .......... .......... .......... .......... .......... 46%  117M 2s\n",
      "  6800K .......... .......... .......... .......... .......... 46%  472M 2s\n",
      "  6850K .......... .......... .......... .......... .......... 47%  414M 2s\n",
      "  6900K .......... .......... .......... .......... .......... 47%  581M 2s\n",
      "  6950K .......... .......... .......... .......... .......... 47%  587M 2s\n",
      "  7000K .......... .......... .......... .......... .......... 48%  138M 2s\n",
      "  7050K .......... .......... .......... .......... .......... 48%  485M 2s\n",
      "  7100K .......... .......... .......... .......... .......... 48%  400M 2s\n",
      "  7150K .......... .......... .......... .......... .......... 49%  407M 2s\n",
      "  7200K .......... .......... .......... .......... .......... 49%  505M 2s\n",
      "  7250K .......... .......... .......... .......... .......... 49%  585M 2s\n",
      "  7300K .......... .......... .......... .......... .......... 50%  518M 2s\n",
      "  7350K .......... .......... .......... .......... .......... 50%  486M 2s\n",
      "  7400K .......... .......... .......... .......... .......... 50%  240M 2s\n",
      "  7450K .......... .......... .......... .......... .......... 51%  124M 2s\n",
      "  7500K .......... .......... .......... .......... .......... 51%  565M 2s\n",
      "  7550K .......... .......... .......... .......... .......... 51%  218M 2s\n",
      "  7600K .......... .......... .......... .......... .......... 52%  465M 1s\n",
      "  7650K .......... .......... .......... .......... .......... 52%  484M 1s\n",
      "  7700K .......... .......... .......... .......... .......... 52%  188M 1s\n",
      "  7750K .......... .......... .......... .......... .......... 53%  539M 1s\n",
      "  7800K .......... .......... .......... .......... .......... 53%  523M 1s\n",
      "  7850K .......... .......... .......... .......... .......... 53%  417M 1s\n",
      "  7900K .......... .......... .......... .......... .......... 54%  623M 1s\n",
      "  7950K .......... .......... .......... .......... .......... 54% 99.4M 1s\n",
      "  8000K .......... .......... .......... .......... .......... 54%  331M 1s\n",
      "  8050K .......... .......... .......... .......... .......... 55%  430M 1s\n",
      "  8100K .......... .......... .......... .......... .......... 55%  432M 1s\n",
      "  8150K .......... .......... .......... .......... .......... 55%  471M 1s\n",
      "  8200K .......... .......... .......... .......... .......... 56%  428M 1s\n",
      "  8250K .......... .......... .......... .......... .......... 56%  596M 1s\n",
      "  8300K .......... .......... .......... .......... .......... 56%  577M 1s\n",
      "  8350K .......... .......... .......... .......... .......... 57%  568M 1s\n",
      "  8400K .......... .......... .......... .......... .......... 57%  492M 1s\n",
      "  8450K .......... .......... .......... .......... .......... 57%  143M 1s\n",
      "  8500K .......... .......... .......... .......... .......... 58%  444M 1s\n",
      "  8550K .......... .......... .......... .......... .......... 58%  572M 1s\n",
      "  8600K .......... .......... .......... .......... .......... 59%  388M 1s\n",
      "  8650K .......... .......... .......... .......... .......... 59%  350M 1s\n",
      "  8700K .......... .......... .......... .......... .......... 59%  529M 1s\n",
      "  8750K .......... .......... .......... .......... .......... 60%  504M 1s\n",
      "  8800K .......... .......... .......... .......... .......... 60%  470M 1s\n",
      "  8850K .......... .......... .......... .......... .......... 60%  449M 1s\n",
      "  8900K .......... .......... .......... .......... .......... 61%  583M 1s\n",
      "  8950K .......... .......... .......... .......... .......... 61%  582M 1s\n",
      "  9000K .......... .......... .......... .......... .......... 61%  513M 1s\n",
      "  9050K .......... .......... .......... .......... .......... 62%  289K 1s\n",
      "  9100K .......... .......... .......... .......... .......... 62%  208M 1s\n",
      "  9150K .......... .......... .......... .......... .......... 62%  186M 1s\n",
      "  9200K .......... .......... .......... .......... .......... 63%  330M 1s\n",
      "  9250K .......... .......... .......... .......... .......... 63%  227M 1s\n",
      "  9300K .......... .......... .......... .......... .......... 63%  256M 1s\n",
      "  9350K .......... .......... .......... .......... .......... 64%  474M 1s\n",
      "  9400K .......... .......... .......... .......... .......... 64%  576M 1s\n",
      "  9450K .......... .......... .......... .......... .......... 64%  447M 1s\n",
      "  9500K .......... .......... .......... .......... .......... 65%  585M 1s\n",
      "  9550K .......... .......... .......... .......... .......... 65%  620M 1s\n",
      "  9600K .......... .......... .......... .......... .......... 65%  586M 1s\n",
      "  9650K .......... .......... .......... .......... .......... 66% 85.2M 1s\n",
      "  9700K .......... .......... .......... .......... .......... 66%  440M 1s\n",
      "  9750K .......... .......... .......... .......... .......... 66%  505M 1s\n",
      "  9800K .......... .......... .......... .......... .......... 67%  450M 1s\n",
      "  9850K .......... .......... .......... .......... .......... 67%  503M 1s\n",
      "  9900K .......... .......... .......... .......... .......... 67%  528M 1s\n",
      "  9950K .......... .......... .......... .......... .......... 68%  167M 1s\n",
      " 10000K .......... .......... .......... .......... .......... 68%  505M 1s\n",
      " 10050K .......... .......... .......... .......... .......... 68%  576M 1s\n",
      " 10100K .......... .......... .......... .......... .......... 69%  534M 1s\n",
      " 10150K .......... .......... .......... .......... .......... 69%  475M 1s\n",
      " 10200K .......... .......... .......... .......... .......... 69%  521M 1s\n",
      " 10250K .......... .......... .......... .......... .......... 70%  616M 1s\n",
      " 10300K .......... .......... .......... .......... .......... 70% 55.6M 1s\n",
      " 10350K .......... .......... .......... .......... .......... 70%  505M 1s\n",
      " 10400K .......... .......... .......... .......... .......... 71%  148M 1s\n",
      " 10450K .......... .......... .......... .......... .......... 71%  370M 1s\n",
      " 10500K .......... .......... .......... .......... .......... 71%  379M 1s\n",
      " 10550K .......... .......... .......... .......... .......... 72%  449M 1s\n",
      " 10600K .......... .......... .......... .......... .......... 72%  589M 1s\n",
      " 10650K .......... .......... .......... .......... .......... 72%  509M 1s\n",
      " 10700K .......... .......... .......... .......... .......... 73%  462M 1s\n",
      " 10750K .......... .......... .......... .......... .......... 73%  422M 1s\n",
      " 10800K .......... .......... .......... .......... .......... 74%  583M 1s\n",
      " 10850K .......... .......... .......... .......... .......... 74%  488M 1s\n",
      " 10900K .......... .......... .......... .......... .......... 74%  580M 1s\n",
      " 10950K .......... .......... .......... .......... .......... 75%  277M 1s\n",
      " 11000K .......... .......... .......... .......... .......... 75%  535M 1s\n",
      " 11050K .......... .......... .......... .......... .......... 75%  217M 1s\n",
      " 11100K .......... .......... .......... .......... .......... 76%  518M 1s\n",
      " 11150K .......... .......... .......... .......... .......... 76%  538M 1s\n",
      " 11200K .......... .......... .......... .......... .......... 76%  120M 1s\n",
      " 11250K .......... .......... .......... .......... .......... 77%  475M 1s\n",
      " 11300K .......... .......... .......... .......... .......... 77%  407M 1s\n",
      " 11350K .......... .......... .......... .......... .......... 77%  477M 1s\n",
      " 11400K .......... .......... .......... .......... .......... 78%  410M 1s\n",
      " 11450K .......... .......... .......... .......... .......... 78%  576M 0s\n",
      " 11500K .......... .......... .......... .......... .......... 78%  561M 0s\n",
      " 11550K .......... .......... .......... .......... .......... 79%  235M 0s\n",
      " 11600K .......... .......... .......... .......... .......... 79%  477M 0s\n",
      " 11650K .......... .......... .......... .......... .......... 79%  499M 0s\n",
      " 11700K .......... .......... .......... .......... .......... 80%  479M 0s\n",
      " 11750K .......... .......... .......... .......... .......... 80%  521M 0s\n",
      " 11800K .......... .......... .......... .......... .......... 80%  376M 0s\n",
      " 11850K .......... .......... .......... .......... .......... 81%  528M 0s\n",
      " 11900K .......... .......... .......... .......... .......... 81%  474M 0s\n",
      " 11950K .......... .......... .......... .......... .......... 81%  290K 0s\n",
      " 12000K .......... .......... .......... .......... .......... 82%  104M 0s\n",
      " 12050K .......... .......... .......... .......... .......... 82%  105M 0s\n",
      " 12100K .......... .......... .......... .......... .......... 82%  573M 0s\n",
      " 12150K .......... .......... .......... .......... .......... 83%  306M 0s\n",
      " 12200K .......... .......... .......... .......... .......... 83%  557M 0s\n",
      " 12250K .......... .......... .......... .......... .......... 83%  492M 0s\n",
      " 12300K .......... .......... .......... .......... .......... 84%  503M 0s\n",
      " 12350K .......... .......... .......... .......... .......... 84%  117M 0s\n",
      " 12400K .......... .......... .......... .......... .......... 84%  460M 0s\n",
      " 12450K .......... .......... .......... .......... .......... 85%  442M 0s\n",
      " 12500K .......... .......... .......... .......... .......... 85%  481M 0s\n",
      " 12550K .......... .......... .......... .......... .......... 85%  550M 0s\n",
      " 12600K .......... .......... .......... .......... .......... 86%  530M 0s\n",
      " 12650K .......... .......... .......... .......... .......... 86%  210M 0s\n",
      " 12700K .......... .......... .......... .......... .......... 86%  340M 0s\n",
      " 12750K .......... .......... .......... .......... .......... 87%  432M 0s\n",
      " 12800K .......... .......... .......... .......... .......... 87%  454M 0s\n",
      " 12850K .......... .......... .......... .......... .......... 87%  717M 0s\n",
      " 12900K .......... .......... .......... .......... .......... 88%  578M 0s\n",
      " 12950K .......... .......... .......... .......... .......... 88%  382M 0s\n",
      " 13000K .......... .......... .......... .......... .......... 89%  537M 0s\n",
      " 13050K .......... .......... .......... .......... .......... 89%  557M 0s\n",
      " 13100K .......... .......... .......... .......... .......... 89%  102M 0s\n",
      " 13150K .......... .......... .......... .......... .......... 90%  399M 0s\n",
      " 13200K .......... .......... .......... .......... .......... 90%  456M 0s\n",
      " 13250K .......... .......... .......... .......... .......... 90%  455M 0s\n",
      " 13300K .......... .......... .......... .......... .......... 91%  424M 0s\n",
      " 13350K .......... .......... .......... .......... .......... 91%  394M 0s\n",
      " 13400K .......... .......... .......... .......... .......... 91%  520M 0s\n",
      " 13450K .......... .......... .......... .......... .......... 92%  177M 0s\n",
      " 13500K .......... .......... .......... .......... .......... 92%  590M 0s\n",
      " 13550K .......... .......... .......... .......... .......... 92%  169M 0s\n",
      " 13600K .......... .......... .......... .......... .......... 93%  483M 0s\n",
      " 13650K .......... .......... .......... .......... .......... 93%  469M 0s\n",
      " 13700K .......... .......... .......... .......... .......... 93%  105M 0s\n",
      " 13750K .......... .......... .......... .......... .......... 94%  183M 0s\n",
      " 13800K .......... .......... .......... .......... .......... 94%  154M 0s\n",
      " 13850K .......... .......... .......... .......... .......... 94%  527M 0s\n",
      " 13900K .......... .......... .......... .......... .......... 95%  199M 0s\n",
      " 13950K .......... .......... .......... .......... .......... 95%  411M 0s\n",
      " 14000K .......... .......... .......... .......... .......... 95%  537M 0s\n",
      " 14050K .......... .......... .......... .......... .......... 96%  212M 0s\n",
      " 14100K .......... .......... .......... .......... .......... 96%  458M 0s\n",
      " 14150K .......... .......... .......... .......... .......... 96%  423M 0s\n",
      " 14200K .......... .......... .......... .......... .......... 97%  392M 0s\n",
      " 14250K .......... .......... .......... .......... .......... 97%  578M 0s\n",
      " 14300K .......... .......... .......... .......... .......... 97%  173M 0s\n",
      " 14350K .......... .......... .......... .......... .......... 98%  486M 0s\n",
      " 14400K .......... .......... .......... .......... .......... 98%  420M 0s\n",
      " 14450K .......... .......... .......... .......... .......... 98%  509M 0s\n",
      " 14500K .......... .......... .......... .......... .......... 99%  577M 0s\n",
      " 14550K .......... .......... .......... .......... .......... 99%  550M 0s\n",
      " 14600K .......... .......... .......... .......... .......... 99%  533M 0s\n",
      " 14650K ..........                                            100%  549M=2.0s\n",
      "\n",
      "2023-02-20 05:33:09 (7.16 MB/s) - ‘rus-eng.zip’ saved [15011848/15011848]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!g2.mig\n",
    "# Скачиваем датасет\n",
    "!wget http://www.manythings.org/anki/rus-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellId": "xoavsjsaeu782s8pub0qs",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xG1xKoQrrJ6n",
    "outputId": "388acecc-cf97-4ff9-8a37-d67d9e65e3c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  rus-eng.zip\n",
      "  inflating: rus-eng/rus.txt         \n",
      "  inflating: rus-eng/_about.txt      \n"
     ]
    }
   ],
   "source": [
    "#!g2.mig\n",
    "# Создаем папку и извлекаем туда файлы из архива\n",
    "!mkdir rus-eng\n",
    "!unzip rus-eng.zip -d rus-eng/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellId": "7rvhlriip5edlin7ht1mqw",
    "id": "TvvlR3IfrOiV"
   },
   "outputs": [],
   "source": [
    "#!g2.mig\n",
    "# Загружаем файл\n",
    "path_to_file = \"/home/jupyter/work/resources/10_Машинный_перевод/rus-eng/rus.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellId": "fa5q89iwx5b1oho66m4v",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BaBQX_1ArWLN",
    "outputId": "efeed19a-9ffb-4884-bd09-a3ed35fd3f67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tМарш!\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1159202 (shanghainese)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!g2.mig\n",
    "# Просмотр файла\n",
    "f = open(path_to_file)\n",
    "for line in f:\n",
    "    print(line)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "kk6jwzj6qb9w3dib5zosql",
    "execution_id": "24080212-3c6c-4128-adc3-82d1f5ce0023",
    "id": "1mHlb7Edrab-"
   },
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "1vinzn61shqbx1koz25p9g",
    "execution_id": "0e89ed1d-6e54-4bb9-ab62-fd1b2315d8db",
    "id": "xy5CqYVRreLp"
   },
   "source": [
    "### Препроцессинг слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellId": "nkdspdbg1dituj9wr0kd4f",
    "id": "4eiNqfCUrZee"
   },
   "outputs": [],
   "source": [
    "#!g2.mig\n",
    "# Функция препроцессинга\n",
    "def preprocess_sentence(w):\n",
    "  #переводим предложение к нижнему регистру и удалем начальные и конечные пробелы\n",
    "    w = w.lower().strip()\n",
    "\n",
    "  # Отделяем пробелом слово и следующую за ним пунктуацию\n",
    "  # Пример: \"he is a boy.\" => \"he is a boy .\"\n",
    "    w = re.sub(r\"([?.!,])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "  # Все, кроме букв и знаков пунктуации, заменяем пробелом\n",
    "    w = re.sub(r\"[^a-zA-Zа-яА-Я?.!,']+\", \" \", w)\n",
    "  \n",
    "  # Удаляем лишние пробелы в начале и конце\n",
    "    w = w.strip()\n",
    "\n",
    "  # Создаем начало и конец последовательности\n",
    "  # Теперь модель знает, где начинать и заканчивать предсказания\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellId": "15iy73fncwub7ua0rohfq",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "yeRVwKeJrp5K",
    "outputId": "5123881d-6e4f-4f5d-ff6c-f6065092d13f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> i can't go . <end>\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.mig\n",
    "# Пример работы препроцессинга\n",
    "preprocess_sentence(\"I can't go.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "l4wosyx6cb3cydd43gl2e",
    "execution_id": "664f7f92-41a5-41af-8aa4-c04170b1e691",
    "id": "hTbsLKmbrtZK"
   },
   "source": [
    "### Создаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellId": "t0bjkpxni2lj2ii655g07",
    "id": "WL2c-yucrshD"
   },
   "outputs": [],
   "source": [
    "#!g2.mig\n",
    "# 1. Убираем акценты\n",
    "# 2. Очищаем предложения\n",
    "# 3. Возвращаем пары слов: [ENG, RUS]\n",
    "def create_dataset(path, num_examples):\n",
    "  # Считываем строки файла\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "  # Каждую строку разделяем на пробелы, берем первые 2 слова, препроцессим их и возвращаем пару\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:2]]  for l in lines[:num_examples]]\n",
    "\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellId": "vpy9ye2a5ski5r4sh2844",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pPa_-nbCryG-",
    "outputId": "66921422-f1ef-464f-e253-5ff100617c58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> go . <end>\n",
      "<start> марш ! <end>\n"
     ]
    }
   ],
   "source": [
    "#!g2.mig\n",
    "# Пример работы\n",
    "en, ru = create_dataset(path_to_file, None)\n",
    "print(en[0])\n",
    "print(ru[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellId": "n130g4faoxdqdzvr8ys2z",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L8LWihWGr4gx",
    "outputId": "63fb2722-c4a9-41f1-cf3b-357d2b534d55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(451436, 451436)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.mig\n",
    "# Количество данных в датасете\n",
    "len(en), len(ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "x4avm1s2q7s5h25qobysbl",
    "execution_id": "72395c04-b417-4f64-aa70-684c3a156286",
    "id": "a9P0vNLor7Ew"
   },
   "source": [
    "### Загрузчик датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellId": "r5nyow1dczo4ntduuta3um",
    "id": "JDgb-rbAr-9b"
   },
   "outputs": [],
   "source": [
    "#!g2.mig\n",
    "def tokenize(lang):\n",
    "      #токенизируем текст, отфильтвовываем пробелы\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "      #обновляем внутренний словарь на основе lang\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "      #преобразуем каждый элемент из lang в последовательность чисел\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "      #преобразуем тензор в матрицу (кол-во тензоров * max-длина), \n",
    "      #при этом короткие последовательности заполняем нулями сзади, а длинные -- обрезаем сзади\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "cfbt7s8y61eojkdavc0s8",
    "execution_id": "2078c08a-6cd7-45e2-95f7-24da7e57ae07",
    "id": "85ecf4HRsBsK"
   },
   "source": [
    "## Создание датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellId": "gr3c9c62xqrrmote032c4b",
    "id": "hlIyu-GBsBHa"
   },
   "outputs": [],
   "source": [
    "#!g2.mig\n",
    "def load_dataset(path, num_examples=None):\n",
    "      # создаем очищенные анг (выходные), русские (входные) пары\n",
    "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "    #применяем токенизацию к каждому элементы из пары\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellId": "o1y9lsn47lhq1qo0ilwb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9rxMiYrasIqN",
    "outputId": "639538cf-a48b-484f-c2cc-d9b10b1e65c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   1, 5663,   24, ...,    0,    0,    0],\n",
       "        [   1,  198,    3, ...,    0,    0,    0],\n",
       "        [   1,  301,    3, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [   1,   33,  234, ...,    0,    0,    0],\n",
       "        [   1,   10,  298, ...,    0,    0,    0],\n",
       "        [   1,   27,   65, ...,    0,    0,    0]], dtype=int32),\n",
       " array([[ 1, 27,  3, ...,  0,  0,  0],\n",
       "        [ 1, 27,  3, ...,  0,  0,  0],\n",
       "        [ 1, 27,  3, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 1, 15,  7, ...,  0,  0,  0],\n",
       "        [ 1, 15,  7, ...,  0,  0,  0],\n",
       "        [ 1, 15,  7, ...,  0,  0,  0]], dtype=int32))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.mig\n",
    "num_examples = 100000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellId": "n20ac7k82jqcf5hx4mwva",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y7yVWCQXsKnv",
    "outputId": "80b162e3-e93a-4ada-9dd9-90b4c5f6a91f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 15)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.mig\n",
    "# Вычисляем максимальную длину тензоров\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
    "max_length_targ, max_length_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellId": "ghl96gqxz85rc9npx5wh5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G-PKbEHjsNKY",
    "outputId": "e7fbab1f-34b1-4d85-e10b-736e89062b54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000 80000 20000 20000\n"
     ]
    }
   ],
   "source": [
    "#!g2.mig\n",
    "# Создаем тренировочные и валидационные датасеты\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Размеры датасетов\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellId": "l3vt63jrrrbfwgoxja0qg",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_YJV1pBLsSCP",
    "outputId": "d2d61884-0c29-4256-e136-f3ac7e1d8468"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[    1,     4,     7, ...,     0,     0,     0],\n",
       "        [    1,     6,     9, ...,     0,     0,     0],\n",
       "        [    1,     6,   203, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    1,    35,     7, ...,     0,     0,     0],\n",
       "        [    1, 15048,   397, ...,     0,     0,     0],\n",
       "        [    1,  1033,    20, ...,     0,     0,     0]], dtype=int32),\n",
       " array([[  1,   4, 114, ...,   0,   0,   0],\n",
       "        [  1,   5, 392, ...,   0,   0,   0],\n",
       "        [  1,   5, 137, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [  1, 316,  69, ...,   0,   0,   0],\n",
       "        [  1, 478,  54, ...,   0,   0,   0],\n",
       "        [  1, 165,  34, ...,   0,   0,   0]], dtype=int32))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.mig\n",
    "input_tensor_train, target_tensor_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellId": "k8yvespi29uun0ggt3n4b",
    "id": "g4D8xFuEsUAx"
   },
   "outputs": [],
   "source": [
    "#!g2.mig\n",
    "# Функция получения из токена текста (выводим токен и его индекс)\n",
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellId": "1a8pgsbhmaz79u2b3myt2p",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fU83iPHvsXUh",
    "outputId": "d8bb2754-52f2-4f6b-d52f-7c4f3832ddaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "4 ----> я\n",
      "7 ----> не\n",
      "1924 ----> занимаюсь\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "4 ----> i\n",
      "114 ----> am\n",
      "31 ----> not\n",
      "845 ----> studying\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "#!g2.mig\n",
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellId": "6wkcqsbvgy6mg0smqnarvi",
    "id": "F8zLp-HNsag-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 05:33:38.661995: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-20 05:33:41.177767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2629 MB memory:  -> device: 0, name: GRID A100X-1-5C MIG 1g.5gb, pci bus id: 0000:8c:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "#!g2.mig\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "# Количество эпох\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 300\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "# Из каждого элемента (input_tensor_train, target_tensor_train) создает тензор\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "# Разбиваем датасет на батчи (списки по 64), удаляя последний неполный батч\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellId": "j642l6dsc6popcxy271mys",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "laDiMRH9sfkZ",
    "outputId": "869e34de-ded1-4122-bdc7-bdfc9caa90c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 15) (64, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(15,), dtype=int32, numpy=\n",
       " array([  1, 419,   9,  17,   4, 517,   3,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0], dtype=int32)>,\n",
       " <tf.Tensor: shape=(11,), dtype=int32, numpy=array([  1,  15,  35,   4, 135,   3,   2,   0,   0,   0,   0], dtype=int32)>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.mig\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "print(example_input_batch.shape, example_target_batch.shape)\n",
    "example_input_batch[0], example_target_batch[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "4f4j1jloqd5tfjznp3i7v",
    "execution_id": "4bbd5c61-d104-41e8-98b7-cb58b2f68c4b",
    "id": "sdlQ_sMQsijW"
   },
   "source": [
    "## Построение модели машинного перевода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "r5or4isuf2b7x7hzi7k2",
    "execution_id": "a9383212-5cbb-4fe5-b1a9-563115a3d646",
    "id": "fFELVTGisloS"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellId": "idvu3ruttmf82mq0iw0jr9",
    "id": "yln6OknMshph"
   },
   "outputs": [],
   "source": [
    "#!g2.mig\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=False,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "    #создаем тензор из нулей размера (батч, кол-во ячеек)\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellId": "qqbgnht6mdbyqwyzj9am1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H65-mj2wsp-g",
    "outputId": "c338caa3-b88c-42e4-c9ad-0a8d80904ac6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 05:33:45.868764: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "#!g2.mig\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# инициализитеруем начальное скрытое состояние (из нулей)\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "# применяем энкодер к входному батчу и скрытому состоянию\n",
    "sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "# print ('Форма выхода энкодера: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellId": "4hl4v45qfms9wimo1xsmus",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4uUuXntcsuYw",
    "outputId": "ecb887a0-46a6-4521-cf17-2dcd7c354058"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 1024), dtype=float32, numpy=\n",
       "array([[ 0.02330193,  0.01872611, -0.00576977, ..., -0.01749688,\n",
       "        -0.02627492, -0.00945185],\n",
       "       [ 0.02349954,  0.01863672, -0.00549863, ..., -0.01758788,\n",
       "        -0.02637304, -0.00938264],\n",
       "       [ 0.02361627,  0.01884528, -0.00534972, ..., -0.01749588,\n",
       "        -0.02672212, -0.00948581],\n",
       "       ...,\n",
       "       [ 0.02353191,  0.01881711, -0.00512481, ..., -0.01756242,\n",
       "        -0.02684424, -0.0095759 ],\n",
       "       [ 0.02355534,  0.01885203, -0.00521339, ..., -0.01759139,\n",
       "        -0.02679635, -0.00949376],\n",
       "       [ 0.02348568,  0.01865501, -0.00551217, ..., -0.01749637,\n",
       "        -0.02636423, -0.00953596]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.mig\n",
    "sample_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "6oypil8yjutd4170x4w9d",
    "execution_id": "57ca6921-94a6-4c93-9938-7cb3672df2b2",
    "id": "qp4VQxCGswDp"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cellId": "xql7k1o4v1r8cznympfr1h",
    "id": "hZhaZOZmsy5q"
   },
   "outputs": [],
   "source": [
    "#!g2.mig\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        # x shape после прохождения через эмбеддинг == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # отправляем в GRU входные данные и скрытое состояние (от энкодера)\n",
    "        #выход GRU (batch_size, timesteps, units)\n",
    "        #размер возвращаемого внутреннего состояния (batch_size, units)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # x shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cellId": "h07i7tkyfv67zez0qejh9i",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-YUF_4Qes1bd",
    "outputId": "7d9362ef-dda8-4b4a-aa3f-91ac65645140"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 05:33:51.128241: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 7335]), TensorShape([64, 1024]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.mig\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "# Применяем декодер к случайному батчу из равномерного распределения (батч,1) и выходу энкодера\n",
    "decoder_sample_x, decoder_sample_h = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden)\n",
    "decoder_sample_x.shape, decoder_sample_h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellId": "4m0g52hnyzw71pg3024wi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AFUlOJBDs4cV",
    "outputId": "1d1084f5-3cce-4d99-86ee-ce6dbe8f894b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 7335), dtype=float32, numpy=\n",
       "array([[ 0.00068364, -0.00066774,  0.0022516 , ...,  0.00801814,\n",
       "         0.00340847, -0.00804379],\n",
       "       [ 0.00073537, -0.0006414 ,  0.00229872, ...,  0.00805619,\n",
       "         0.00346467, -0.00795238],\n",
       "       [ 0.00078047, -0.00060194,  0.00228008, ...,  0.00805415,\n",
       "         0.00352017, -0.00791376],\n",
       "       ...,\n",
       "       [ 0.00077555, -0.00059668,  0.00229293, ...,  0.00810161,\n",
       "         0.00353668, -0.0078459 ],\n",
       "       [ 0.00078429, -0.00058722,  0.00228078, ...,  0.00807798,\n",
       "         0.00353996, -0.00785472],\n",
       "       [ 0.00070706, -0.00062992,  0.00224415, ...,  0.00800894,\n",
       "         0.00348977, -0.00798003]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.mig\n",
    "decoder_sample_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "yo9nenqoavggr36jo4zeq",
    "execution_id": "1dea6319-e8d3-4ba4-9f1f-ed5728a88e72",
    "id": "XXQId1hPs7PZ"
   },
   "source": [
    "### Компиляция модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cellId": "1sz93ds5njoh0mfhlw4gqp",
    "id": "8K-eY_Fps6JH"
   },
   "outputs": [],
   "source": [
    "#!g2.mig\n",
    "# Оптимизатор\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "# Функция потерь\n",
    "def loss_function(real, pred):\n",
    "      #делаем инверсию значений сравнения каждого из real с нулем (возвращается true или false)\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "      # Применяем функцию ошибок к реальным данным и предсказанным\n",
    "    loss_ = loss_object(real, pred)\n",
    "      # Приводим тензор mask к новому типу loss_.dtype\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "      # Умножаем loss_ на mask\n",
    "    loss_ *= mask\n",
    "      # Возвращаем среднее значениe всех элементов\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "q3xha5mzh2lomo2k0f124",
    "execution_id": "c615b8f8-d3fb-40d5-8dba-903168a46703",
    "id": "HXgSuW57tFxW"
   },
   "source": [
    "### Сheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cellId": "a05lpoqllgp9rcw63jbpr",
    "id": "2qDLysQutIC5"
   },
   "outputs": [],
   "source": [
    "#!g2.mig\n",
    "checkpoint_dir = './training_nmt_checkpoints'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "lywiyueuxiibud64li6yuh",
    "execution_id": "a285ee48-d4c6-4c4e-b5fb-411e67c99040",
    "id": "qTrA8mOUtLSq"
   },
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cellId": "pguia2g65xli7szmutxvs",
    "id": "1B4AlQGStNe2"
   },
   "outputs": [],
   "source": [
    "#!g2.mig\n",
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "  # Перечисляем операции для автоматического дифференцирования\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Получаем выход encoder\n",
    "        enc_hidden = encoder(inp, enc_hidden)\n",
    "        # Помещаем его в скрытое состояние decoder\n",
    "        dec_hidden = enc_hidden\n",
    "        # Формируем вход декодера:\n",
    "                 # Берем список длины батч из индексов тега <start> (1)\n",
    "                 # Приписываем списку размерность 1 сзади (батч, 1)\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - выводим target в качестве следующего входа\n",
    "        for t in range(1, targ.shape[1]):\n",
    "          # Помещаем enc_output в decoder\n",
    "            predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "          # Считаем функцию потерь \n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "          # Используем teacher forcing (приписываем списку размерность 1 сзади)\n",
    "          # Посылаем dec_input на вход декордера \n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    # Переменные\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    # Вычисляем градиенты loss по variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    # Оптимизатор применяет подсчитанные градиенты\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cellId": "wgpeot5h06xl1w0pcaro",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution_id": "02f5dc09-234c-4ef6-beb7-b05ccf04527b",
    "id": "mfkqGGUxtaSG",
    "outputId": "57f52ae9-891e-4727-f597-25230ac3bb9a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 05:34:00.173097: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.6273\n",
      "Epoch 1 Batch 100 Loss 2.0475\n",
      "Epoch 1 Batch 200 Loss 1.8684\n",
      "Epoch 1 Batch 300 Loss 1.5631\n",
      "Epoch 1 Batch 400 Loss 1.4717\n",
      "Epoch 1 Batch 500 Loss 1.4235\n",
      "Epoch 1 Batch 600 Loss 1.2639\n",
      "Epoch 1 Batch 700 Loss 1.2760\n",
      "Epoch 1 Batch 800 Loss 1.2481\n",
      "Epoch 1 Batch 900 Loss 1.0421\n",
      "Epoch 1 Batch 1000 Loss 1.0551\n",
      "Epoch 1 Batch 1100 Loss 0.9110\n",
      "Epoch 1 Batch 1200 Loss 0.8704\n",
      "Epoch 1 Loss 1.3991\n",
      "Time taken for 1 epoch 77.67933869361877 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.8593\n",
      "Epoch 2 Batch 100 Loss 0.8048\n",
      "Epoch 2 Batch 200 Loss 0.8494\n",
      "Epoch 2 Batch 300 Loss 0.7857\n",
      "Epoch 2 Batch 400 Loss 0.8179\n",
      "Epoch 2 Batch 500 Loss 0.6948\n",
      "Epoch 2 Batch 600 Loss 0.7596\n",
      "Epoch 2 Batch 700 Loss 0.6758\n",
      "Epoch 2 Batch 800 Loss 0.7247\n",
      "Epoch 2 Batch 900 Loss 0.5929\n",
      "Epoch 2 Batch 1000 Loss 0.5928\n",
      "Epoch 2 Batch 1100 Loss 0.5978\n",
      "Epoch 2 Batch 1200 Loss 0.5692\n",
      "Epoch 2 Loss 0.6951\n",
      "Time taken for 1 epoch 70.99302291870117 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.4453\n",
      "Epoch 3 Batch 100 Loss 0.4176\n",
      "Epoch 3 Batch 200 Loss 0.4343\n",
      "Epoch 3 Batch 300 Loss 0.4044\n",
      "Epoch 3 Batch 400 Loss 0.3714\n",
      "Epoch 3 Batch 500 Loss 0.4988\n",
      "Epoch 3 Batch 600 Loss 0.4188\n",
      "Epoch 3 Batch 700 Loss 0.3786\n",
      "Epoch 3 Batch 800 Loss 0.3860\n",
      "Epoch 3 Batch 900 Loss 0.2478\n",
      "Epoch 3 Batch 1000 Loss 0.3438\n",
      "Epoch 3 Batch 1100 Loss 0.3170\n",
      "Epoch 3 Batch 1200 Loss 0.3008\n",
      "Epoch 3 Loss 0.3805\n",
      "Time taken for 1 epoch 69.000483751297 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.2207\n",
      "Epoch 4 Batch 100 Loss 0.2088\n",
      "Epoch 4 Batch 200 Loss 0.1709\n",
      "Epoch 4 Batch 300 Loss 0.2479\n",
      "Epoch 4 Batch 400 Loss 0.2419\n",
      "Epoch 4 Batch 500 Loss 0.2695\n",
      "Epoch 4 Batch 600 Loss 0.2178\n",
      "Epoch 4 Batch 700 Loss 0.2253\n",
      "Epoch 4 Batch 800 Loss 0.2142\n",
      "Epoch 4 Batch 900 Loss 0.2779\n",
      "Epoch 4 Batch 1000 Loss 0.2157\n",
      "Epoch 4 Batch 1100 Loss 0.2342\n",
      "Epoch 4 Batch 1200 Loss 0.2252\n",
      "Epoch 4 Loss 0.2271\n",
      "Time taken for 1 epoch 70.87761664390564 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.1538\n",
      "Epoch 5 Batch 100 Loss 0.1453\n",
      "Epoch 5 Batch 200 Loss 0.1527\n",
      "Epoch 5 Batch 300 Loss 0.1431\n",
      "Epoch 5 Batch 400 Loss 0.1254\n",
      "Epoch 5 Batch 500 Loss 0.1182\n",
      "Epoch 5 Batch 600 Loss 0.1578\n",
      "Epoch 5 Batch 700 Loss 0.1315\n",
      "Epoch 5 Batch 800 Loss 0.1431\n",
      "Epoch 5 Batch 900 Loss 0.1695\n",
      "Epoch 5 Batch 1000 Loss 0.1668\n",
      "Epoch 5 Batch 1100 Loss 0.1609\n",
      "Epoch 5 Batch 1200 Loss 0.1714\n",
      "Epoch 5 Loss 0.1541\n",
      "Time taken for 1 epoch 68.99554204940796 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.1382\n",
      "Epoch 6 Batch 100 Loss 0.1015\n",
      "Epoch 6 Batch 200 Loss 0.1038\n",
      "Epoch 6 Batch 300 Loss 0.0937\n",
      "Epoch 6 Batch 400 Loss 0.1201\n",
      "Epoch 6 Batch 500 Loss 0.1261\n",
      "Epoch 6 Batch 600 Loss 0.0995\n",
      "Epoch 6 Batch 700 Loss 0.0800\n",
      "Epoch 6 Batch 800 Loss 0.0909\n",
      "Epoch 6 Batch 900 Loss 0.1172\n",
      "Epoch 6 Batch 1000 Loss 0.0945\n",
      "Epoch 6 Batch 1100 Loss 0.1329\n",
      "Epoch 6 Batch 1200 Loss 0.1484\n",
      "Epoch 6 Loss 0.1186\n",
      "Time taken for 1 epoch 70.88530230522156 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0733\n",
      "Epoch 7 Batch 100 Loss 0.0723\n",
      "Epoch 7 Batch 200 Loss 0.0856\n",
      "Epoch 7 Batch 300 Loss 0.0782\n",
      "Epoch 7 Batch 400 Loss 0.1424\n",
      "Epoch 7 Batch 500 Loss 0.1052\n",
      "Epoch 7 Batch 600 Loss 0.0734\n",
      "Epoch 7 Batch 700 Loss 0.1260\n",
      "Epoch 7 Batch 800 Loss 0.1107\n",
      "Epoch 7 Batch 900 Loss 0.1330\n",
      "Epoch 7 Batch 1000 Loss 0.0930\n",
      "Epoch 7 Batch 1100 Loss 0.1054\n",
      "Epoch 7 Batch 1200 Loss 0.1145\n",
      "Epoch 7 Loss 0.1008\n",
      "Time taken for 1 epoch 68.99496150016785 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0745\n",
      "Epoch 8 Batch 100 Loss 0.0677\n",
      "Epoch 8 Batch 200 Loss 0.0770\n",
      "Epoch 8 Batch 300 Loss 0.0932\n",
      "Epoch 8 Batch 400 Loss 0.0747\n",
      "Epoch 8 Batch 500 Loss 0.0975\n",
      "Epoch 8 Batch 600 Loss 0.0936\n",
      "Epoch 8 Batch 700 Loss 0.1227\n",
      "Epoch 8 Batch 800 Loss 0.0935\n",
      "Epoch 8 Batch 900 Loss 0.1068\n",
      "Epoch 8 Batch 1000 Loss 0.0870\n",
      "Epoch 8 Batch 1100 Loss 0.0755\n",
      "Epoch 8 Batch 1200 Loss 0.1221\n",
      "Epoch 8 Loss 0.0893\n",
      "Time taken for 1 epoch 70.99048924446106 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.1192\n",
      "Epoch 9 Batch 100 Loss 0.0548\n",
      "Epoch 9 Batch 200 Loss 0.0579\n",
      "Epoch 9 Batch 300 Loss 0.0820\n",
      "Epoch 9 Batch 400 Loss 0.0748\n",
      "Epoch 9 Batch 500 Loss 0.0877\n",
      "Epoch 9 Batch 600 Loss 0.0772\n",
      "Epoch 9 Batch 700 Loss 0.0686\n",
      "Epoch 9 Batch 800 Loss 0.0875\n",
      "Epoch 9 Batch 900 Loss 0.0740\n",
      "Epoch 9 Batch 1000 Loss 0.0520\n",
      "Epoch 9 Batch 1100 Loss 0.0693\n",
      "Epoch 9 Batch 1200 Loss 0.1225\n",
      "Epoch 9 Loss 0.0835\n",
      "Time taken for 1 epoch 68.99705576896667 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0643\n",
      "Epoch 10 Batch 100 Loss 0.0677\n",
      "Epoch 10 Batch 200 Loss 0.0610\n",
      "Epoch 10 Batch 300 Loss 0.0900\n",
      "Epoch 10 Batch 400 Loss 0.0530\n",
      "Epoch 10 Batch 500 Loss 0.0939\n",
      "Epoch 10 Batch 600 Loss 0.0861\n",
      "Epoch 10 Batch 700 Loss 0.0880\n",
      "Epoch 10 Batch 800 Loss 0.0853\n",
      "Epoch 10 Batch 900 Loss 0.1071\n",
      "Epoch 10 Batch 1000 Loss 0.1108\n",
      "Epoch 10 Batch 1100 Loss 0.0736\n",
      "Epoch 10 Batch 1200 Loss 0.0878\n",
      "Epoch 10 Loss 0.0790\n",
      "Time taken for 1 epoch 71.06072521209717 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.0960\n",
      "Epoch 11 Batch 100 Loss 0.0619\n",
      "Epoch 11 Batch 200 Loss 0.0829\n",
      "Epoch 11 Batch 300 Loss 0.0827\n",
      "Epoch 11 Batch 400 Loss 0.0909\n",
      "Epoch 11 Batch 500 Loss 0.0925\n",
      "Epoch 11 Batch 600 Loss 0.0929\n",
      "Epoch 11 Batch 700 Loss 0.0830\n",
      "Epoch 11 Batch 800 Loss 0.1093\n",
      "Epoch 11 Batch 900 Loss 0.0461\n",
      "Epoch 11 Batch 1000 Loss 0.0775\n",
      "Epoch 11 Batch 1100 Loss 0.0894\n",
      "Epoch 11 Batch 1200 Loss 0.0988\n",
      "Epoch 11 Loss 0.0743\n",
      "Time taken for 1 epoch 69.02848100662231 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.0498\n",
      "Epoch 12 Batch 100 Loss 0.0431\n",
      "Epoch 12 Batch 200 Loss 0.0806\n",
      "Epoch 12 Batch 300 Loss 0.1105\n",
      "Epoch 12 Batch 400 Loss 0.0348\n",
      "Epoch 12 Batch 500 Loss 0.0584\n",
      "Epoch 12 Batch 600 Loss 0.0706\n",
      "Epoch 12 Batch 700 Loss 0.0647\n",
      "Epoch 12 Batch 800 Loss 0.0936\n",
      "Epoch 12 Batch 900 Loss 0.0507\n",
      "Epoch 12 Batch 1000 Loss 0.0808\n",
      "Epoch 12 Batch 1100 Loss 0.1190\n",
      "Epoch 12 Batch 1200 Loss 0.0635\n",
      "Epoch 12 Loss 0.0720\n",
      "Time taken for 1 epoch 71.07995200157166 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.0511\n",
      "Epoch 13 Batch 100 Loss 0.0517\n",
      "Epoch 13 Batch 200 Loss 0.0660\n",
      "Epoch 13 Batch 300 Loss 0.0800\n",
      "Epoch 13 Batch 400 Loss 0.0930\n",
      "Epoch 13 Batch 500 Loss 0.0553\n",
      "Epoch 13 Batch 600 Loss 0.0499\n",
      "Epoch 13 Batch 700 Loss 0.0549\n",
      "Epoch 13 Batch 800 Loss 0.0577\n",
      "Epoch 13 Batch 900 Loss 0.0879\n",
      "Epoch 13 Batch 1000 Loss 0.0706\n",
      "Epoch 13 Batch 1100 Loss 0.1247\n",
      "Epoch 13 Batch 1200 Loss 0.0566\n",
      "Epoch 13 Loss 0.0692\n",
      "Time taken for 1 epoch 69.0249092578888 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.0667\n",
      "Epoch 14 Batch 100 Loss 0.0527\n",
      "Epoch 14 Batch 200 Loss 0.0398\n",
      "Epoch 14 Batch 300 Loss 0.0455\n",
      "Epoch 14 Batch 400 Loss 0.0437\n",
      "Epoch 14 Batch 500 Loss 0.0584\n",
      "Epoch 14 Batch 600 Loss 0.0738\n",
      "Epoch 14 Batch 700 Loss 0.0608\n",
      "Epoch 14 Batch 800 Loss 0.0572\n",
      "Epoch 14 Batch 900 Loss 0.0862\n",
      "Epoch 14 Batch 1000 Loss 0.0862\n",
      "Epoch 14 Batch 1100 Loss 0.0675\n",
      "Epoch 14 Batch 1200 Loss 0.1228\n",
      "Epoch 14 Loss 0.0682\n",
      "Time taken for 1 epoch 71.05016493797302 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.0772\n",
      "Epoch 15 Batch 100 Loss 0.0217\n",
      "Epoch 15 Batch 200 Loss 0.0586\n",
      "Epoch 15 Batch 300 Loss 0.0470\n",
      "Epoch 15 Batch 400 Loss 0.0677\n",
      "Epoch 15 Batch 500 Loss 0.0336\n",
      "Epoch 15 Batch 600 Loss 0.1013\n",
      "Epoch 15 Batch 700 Loss 0.0688\n",
      "Epoch 15 Batch 800 Loss 0.0376\n",
      "Epoch 15 Batch 900 Loss 0.0675\n",
      "Epoch 15 Batch 1000 Loss 0.0231\n",
      "Epoch 15 Batch 1100 Loss 0.1165\n",
      "Epoch 15 Batch 1200 Loss 0.0755\n",
      "Epoch 15 Loss 0.0668\n",
      "Time taken for 1 epoch 69.04774212837219 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.0671\n",
      "Epoch 16 Batch 100 Loss 0.0710\n",
      "Epoch 16 Batch 200 Loss 0.0919\n",
      "Epoch 16 Batch 300 Loss 0.0585\n",
      "Epoch 16 Batch 400 Loss 0.0415\n",
      "Epoch 16 Batch 500 Loss 0.0781\n",
      "Epoch 16 Batch 600 Loss 0.0544\n",
      "Epoch 16 Batch 700 Loss 0.0537\n",
      "Epoch 16 Batch 800 Loss 0.0675\n",
      "Epoch 16 Batch 900 Loss 0.0466\n",
      "Epoch 16 Batch 1000 Loss 0.0515\n",
      "Epoch 16 Batch 1100 Loss 0.1412\n",
      "Epoch 16 Batch 1200 Loss 0.0714\n",
      "Epoch 16 Loss 0.0645\n",
      "Time taken for 1 epoch 70.98867273330688 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.0626\n",
      "Epoch 17 Batch 100 Loss 0.0438\n",
      "Epoch 17 Batch 200 Loss 0.0761\n",
      "Epoch 17 Batch 300 Loss 0.0476\n",
      "Epoch 17 Batch 400 Loss 0.0496\n",
      "Epoch 17 Batch 500 Loss 0.0395\n",
      "Epoch 17 Batch 600 Loss 0.0542\n",
      "Epoch 17 Batch 700 Loss 0.0713\n",
      "Epoch 17 Batch 800 Loss 0.0948\n",
      "Epoch 17 Batch 900 Loss 0.0743\n",
      "Epoch 17 Batch 1000 Loss 0.0735\n",
      "Epoch 17 Batch 1100 Loss 0.0687\n",
      "Epoch 17 Batch 1200 Loss 0.0804\n",
      "Epoch 17 Loss 0.0633\n",
      "Time taken for 1 epoch 69.01521921157837 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.0380\n",
      "Epoch 18 Batch 100 Loss 0.0432\n",
      "Epoch 18 Batch 200 Loss 0.0651\n",
      "Epoch 18 Batch 300 Loss 0.0354\n",
      "Epoch 18 Batch 400 Loss 0.0562\n",
      "Epoch 18 Batch 500 Loss 0.0975\n",
      "Epoch 18 Batch 600 Loss 0.0755\n",
      "Epoch 18 Batch 700 Loss 0.0476\n",
      "Epoch 18 Batch 800 Loss 0.0614\n",
      "Epoch 18 Batch 900 Loss 0.0735\n",
      "Epoch 18 Batch 1000 Loss 0.0600\n",
      "Epoch 18 Batch 1100 Loss 0.1025\n",
      "Epoch 18 Batch 1200 Loss 0.0753\n",
      "Epoch 18 Loss 0.0616\n",
      "Time taken for 1 epoch 71.00489234924316 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.0451\n",
      "Epoch 19 Batch 100 Loss 0.0410\n",
      "Epoch 19 Batch 200 Loss 0.0335\n",
      "Epoch 19 Batch 300 Loss 0.0670\n",
      "Epoch 19 Batch 400 Loss 0.0779\n",
      "Epoch 19 Batch 500 Loss 0.0646\n",
      "Epoch 19 Batch 600 Loss 0.0516\n",
      "Epoch 19 Batch 700 Loss 0.0695\n",
      "Epoch 19 Batch 800 Loss 0.0712\n",
      "Epoch 19 Batch 900 Loss 0.1203\n",
      "Epoch 19 Batch 1000 Loss 0.0758\n",
      "Epoch 19 Batch 1100 Loss 0.0789\n",
      "Epoch 19 Batch 1200 Loss 0.0679\n",
      "Epoch 19 Loss 0.0612\n",
      "Time taken for 1 epoch 69.01583290100098 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.0320\n",
      "Epoch 20 Batch 100 Loss 0.0497\n",
      "Epoch 20 Batch 200 Loss 0.0587\n",
      "Epoch 20 Batch 300 Loss 0.0420\n",
      "Epoch 20 Batch 400 Loss 0.0562\n",
      "Epoch 20 Batch 500 Loss 0.0710\n",
      "Epoch 20 Batch 600 Loss 0.0853\n",
      "Epoch 20 Batch 700 Loss 0.0478\n",
      "Epoch 20 Batch 800 Loss 0.0737\n",
      "Epoch 20 Batch 900 Loss 0.0438\n",
      "Epoch 20 Batch 1000 Loss 0.0607\n",
      "Epoch 20 Batch 1100 Loss 0.0543\n",
      "Epoch 20 Batch 1200 Loss 0.0473\n",
      "Epoch 20 Loss 0.0586\n",
      "Time taken for 1 epoch 71.15649819374084 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.0647\n",
      "Epoch 21 Batch 100 Loss 0.0390\n",
      "Epoch 21 Batch 200 Loss 0.0611\n",
      "Epoch 21 Batch 300 Loss 0.0239\n",
      "Epoch 21 Batch 400 Loss 0.0477\n",
      "Epoch 21 Batch 500 Loss 0.0611\n",
      "Epoch 21 Batch 600 Loss 0.0806\n",
      "Epoch 21 Batch 700 Loss 0.0464\n",
      "Epoch 21 Batch 800 Loss 0.0810\n",
      "Epoch 21 Batch 900 Loss 0.0589\n",
      "Epoch 21 Batch 1000 Loss 0.0524\n",
      "Epoch 21 Batch 1100 Loss 0.0554\n",
      "Epoch 21 Batch 1200 Loss 0.0589\n",
      "Epoch 21 Loss 0.0585\n",
      "Time taken for 1 epoch 69.01479411125183 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.0624\n",
      "Epoch 22 Batch 100 Loss 0.0639\n",
      "Epoch 22 Batch 200 Loss 0.0619\n",
      "Epoch 22 Batch 300 Loss 0.0574\n",
      "Epoch 22 Batch 400 Loss 0.1214\n",
      "Epoch 22 Batch 500 Loss 0.0819\n",
      "Epoch 22 Batch 600 Loss 0.0584\n",
      "Epoch 22 Batch 700 Loss 0.0416\n",
      "Epoch 22 Batch 800 Loss 0.0677\n",
      "Epoch 22 Batch 900 Loss 0.0344\n",
      "Epoch 22 Batch 1000 Loss 0.0738\n",
      "Epoch 22 Batch 1100 Loss 0.0615\n",
      "Epoch 22 Batch 1200 Loss 0.0567\n",
      "Epoch 22 Loss 0.0574\n",
      "Time taken for 1 epoch 70.90040040016174 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.0597\n",
      "Epoch 23 Batch 100 Loss 0.0574\n",
      "Epoch 23 Batch 200 Loss 0.0364\n",
      "Epoch 23 Batch 300 Loss 0.0539\n",
      "Epoch 23 Batch 400 Loss 0.0511\n",
      "Epoch 23 Batch 500 Loss 0.0661\n",
      "Epoch 23 Batch 600 Loss 0.0949\n",
      "Epoch 23 Batch 700 Loss 0.0406\n",
      "Epoch 23 Batch 800 Loss 0.0347\n",
      "Epoch 23 Batch 900 Loss 0.0301\n",
      "Epoch 23 Batch 1000 Loss 0.0325\n",
      "Epoch 23 Batch 1100 Loss 0.0746\n",
      "Epoch 23 Batch 1200 Loss 0.0471\n",
      "Epoch 23 Loss 0.0568\n",
      "Time taken for 1 epoch 69.03620934486389 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.0570\n",
      "Epoch 24 Batch 100 Loss 0.0422\n",
      "Epoch 24 Batch 200 Loss 0.0453\n",
      "Epoch 24 Batch 300 Loss 0.0612\n",
      "Epoch 24 Batch 400 Loss 0.0570\n",
      "Epoch 24 Batch 500 Loss 0.0642\n",
      "Epoch 24 Batch 600 Loss 0.0536\n",
      "Epoch 24 Batch 700 Loss 0.0649\n",
      "Epoch 24 Batch 800 Loss 0.0710\n",
      "Epoch 24 Batch 900 Loss 0.0592\n",
      "Epoch 24 Batch 1000 Loss 0.0807\n",
      "Epoch 24 Batch 1100 Loss 0.0607\n",
      "Epoch 24 Batch 1200 Loss 0.0420\n",
      "Epoch 24 Loss 0.0558\n",
      "Time taken for 1 epoch 71.09938430786133 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.0382\n",
      "Epoch 25 Batch 100 Loss 0.0671\n",
      "Epoch 25 Batch 200 Loss 0.0209\n",
      "Epoch 25 Batch 300 Loss 0.0372\n",
      "Epoch 25 Batch 400 Loss 0.0444\n",
      "Epoch 25 Batch 500 Loss 0.0477\n",
      "Epoch 25 Batch 600 Loss 0.0562\n",
      "Epoch 25 Batch 700 Loss 0.0514\n",
      "Epoch 25 Batch 800 Loss 0.0439\n",
      "Epoch 25 Batch 900 Loss 0.0603\n",
      "Epoch 25 Batch 1000 Loss 0.0398\n",
      "Epoch 25 Batch 1100 Loss 0.0512\n",
      "Epoch 25 Batch 1200 Loss 0.0561\n",
      "Epoch 25 Loss 0.0544\n",
      "Time taken for 1 epoch 69.03253984451294 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.0591\n",
      "Epoch 26 Batch 100 Loss 0.0566\n",
      "Epoch 26 Batch 200 Loss 0.0469\n",
      "Epoch 26 Batch 300 Loss 0.0697\n",
      "Epoch 26 Batch 400 Loss 0.0421\n",
      "Epoch 26 Batch 500 Loss 0.0407\n",
      "Epoch 26 Batch 600 Loss 0.0422\n",
      "Epoch 26 Batch 700 Loss 0.0688\n",
      "Epoch 26 Batch 800 Loss 0.0618\n",
      "Epoch 26 Batch 900 Loss 0.0460\n",
      "Epoch 26 Batch 1000 Loss 0.0361\n",
      "Epoch 26 Batch 1100 Loss 0.0366\n",
      "Epoch 26 Batch 1200 Loss 0.0864\n",
      "Epoch 26 Loss 0.0541\n",
      "Time taken for 1 epoch 70.9617280960083 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.0322\n",
      "Epoch 27 Batch 100 Loss 0.0279\n",
      "Epoch 27 Batch 200 Loss 0.0689\n",
      "Epoch 27 Batch 300 Loss 0.0243\n",
      "Epoch 27 Batch 400 Loss 0.0384\n",
      "Epoch 27 Batch 500 Loss 0.0600\n",
      "Epoch 27 Batch 600 Loss 0.0343\n",
      "Epoch 27 Batch 700 Loss 0.0843\n",
      "Epoch 27 Batch 800 Loss 0.0458\n",
      "Epoch 27 Batch 900 Loss 0.0358\n",
      "Epoch 27 Batch 1000 Loss 0.0686\n",
      "Epoch 27 Batch 1100 Loss 0.0337\n",
      "Epoch 27 Batch 1200 Loss 0.0482\n",
      "Epoch 27 Loss 0.0535\n",
      "Time taken for 1 epoch 69.02426934242249 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.0573\n",
      "Epoch 28 Batch 100 Loss 0.0319\n",
      "Epoch 28 Batch 200 Loss 0.0467\n",
      "Epoch 28 Batch 300 Loss 0.0407\n",
      "Epoch 28 Batch 400 Loss 0.0451\n",
      "Epoch 28 Batch 500 Loss 0.1031\n",
      "Epoch 28 Batch 600 Loss 0.0517\n",
      "Epoch 28 Batch 700 Loss 0.0547\n",
      "Epoch 28 Batch 800 Loss 0.0491\n",
      "Epoch 28 Batch 900 Loss 0.0386\n",
      "Epoch 28 Batch 1000 Loss 0.0793\n",
      "Epoch 28 Batch 1100 Loss 0.0421\n",
      "Epoch 28 Batch 1200 Loss 0.0535\n",
      "Epoch 28 Loss 0.0525\n",
      "Time taken for 1 epoch 71.03588557243347 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.0245\n",
      "Epoch 29 Batch 100 Loss 0.0519\n",
      "Epoch 29 Batch 200 Loss 0.0660\n",
      "Epoch 29 Batch 300 Loss 0.0344\n",
      "Epoch 29 Batch 400 Loss 0.0497\n",
      "Epoch 29 Batch 500 Loss 0.0606\n",
      "Epoch 29 Batch 600 Loss 0.0505\n",
      "Epoch 29 Batch 700 Loss 0.0280\n",
      "Epoch 29 Batch 800 Loss 0.0679\n",
      "Epoch 29 Batch 900 Loss 0.0596\n",
      "Epoch 29 Batch 1000 Loss 0.0678\n",
      "Epoch 29 Batch 1100 Loss 0.0600\n",
      "Epoch 29 Batch 1200 Loss 0.0614\n",
      "Epoch 29 Loss 0.0521\n",
      "Time taken for 1 epoch 69.02784156799316 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.0523\n",
      "Epoch 30 Batch 100 Loss 0.0357\n",
      "Epoch 30 Batch 200 Loss 0.0300\n",
      "Epoch 30 Batch 300 Loss 0.0370\n",
      "Epoch 30 Batch 400 Loss 0.0276\n",
      "Epoch 30 Batch 500 Loss 0.0392\n",
      "Epoch 30 Batch 600 Loss 0.0378\n",
      "Epoch 30 Batch 700 Loss 0.0385\n",
      "Epoch 30 Batch 800 Loss 0.0535\n",
      "Epoch 30 Batch 900 Loss 0.0556\n",
      "Epoch 30 Batch 1000 Loss 0.0456\n",
      "Epoch 30 Batch 1100 Loss 0.0805\n",
      "Epoch 30 Batch 1200 Loss 0.0661\n",
      "Epoch 30 Loss 0.0512\n",
      "Time taken for 1 epoch 70.97419881820679 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.0347\n",
      "Epoch 31 Batch 100 Loss 0.0181\n",
      "Epoch 31 Batch 200 Loss 0.0446\n",
      "Epoch 31 Batch 300 Loss 0.0438\n",
      "Epoch 31 Batch 400 Loss 0.0619\n",
      "Epoch 31 Batch 500 Loss 0.0373\n",
      "Epoch 31 Batch 600 Loss 0.0242\n",
      "Epoch 31 Batch 700 Loss 0.0692\n",
      "Epoch 31 Batch 800 Loss 0.0578\n",
      "Epoch 31 Batch 900 Loss 0.0613\n",
      "Epoch 31 Batch 1000 Loss 0.0420\n",
      "Epoch 31 Batch 1100 Loss 0.0664\n",
      "Epoch 31 Batch 1200 Loss 0.0790\n",
      "Epoch 31 Loss 0.0506\n",
      "Time taken for 1 epoch 69.01847815513611 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.0634\n",
      "Epoch 32 Batch 100 Loss 0.0341\n",
      "Epoch 32 Batch 200 Loss 0.0204\n",
      "Epoch 32 Batch 300 Loss 0.0433\n",
      "Epoch 32 Batch 400 Loss 0.0341\n",
      "Epoch 32 Batch 500 Loss 0.0296\n",
      "Epoch 32 Batch 600 Loss 0.0614\n",
      "Epoch 32 Batch 700 Loss 0.0676\n",
      "Epoch 32 Batch 800 Loss 0.0236\n",
      "Epoch 32 Batch 900 Loss 0.0372\n",
      "Epoch 32 Batch 1000 Loss 0.0712\n",
      "Epoch 32 Batch 1100 Loss 0.0614\n",
      "Epoch 32 Batch 1200 Loss 0.0262\n",
      "Epoch 32 Loss 0.0500\n",
      "Time taken for 1 epoch 71.0816879272461 sec\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.0367\n",
      "Epoch 33 Batch 100 Loss 0.0269\n",
      "Epoch 33 Batch 200 Loss 0.0463\n",
      "Epoch 33 Batch 300 Loss 0.0399\n",
      "Epoch 33 Batch 400 Loss 0.0315\n",
      "Epoch 33 Batch 500 Loss 0.0572\n",
      "Epoch 33 Batch 600 Loss 0.0483\n",
      "Epoch 33 Batch 700 Loss 0.0377\n",
      "Epoch 33 Batch 800 Loss 0.0627\n",
      "Epoch 33 Batch 900 Loss 0.0713\n",
      "Epoch 33 Batch 1000 Loss 0.0573\n",
      "Epoch 33 Batch 1100 Loss 0.0700\n",
      "Epoch 33 Batch 1200 Loss 0.0459\n",
      "Epoch 33 Loss 0.0494\n",
      "Time taken for 1 epoch 69.02880859375 sec\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.0138\n",
      "Epoch 34 Batch 100 Loss 0.0381\n",
      "Epoch 34 Batch 200 Loss 0.0442\n",
      "Epoch 34 Batch 300 Loss 0.0547\n",
      "Epoch 34 Batch 400 Loss 0.0783\n",
      "Epoch 34 Batch 500 Loss 0.0681\n",
      "Epoch 34 Batch 600 Loss 0.0621\n",
      "Epoch 34 Batch 700 Loss 0.0383\n",
      "Epoch 34 Batch 800 Loss 0.0299\n",
      "Epoch 34 Batch 900 Loss 0.0581\n",
      "Epoch 34 Batch 1000 Loss 0.0436\n",
      "Epoch 34 Batch 1100 Loss 0.0535\n",
      "Epoch 34 Batch 1200 Loss 0.0562\n",
      "Epoch 34 Loss 0.0489\n",
      "Time taken for 1 epoch 71.02941942214966 sec\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.0532\n",
      "Epoch 35 Batch 100 Loss 0.0289\n",
      "Epoch 35 Batch 200 Loss 0.0422\n",
      "Epoch 35 Batch 300 Loss 0.0461\n",
      "Epoch 35 Batch 400 Loss 0.0389\n",
      "Epoch 35 Batch 500 Loss 0.0324\n",
      "Epoch 35 Batch 600 Loss 0.0723\n",
      "Epoch 35 Batch 700 Loss 0.0697\n",
      "Epoch 35 Batch 800 Loss 0.0326\n",
      "Epoch 35 Batch 900 Loss 0.0636\n",
      "Epoch 35 Batch 1000 Loss 0.0690\n",
      "Epoch 35 Batch 1100 Loss 0.1054\n",
      "Epoch 35 Batch 1200 Loss 0.0554\n",
      "Epoch 35 Loss 0.0482\n",
      "Time taken for 1 epoch 69.01102709770203 sec\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.0642\n",
      "Epoch 36 Batch 100 Loss 0.0575\n",
      "Epoch 36 Batch 200 Loss 0.0295\n",
      "Epoch 36 Batch 300 Loss 0.0310\n",
      "Epoch 36 Batch 400 Loss 0.0185\n",
      "Epoch 36 Batch 500 Loss 0.0366\n",
      "Epoch 36 Batch 600 Loss 0.0513\n",
      "Epoch 36 Batch 700 Loss 0.0552\n",
      "Epoch 36 Batch 800 Loss 0.0412\n",
      "Epoch 36 Batch 900 Loss 0.0609\n",
      "Epoch 36 Batch 1000 Loss 0.0458\n",
      "Epoch 36 Batch 1100 Loss 0.0468\n",
      "Epoch 36 Batch 1200 Loss 0.0507\n",
      "Epoch 36 Loss 0.0475\n",
      "Time taken for 1 epoch 70.93268871307373 sec\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.0310\n",
      "Epoch 37 Batch 100 Loss 0.0265\n",
      "Epoch 37 Batch 200 Loss 0.0216\n",
      "Epoch 37 Batch 300 Loss 0.0362\n",
      "Epoch 37 Batch 400 Loss 0.0285\n",
      "Epoch 37 Batch 500 Loss 0.0424\n",
      "Epoch 37 Batch 600 Loss 0.0334\n",
      "Epoch 37 Batch 700 Loss 0.0707\n",
      "Epoch 37 Batch 800 Loss 0.0590\n",
      "Epoch 37 Batch 900 Loss 0.0328\n",
      "Epoch 37 Batch 1000 Loss 0.0607\n",
      "Epoch 37 Batch 1100 Loss 0.0356\n",
      "Epoch 37 Batch 1200 Loss 0.0481\n",
      "Epoch 37 Loss 0.0474\n",
      "Time taken for 1 epoch 69.01634311676025 sec\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.0388\n",
      "Epoch 38 Batch 100 Loss 0.0327\n",
      "Epoch 38 Batch 200 Loss 0.0264\n",
      "Epoch 38 Batch 300 Loss 0.0555\n",
      "Epoch 38 Batch 400 Loss 0.0384\n",
      "Epoch 38 Batch 500 Loss 0.0444\n",
      "Epoch 38 Batch 600 Loss 0.0703\n",
      "Epoch 38 Batch 700 Loss 0.0645\n",
      "Epoch 38 Batch 800 Loss 0.0468\n",
      "Epoch 38 Batch 900 Loss 0.0308\n",
      "Epoch 38 Batch 1000 Loss 0.0788\n",
      "Epoch 38 Batch 1100 Loss 0.0491\n",
      "Epoch 38 Batch 1200 Loss 0.0385\n",
      "Epoch 38 Loss 0.0472\n",
      "Time taken for 1 epoch 70.92147374153137 sec\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.0312\n",
      "Epoch 39 Batch 100 Loss 0.0251\n",
      "Epoch 39 Batch 200 Loss 0.0253\n",
      "Epoch 39 Batch 300 Loss 0.0221\n",
      "Epoch 39 Batch 400 Loss 0.0147\n",
      "Epoch 39 Batch 500 Loss 0.0211\n",
      "Epoch 39 Batch 600 Loss 0.0560\n",
      "Epoch 39 Batch 700 Loss 0.0405\n",
      "Epoch 39 Batch 800 Loss 0.0592\n",
      "Epoch 39 Batch 900 Loss 0.0572\n",
      "Epoch 39 Batch 1000 Loss 0.0601\n",
      "Epoch 39 Batch 1100 Loss 0.0660\n",
      "Epoch 39 Batch 1200 Loss 0.0667\n",
      "Epoch 39 Loss 0.0459\n",
      "Time taken for 1 epoch 69.02273678779602 sec\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.0245\n",
      "Epoch 40 Batch 100 Loss 0.0345\n",
      "Epoch 40 Batch 200 Loss 0.0353\n",
      "Epoch 40 Batch 300 Loss 0.0472\n",
      "Epoch 40 Batch 400 Loss 0.0298\n",
      "Epoch 40 Batch 500 Loss 0.0520\n",
      "Epoch 40 Batch 600 Loss 0.0422\n",
      "Epoch 40 Batch 700 Loss 0.0723\n",
      "Epoch 40 Batch 800 Loss 0.0597\n",
      "Epoch 40 Batch 900 Loss 0.0310\n",
      "Epoch 40 Batch 1000 Loss 0.0525\n",
      "Epoch 40 Batch 1100 Loss 0.0400\n",
      "Epoch 40 Batch 1200 Loss 0.0530\n",
      "Epoch 40 Loss 0.0459\n",
      "Time taken for 1 epoch 71.02414536476135 sec\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.0184\n",
      "Epoch 41 Batch 100 Loss 0.0412\n",
      "Epoch 41 Batch 200 Loss 0.0571\n",
      "Epoch 41 Batch 300 Loss 0.0568\n",
      "Epoch 41 Batch 400 Loss 0.0433\n",
      "Epoch 41 Batch 500 Loss 0.0664\n",
      "Epoch 41 Batch 600 Loss 0.0361\n",
      "Epoch 41 Batch 700 Loss 0.0277\n",
      "Epoch 41 Batch 800 Loss 0.0475\n",
      "Epoch 41 Batch 900 Loss 0.0216\n",
      "Epoch 41 Batch 1000 Loss 0.0570\n",
      "Epoch 41 Batch 1100 Loss 0.0705\n",
      "Epoch 41 Batch 1200 Loss 0.0300\n",
      "Epoch 41 Loss 0.0452\n",
      "Time taken for 1 epoch 69.03675174713135 sec\n",
      "\n",
      "Epoch 42 Batch 0 Loss 0.0327\n",
      "Epoch 42 Batch 100 Loss 0.0283\n",
      "Epoch 42 Batch 200 Loss 0.0626\n",
      "Epoch 42 Batch 300 Loss 0.0236\n",
      "Epoch 42 Batch 400 Loss 0.0569\n",
      "Epoch 42 Batch 500 Loss 0.0642\n",
      "Epoch 42 Batch 600 Loss 0.0484\n",
      "Epoch 42 Batch 700 Loss 0.0233\n",
      "Epoch 42 Batch 800 Loss 0.0722\n",
      "Epoch 42 Batch 900 Loss 0.0380\n",
      "Epoch 42 Batch 1000 Loss 0.0931\n",
      "Epoch 42 Batch 1100 Loss 0.1011\n",
      "Epoch 42 Batch 1200 Loss 0.0290\n",
      "Epoch 42 Loss 0.0453\n",
      "Time taken for 1 epoch 71.01133298873901 sec\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.0178\n",
      "Epoch 43 Batch 100 Loss 0.0429\n",
      "Epoch 43 Batch 200 Loss 0.0244\n",
      "Epoch 43 Batch 300 Loss 0.0494\n",
      "Epoch 43 Batch 400 Loss 0.0351\n",
      "Epoch 43 Batch 500 Loss 0.0213\n",
      "Epoch 43 Batch 600 Loss 0.0277\n",
      "Epoch 43 Batch 700 Loss 0.0453\n",
      "Epoch 43 Batch 800 Loss 0.0291\n",
      "Epoch 43 Batch 900 Loss 0.0267\n",
      "Epoch 43 Batch 1000 Loss 0.0666\n",
      "Epoch 43 Batch 1100 Loss 0.0676\n",
      "Epoch 43 Batch 1200 Loss 0.0669\n",
      "Epoch 43 Loss 0.0445\n",
      "Time taken for 1 epoch 69.04731273651123 sec\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.0397\n",
      "Epoch 44 Batch 100 Loss 0.0423\n",
      "Epoch 44 Batch 200 Loss 0.0376\n",
      "Epoch 44 Batch 300 Loss 0.0377\n",
      "Epoch 44 Batch 400 Loss 0.0365\n",
      "Epoch 44 Batch 500 Loss 0.0377\n",
      "Epoch 44 Batch 600 Loss 0.0628\n",
      "Epoch 44 Batch 700 Loss 0.0536\n",
      "Epoch 44 Batch 800 Loss 0.0564\n",
      "Epoch 44 Batch 900 Loss 0.0420\n",
      "Epoch 44 Batch 1000 Loss 0.0339\n",
      "Epoch 44 Batch 1100 Loss 0.0550\n",
      "Epoch 44 Batch 1200 Loss 0.0369\n",
      "Epoch 44 Loss 0.0444\n",
      "Time taken for 1 epoch 70.86043286323547 sec\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.0479\n",
      "Epoch 45 Batch 100 Loss 0.0287\n",
      "Epoch 45 Batch 200 Loss 0.0370\n",
      "Epoch 45 Batch 300 Loss 0.0529\n",
      "Epoch 45 Batch 400 Loss 0.0213\n",
      "Epoch 45 Batch 500 Loss 0.0221\n",
      "Epoch 45 Batch 600 Loss 0.0317\n",
      "Epoch 45 Batch 700 Loss 0.0208\n",
      "Epoch 45 Batch 800 Loss 0.0434\n",
      "Epoch 45 Batch 900 Loss 0.0330\n",
      "Epoch 45 Batch 1000 Loss 0.0535\n",
      "Epoch 45 Batch 1100 Loss 0.0526\n",
      "Epoch 45 Batch 1200 Loss 0.0548\n",
      "Epoch 45 Loss 0.0446\n",
      "Time taken for 1 epoch 69.07174921035767 sec\n",
      "\n",
      "Epoch 46 Batch 0 Loss 0.0287\n",
      "Epoch 46 Batch 100 Loss 0.0273\n",
      "Epoch 46 Batch 200 Loss 0.0455\n",
      "Epoch 46 Batch 300 Loss 0.0522\n",
      "Epoch 46 Batch 400 Loss 0.0465\n",
      "Epoch 46 Batch 500 Loss 0.0394\n",
      "Epoch 46 Batch 600 Loss 0.0338\n",
      "Epoch 46 Batch 700 Loss 0.0385\n",
      "Epoch 46 Batch 800 Loss 0.0537\n",
      "Epoch 46 Batch 900 Loss 0.0481\n",
      "Epoch 46 Batch 1000 Loss 0.0532\n",
      "Epoch 46 Batch 1100 Loss 0.0502\n",
      "Epoch 46 Batch 1200 Loss 0.0420\n",
      "Epoch 46 Loss 0.0435\n",
      "Time taken for 1 epoch 70.86570763587952 sec\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.0209\n",
      "Epoch 47 Batch 100 Loss 0.0227\n",
      "Epoch 47 Batch 200 Loss 0.0498\n",
      "Epoch 47 Batch 300 Loss 0.0249\n",
      "Epoch 47 Batch 400 Loss 0.0424\n",
      "Epoch 47 Batch 500 Loss 0.0260\n",
      "Epoch 47 Batch 600 Loss 0.0270\n",
      "Epoch 47 Batch 700 Loss 0.0485\n",
      "Epoch 47 Batch 800 Loss 0.0406\n",
      "Epoch 47 Batch 900 Loss 0.0796\n",
      "Epoch 47 Batch 1000 Loss 0.0544\n",
      "Epoch 47 Batch 1100 Loss 0.0327\n",
      "Epoch 47 Batch 1200 Loss 0.0406\n",
      "Epoch 47 Loss 0.0432\n",
      "Time taken for 1 epoch 69.03158044815063 sec\n",
      "\n",
      "Epoch 48 Batch 0 Loss 0.0273\n",
      "Epoch 48 Batch 100 Loss 0.0310\n",
      "Epoch 48 Batch 200 Loss 0.0330\n",
      "Epoch 48 Batch 300 Loss 0.0370\n",
      "Epoch 48 Batch 400 Loss 0.0341\n",
      "Epoch 48 Batch 500 Loss 0.0420\n",
      "Epoch 48 Batch 600 Loss 0.0485\n",
      "Epoch 48 Batch 700 Loss 0.0300\n",
      "Epoch 48 Batch 800 Loss 0.0258\n",
      "Epoch 48 Batch 900 Loss 0.0718\n",
      "Epoch 48 Batch 1000 Loss 0.0605\n",
      "Epoch 48 Batch 1100 Loss 0.0746\n",
      "Epoch 48 Batch 1200 Loss 0.0341\n",
      "Epoch 48 Loss 0.0434\n",
      "Time taken for 1 epoch 71.09084844589233 sec\n",
      "\n",
      "Epoch 49 Batch 0 Loss 0.0240\n",
      "Epoch 49 Batch 100 Loss 0.0207\n",
      "Epoch 49 Batch 200 Loss 0.0453\n",
      "Epoch 49 Batch 300 Loss 0.0246\n",
      "Epoch 49 Batch 400 Loss 0.0286\n",
      "Epoch 49 Batch 500 Loss 0.0470\n",
      "Epoch 49 Batch 600 Loss 0.0416\n",
      "Epoch 49 Batch 700 Loss 0.0359\n",
      "Epoch 49 Batch 800 Loss 0.0514\n",
      "Epoch 49 Batch 900 Loss 0.0416\n",
      "Epoch 49 Batch 1000 Loss 0.0751\n",
      "Epoch 49 Batch 1100 Loss 0.0317\n",
      "Epoch 49 Batch 1200 Loss 0.0586\n",
      "Epoch 49 Loss 0.0431\n",
      "Time taken for 1 epoch 69.02157711982727 sec\n",
      "\n",
      "Epoch 50 Batch 0 Loss 0.0242\n",
      "Epoch 50 Batch 100 Loss 0.0252\n",
      "Epoch 50 Batch 200 Loss 0.0281\n",
      "Epoch 50 Batch 300 Loss 0.0309\n",
      "Epoch 50 Batch 400 Loss 0.0222\n",
      "Epoch 50 Batch 500 Loss 0.0500\n",
      "Epoch 50 Batch 600 Loss 0.0424\n",
      "Epoch 50 Batch 700 Loss 0.0640\n",
      "Epoch 50 Batch 800 Loss 0.0579\n",
      "Epoch 50 Batch 900 Loss 0.0389\n",
      "Epoch 50 Batch 1000 Loss 0.0386\n",
      "Epoch 50 Batch 1100 Loss 0.0757\n",
      "Epoch 50 Batch 1200 Loss 0.0400\n",
      "Epoch 50 Loss 0.0426\n",
      "Time taken for 1 epoch 71.12205839157104 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!g2.mig\n",
    "EPOCHS = 50\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "  # Инициализируем входное скрытое состояние (из нулей) размера (батч, кол-во рекуррентных ячеек)\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        # Делаем шаг обучения. находим оштбку за этоху\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        # Считаем ошибку\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                       batch,\n",
    "                                                       batch_loss.numpy()))\n",
    "  # Сохраняем контрольные точки каждые 2 эпохи\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "pfzkvgcmfuhkn16eihzma",
    "execution_id": "ade7bc34-57ad-4709-9c6c-3a36d69f2aeb",
    "id": "1v35cvDbtoqC"
   },
   "source": [
    "### Перевод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "krzzued6jr7z2jdy6vq3v",
    "execution_id": "fd90a001-4e63-48d3-8d38-3d952501e7d5",
    "id": "xV1I1Q_ltrlP"
   },
   "source": [
    "* Функция оценки аналогична циклу обучения, за исключением того, что здесь мы не используем teacher forcing. Входным сигналом для декодера на каждом временном шаге являются его предыдущие предсказания вместе со скрытым состоянием и выходным сигналом энсодера.\n",
    "* Предсказания модели прекращаются, когда модель предскажет end token.\n",
    "* Сохраняем веса внимания для каждого временного шага.\n",
    "\n",
    "Примечание: Выходной сигнал энкодера вычисляется только один раз для одного входа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cellId": "19a8irg164x6wuw5wq5oby",
    "execution_id": "bb6d1268-3f85-4dc6-8911-e3c7f5c250af",
    "id": "-3_2urw6tzrR"
   },
   "outputs": [],
   "source": [
    "#!g2.mig\n",
    "def evaluate(sentence):\n",
    "  # Препоцессим предложение\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "      # Разбиваем предложение по пробелам и составляем список индексов каждого слова\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "      # Добиваем inputs нулями справа до максимальной длины входного текста\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                             maxlen=max_length_inp,\n",
    "                                                             padding='post')\n",
    "      # Преобразуем inputs в тензор\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "      # Инициализируем входной хидден из нулей размера (1, units)\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "      # Подаем inputs и hidden в encoder\n",
    "    enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "      # Инициализируем входной хидден декодера -- выходной хидден энкодера\n",
    "    dec_hidden = enc_hidden\n",
    "      # Вход декодера -- список [индекс start] размера(1,1)\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "            # Получаем выход декодера\n",
    "        predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "   # Заканчиваем на токене end\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence\n",
    "\n",
    "    # Предсказанный predicted ID подаем обратно в декодер (размер (1,1))\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cellId": "2udgvn7plorv4fh5oyznic",
    "execution_id": "5262e8d7-86fb-4f61-8412-614dec2e368b",
    "id": "_lZv63DSuBoU"
   },
   "outputs": [],
   "source": [
    "#!g2.mig\n",
    "# Функция перевода\n",
    "def translate(sentence):\n",
    "    result, sentence = evaluate(sentence)\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "cellId": "q9l6vtsy27g6zn3wdnudwp",
    "execution_id": "4172d2de-f3ff-4a65-96d1-9e3a9ff3bebf",
    "id": "fX9SPdZ2uEG4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f3ddf7eefa0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.mig\n",
    "# Загружаем последний checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "cellId": "jft74ukdkrs20r6afooszj",
    "execution_id": "432b7b27-80b1-4de1-b78b-c6fc52b3a54b",
    "id": "Ag_K7PQHuGsU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> хорошая погода <end>\n",
      "Predicted translation: this is a good cold . <end> \n"
     ]
    }
   ],
   "source": [
    "#!g2.mig\n",
    "translate('Хорошая погода')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "cellId": "6mj9lvhkol55n58cmeyop4",
    "execution_id": "7a2563b4-1ed1-4335-80c1-54ac194f3a0f",
    "id": "sGUj1hKouIy1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> я иду в школу <end>\n",
      "Predicted translation: i walk to school . <end> \n"
     ]
    }
   ],
   "source": [
    "#!g2.mig\n",
    "translate('Я иду в школу')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "cellId": "5a1mtl4cz1o2txa7ewjtpg",
    "execution_id": "d3b86c06-e40a-4c1c-9f73-559101c347d1",
    "id": "fcfaz5AVuMqn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> мама приготовила вкусный обед для нашей большой семьи <end>\n",
      "Predicted translation: my dog works quickly . <end> \n"
     ]
    }
   ],
   "source": [
    "#!g2.mig\n",
    "translate('Мама приготовила вкусный обед для нашей большой семьи')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "cellId": "co5j993pv89rwno1jge8m",
    "execution_id": "44415a77-1596-473a-aa0d-6f556b80a206",
    "id": "kPEyePttuQE-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> мне здесь нравится , но я очень скучаю по дому . там остались мама и папа . <end>\n",
      "Predicted translation: i get your point . <end> \n"
     ]
    }
   ],
   "source": [
    "#!g2.mig\n",
    "translate(u'Мне здесь нравится, но я очень скучаю по дому. Там остались мама и папа.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "vfdl7vtfyjdtph6fyes12",
    "execution_id": "28f99055-bae6-4da2-8e6d-5f44579f20ed",
    "id": "0JeyF4wvuUIG"
   },
   "source": [
    "**Вывод:** на длинных предложениях перевод хуже, что может быть связано с отсутствием учета контекста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "hdvv615g8ev3sagyja4deo",
    "execution_id": "86a5e0b6-3d7f-4296-adb6-11f533728cf2",
    "id": "v3O4f0bXugjX"
   },
   "source": [
    "## Построение модели машинного перевода с вниманием"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "5airy07rq4xkc1x00hgjzr",
    "execution_id": "6d711e49-b2ff-4ea2-b863-483212a1db95",
    "id": "A4md-8mpulWx"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "cellId": "rpsxlsvgetp5rm66b67ft2",
    "execution_id": "79df242d-cef2-4aaf-aa09-d7c047193666",
    "id": "W2v9Zy6Fujj7"
   },
   "outputs": [],
   "source": [
    "#!g2.mig\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "cellId": "2j15289wk6mls2f0pq2sja",
    "execution_id": "f1202e4f-e465-45fc-9a41-173c23ef6d0c",
    "id": "aRMYvrKFup7g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 15, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "#!g2.mig\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# Инициализируем начальное скрытое состояние из нулей\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "# Получаем выход энкодера и последнее скрытое состояние\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "f6qlk4zvq6jf1qlsn9zsrg",
    "execution_id": "eaab2a6d-cc4e-46e1-9ebb-4bf875030c3c",
    "id": "fRwmN20Vu1RP"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "cellId": "88wf17cyzm2xexjb0xow0l",
    "execution_id": "97fa9aa1-8243-443c-adb8-ebc0683fe2b1",
    "id": "Obrtwp9ou0OU"
   },
   "outputs": [],
   "source": [
    "#!g2.mig\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # Query hidden state shape == (batch_size, hidden size)\n",
    "        # Values shape == (batch_size, max_len, hidden size)\n",
    "\n",
    "        # Query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # Score shape == (batch_size, max_length, 1)\n",
    "        # We get 1 at the last axis because we are applying score to self.V\n",
    "        # The shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        \n",
    "        # Применяем к векторам скрытого состояния и выходов энкодера полносвязный слой (выход (batch_size, 1, units) и (batch_size, max_length, units))\n",
    "        # Складываем полученные векторы, применяем к сумму тангенс выход (batch_size, max_length, units)\n",
    "        # Проводим результат через dense слой выход (batch_size, max_length, 1)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # Attention_weights shape == (batch_size, max_length, 1)\n",
    "        # Получаем вероятностное распределение\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # Context_vector shape after sum == (batch_size, hidden_size)\n",
    "        # Умножаем веса внимания умножаем на векторы значенй выход (batch_size, max_len, hidden size)\n",
    "        context_vector = attention_weights * values\n",
    "        # Находим вдоль столбцов (batch_size, hidden_size)\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "cellId": "eervg1fnqtvyfu4lqsp9z",
    "execution_id": "6957d6b4-338f-4b30-8ff0-85762cf17831",
    "id": "ASqE2_7mvKF0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 15, 1)\n"
     ]
    }
   ],
   "source": [
    "#!g2.mig\n",
    "# Создаем слой внимания\n",
    "attention_layer = BahdanauAttention(10)\n",
    "# Передаем выход энкодера и его скрытое состояние\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "cellId": "frq1fn1c2gqty7cpcycdqi",
    "execution_id": "b9b1c50b-eaea-4a7e-8864-75aaa014cdff",
    "id": "wUY7bR-AvRb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 1024), dtype=float32, numpy=\n",
       "array([[-0.00095868,  0.0037712 ,  0.00030837, ..., -0.01048669,\n",
       "        -0.00051526, -0.00482616],\n",
       "       [-0.00041407,  0.00849887, -0.00266971, ..., -0.01305983,\n",
       "        -0.00457067, -0.00124785],\n",
       "       [-0.00046182,  0.00472675, -0.00431802, ..., -0.01144347,\n",
       "        -0.00294155, -0.00372045],\n",
       "       ...,\n",
       "       [-0.00211136,  0.00707744, -0.00192837, ..., -0.01122575,\n",
       "         0.00064262, -0.00166406],\n",
       "       [ 0.0014904 ,  0.00625178, -0.00196154, ..., -0.01102399,\n",
       "        -0.00055026, -0.00374676],\n",
       "       [ 0.00312438,  0.00319219,  0.00135375, ..., -0.00911092,\n",
       "        -0.00023989, -0.00515693]], dtype=float32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.mig\n",
    "# Вектор контекста\n",
    "attention_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "cellId": "tw7zfjfjmp95lvdwuvcahg",
    "execution_id": "023e88ad-b9fb-4b92-90c2-02559150df08",
    "id": "1nf5SL7DvVFM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 15, 1), dtype=float32, numpy=\n",
       "array([[[0.06607861],\n",
       "        [0.06707446],\n",
       "        [0.0673137 ],\n",
       "        [0.06809489],\n",
       "        [0.06647553],\n",
       "        [0.06607323],\n",
       "        [0.06739506],\n",
       "        [0.06709792],\n",
       "        [0.06706234],\n",
       "        [0.06682284],\n",
       "        [0.06654294],\n",
       "        [0.06628067],\n",
       "        [0.06605876],\n",
       "        [0.0658818 ],\n",
       "        [0.06574725]],\n",
       "\n",
       "       [[0.06624756],\n",
       "        [0.06704309],\n",
       "        [0.06631398],\n",
       "        [0.06729461],\n",
       "        [0.06651884],\n",
       "        [0.06794392],\n",
       "        [0.06749769],\n",
       "        [0.06731556],\n",
       "        [0.06697959],\n",
       "        [0.06665399],\n",
       "        [0.06637914],\n",
       "        [0.06616081],\n",
       "        [0.06599401],\n",
       "        [0.06587204],\n",
       "        [0.06578515]],\n",
       "\n",
       "       [[0.06591919],\n",
       "        [0.06730799],\n",
       "        [0.0671017 ],\n",
       "        [0.06818235],\n",
       "        [0.06874469],\n",
       "        [0.06782924],\n",
       "        [0.06738321],\n",
       "        [0.0668905 ],\n",
       "        [0.06647083],\n",
       "        [0.06613895],\n",
       "        [0.06588686],\n",
       "        [0.06569983],\n",
       "        [0.0655652 ],\n",
       "        [0.06547178],\n",
       "        [0.06540767]],\n",
       "\n",
       "       [[0.06652966],\n",
       "        [0.06546178],\n",
       "        [0.06680559],\n",
       "        [0.06531513],\n",
       "        [0.06728515],\n",
       "        [0.06809215],\n",
       "        [0.06754567],\n",
       "        [0.06740727],\n",
       "        [0.06713913],\n",
       "        [0.06686612],\n",
       "        [0.06662693],\n",
       "        [0.06642878],\n",
       "        [0.06627256],\n",
       "        [0.06615526],\n",
       "        [0.06606881]],\n",
       "\n",
       "       [[0.06658128],\n",
       "        [0.0661604 ],\n",
       "        [0.06583463],\n",
       "        [0.06769019],\n",
       "        [0.06756611],\n",
       "        [0.06757177],\n",
       "        [0.06733457],\n",
       "        [0.06704941],\n",
       "        [0.06678278],\n",
       "        [0.06655704],\n",
       "        [0.0663776 ],\n",
       "        [0.06624289],\n",
       "        [0.06614579],\n",
       "        [0.06607611],\n",
       "        [0.06602943]],\n",
       "\n",
       "       [[0.06639569],\n",
       "        [0.06577615],\n",
       "        [0.06535003],\n",
       "        [0.0663529 ],\n",
       "        [0.06740768],\n",
       "        [0.06826369],\n",
       "        [0.06769367],\n",
       "        [0.06751484],\n",
       "        [0.06719719],\n",
       "        [0.06687801],\n",
       "        [0.06659933],\n",
       "        [0.0663692 ],\n",
       "        [0.06618889],\n",
       "        [0.06605483],\n",
       "        [0.06595787]],\n",
       "\n",
       "       [[0.06638445],\n",
       "        [0.06576501],\n",
       "        [0.065947  ],\n",
       "        [0.06723502],\n",
       "        [0.06823972],\n",
       "        [0.06770257],\n",
       "        [0.06752717],\n",
       "        [0.0672107 ],\n",
       "        [0.0668918 ],\n",
       "        [0.06661095],\n",
       "        [0.06637965],\n",
       "        [0.06619732],\n",
       "        [0.06605963],\n",
       "        [0.0659596 ],\n",
       "        [0.06588939]],\n",
       "\n",
       "       [[0.06617717],\n",
       "        [0.06759901],\n",
       "        [0.06693079],\n",
       "        [0.0672541 ],\n",
       "        [0.06607969],\n",
       "        [0.06657666],\n",
       "        [0.06764767],\n",
       "        [0.06718101],\n",
       "        [0.06706648],\n",
       "        [0.06680743],\n",
       "        [0.06653875],\n",
       "        [0.0662962 ],\n",
       "        [0.06609573],\n",
       "        [0.06593543],\n",
       "        [0.06581388]],\n",
       "\n",
       "       [[0.06671838],\n",
       "        [0.06564748],\n",
       "        [0.06583159],\n",
       "        [0.06744891],\n",
       "        [0.06738862],\n",
       "        [0.06749686],\n",
       "        [0.06733614],\n",
       "        [0.06710178],\n",
       "        [0.06686666],\n",
       "        [0.06665999],\n",
       "        [0.06649407],\n",
       "        [0.06636632],\n",
       "        [0.06627306],\n",
       "        [0.06620809],\n",
       "        [0.06616208]],\n",
       "\n",
       "       [[0.06625462],\n",
       "        [0.06749178],\n",
       "        [0.06741104],\n",
       "        [0.06699056],\n",
       "        [0.06785214],\n",
       "        [0.06730545],\n",
       "        [0.06716148],\n",
       "        [0.06689091],\n",
       "        [0.06661738],\n",
       "        [0.06637694],\n",
       "        [0.06617614],\n",
       "        [0.06601839],\n",
       "        [0.06589767],\n",
       "        [0.06580927],\n",
       "        [0.06574626]],\n",
       "\n",
       "       [[0.06638469],\n",
       "        [0.06673435],\n",
       "        [0.06611837],\n",
       "        [0.06696772],\n",
       "        [0.06806538],\n",
       "        [0.0675528 ],\n",
       "        [0.0673967 ],\n",
       "        [0.06710093],\n",
       "        [0.06680323],\n",
       "        [0.06654286],\n",
       "        [0.06632776],\n",
       "        [0.06615872],\n",
       "        [0.0660324 ],\n",
       "        [0.06593901],\n",
       "        [0.06587508]],\n",
       "\n",
       "       [[0.06666043],\n",
       "        [0.06679062],\n",
       "        [0.06612662],\n",
       "        [0.0663146 ],\n",
       "        [0.06630228],\n",
       "        [0.06703079],\n",
       "        [0.06744212],\n",
       "        [0.06738199],\n",
       "        [0.06716039],\n",
       "        [0.06690545],\n",
       "        [0.06667276],\n",
       "        [0.06648325],\n",
       "        [0.06633782],\n",
       "        [0.06623223],\n",
       "        [0.06615866]],\n",
       "\n",
       "       [[0.06620359],\n",
       "        [0.06709838],\n",
       "        [0.06670873],\n",
       "        [0.06703363],\n",
       "        [0.0671084 ],\n",
       "        [0.06798216],\n",
       "        [0.06736899],\n",
       "        [0.06716215],\n",
       "        [0.06685097],\n",
       "        [0.06655546],\n",
       "        [0.0663056 ],\n",
       "        [0.06610317],\n",
       "        [0.06594571],\n",
       "        [0.06582917],\n",
       "        [0.06574386]],\n",
       "\n",
       "       [[0.06667788],\n",
       "        [0.06605571],\n",
       "        [0.06583506],\n",
       "        [0.06537106],\n",
       "        [0.06747934],\n",
       "        [0.06752692],\n",
       "        [0.06763094],\n",
       "        [0.06743342],\n",
       "        [0.06716041],\n",
       "        [0.06689589],\n",
       "        [0.06666843],\n",
       "        [0.06648656],\n",
       "        [0.06635021],\n",
       "        [0.06624897],\n",
       "        [0.06617919]],\n",
       "\n",
       "       [[0.06606158],\n",
       "        [0.06677956],\n",
       "        [0.06723452],\n",
       "        [0.06637084],\n",
       "        [0.06714918],\n",
       "        [0.0682109 ],\n",
       "        [0.06761672],\n",
       "        [0.06736742],\n",
       "        [0.06698692],\n",
       "        [0.06662169],\n",
       "        [0.06631042],\n",
       "        [0.0660618 ],\n",
       "        [0.06586988],\n",
       "        [0.06572945],\n",
       "        [0.06562913]],\n",
       "\n",
       "       [[0.06659975],\n",
       "        [0.06586372],\n",
       "        [0.06597539],\n",
       "        [0.06776587],\n",
       "        [0.06759061],\n",
       "        [0.06757579],\n",
       "        [0.06732837],\n",
       "        [0.06704186],\n",
       "        [0.0667773 ],\n",
       "        [0.06655607],\n",
       "        [0.06638113],\n",
       "        [0.06625149],\n",
       "        [0.06615618],\n",
       "        [0.06609034],\n",
       "        [0.06604613]],\n",
       "\n",
       "       [[0.06638341],\n",
       "        [0.06728062],\n",
       "        [0.06618616],\n",
       "        [0.06683551],\n",
       "        [0.0654546 ],\n",
       "        [0.06765208],\n",
       "        [0.06755319],\n",
       "        [0.06750111],\n",
       "        [0.06720154],\n",
       "        [0.06687398],\n",
       "        [0.06658488],\n",
       "        [0.0663503 ],\n",
       "        [0.0661689 ],\n",
       "        [0.06603504],\n",
       "        [0.0659387 ]],\n",
       "\n",
       "       [[0.06670386],\n",
       "        [0.06563319],\n",
       "        [0.06767492],\n",
       "        [0.06529549],\n",
       "        [0.06724634],\n",
       "        [0.06723817],\n",
       "        [0.067357  ],\n",
       "        [0.06721374],\n",
       "        [0.06700388],\n",
       "        [0.06679267],\n",
       "        [0.06660702],\n",
       "        [0.06645539],\n",
       "        [0.06633817],\n",
       "        [0.06625094],\n",
       "        [0.06618924]],\n",
       "\n",
       "       [[0.06646198],\n",
       "        [0.06584182],\n",
       "        [0.06570455],\n",
       "        [0.06711795],\n",
       "        [0.06816023],\n",
       "        [0.06767699],\n",
       "        [0.06753043],\n",
       "        [0.06722584],\n",
       "        [0.06691301],\n",
       "        [0.06663832],\n",
       "        [0.06641354],\n",
       "        [0.06623918],\n",
       "        [0.06610923],\n",
       "        [0.066016  ],\n",
       "        [0.06595093]],\n",
       "\n",
       "       [[0.06667399],\n",
       "        [0.06757511],\n",
       "        [0.06752679],\n",
       "        [0.06629271],\n",
       "        [0.06661685],\n",
       "        [0.06697641],\n",
       "        [0.06699437],\n",
       "        [0.06687827],\n",
       "        [0.06671663],\n",
       "        [0.06655423],\n",
       "        [0.06641332],\n",
       "        [0.06630009],\n",
       "        [0.06621481],\n",
       "        [0.06615438],\n",
       "        [0.06611204]],\n",
       "\n",
       "       [[0.0664143 ],\n",
       "        [0.06534828],\n",
       "        [0.06475467],\n",
       "        [0.06709872],\n",
       "        [0.06755348],\n",
       "        [0.06830634],\n",
       "        [0.06766733],\n",
       "        [0.06747768],\n",
       "        [0.06717771],\n",
       "        [0.06688117],\n",
       "        [0.06661878],\n",
       "        [0.066399  ],\n",
       "        [0.0662228 ],\n",
       "        [0.06608892],\n",
       "        [0.06599075]],\n",
       "\n",
       "       [[0.06587775],\n",
       "        [0.06701728],\n",
       "        [0.0660442 ],\n",
       "        [0.0675799 ],\n",
       "        [0.06685051],\n",
       "        [0.06848656],\n",
       "        [0.0680524 ],\n",
       "        [0.06646105],\n",
       "        [0.06654545],\n",
       "        [0.06669512],\n",
       "        [0.06654269],\n",
       "        [0.06630015],\n",
       "        [0.06605143],\n",
       "        [0.06583462],\n",
       "        [0.06566088]],\n",
       "\n",
       "       [[0.06658246],\n",
       "        [0.06622533],\n",
       "        [0.06692421],\n",
       "        [0.06753886],\n",
       "        [0.06640055],\n",
       "        [0.06468943],\n",
       "        [0.06708872],\n",
       "        [0.0672336 ],\n",
       "        [0.06737027],\n",
       "        [0.06719875],\n",
       "        [0.06695333],\n",
       "        [0.06671394],\n",
       "        [0.06650888],\n",
       "        [0.06634749],\n",
       "        [0.06622418]],\n",
       "\n",
       "       [[0.06646913],\n",
       "        [0.06584891],\n",
       "        [0.06720361],\n",
       "        [0.06805655],\n",
       "        [0.06755415],\n",
       "        [0.06744015],\n",
       "        [0.06717806],\n",
       "        [0.06689674],\n",
       "        [0.06664076],\n",
       "        [0.06642549],\n",
       "        [0.06625486],\n",
       "        [0.06612401],\n",
       "        [0.06602837],\n",
       "        [0.06596218],\n",
       "        [0.06591704]],\n",
       "\n",
       "       [[0.06616177],\n",
       "        [0.06650486],\n",
       "        [0.06648895],\n",
       "        [0.06782843],\n",
       "        [0.06845368],\n",
       "        [0.06769255],\n",
       "        [0.06739981],\n",
       "        [0.06702831],\n",
       "        [0.06668483],\n",
       "        [0.06639327],\n",
       "        [0.06615685],\n",
       "        [0.06597287],\n",
       "        [0.06583445],\n",
       "        [0.06573451],\n",
       "        [0.06566483]],\n",
       "\n",
       "       [[0.0662904 ],\n",
       "        [0.06567183],\n",
       "        [0.06650861],\n",
       "        [0.06774801],\n",
       "        [0.06670922],\n",
       "        [0.06790178],\n",
       "        [0.06745189],\n",
       "        [0.06732043],\n",
       "        [0.06703448],\n",
       "        [0.06673773],\n",
       "        [0.06647478],\n",
       "        [0.06625591],\n",
       "        [0.06608374],\n",
       "        [0.06595359],\n",
       "        [0.06585763]],\n",
       "\n",
       "       [[0.06597519],\n",
       "        [0.06535955],\n",
       "        [0.06635692],\n",
       "        [0.06714547],\n",
       "        [0.06663775],\n",
       "        [0.06792779],\n",
       "        [0.0669482 ],\n",
       "        [0.06795276],\n",
       "        [0.06737817],\n",
       "        [0.06715272],\n",
       "        [0.06680236],\n",
       "        [0.06646559],\n",
       "        [0.06617913],\n",
       "        [0.06594808],\n",
       "        [0.06577037]],\n",
       "\n",
       "       [[0.06632948],\n",
       "        [0.06722595],\n",
       "        [0.06661442],\n",
       "        [0.06807934],\n",
       "        [0.06770135],\n",
       "        [0.06754178],\n",
       "        [0.06719752],\n",
       "        [0.06684966],\n",
       "        [0.06654914],\n",
       "        [0.06630828],\n",
       "        [0.0661236 ],\n",
       "        [0.06598723],\n",
       "        [0.0658911 ],\n",
       "        [0.06582316],\n",
       "        [0.06577799]],\n",
       "\n",
       "       [[0.06644133],\n",
       "        [0.06716342],\n",
       "        [0.06710783],\n",
       "        [0.06686177],\n",
       "        [0.06667816],\n",
       "        [0.06623459],\n",
       "        [0.06680629],\n",
       "        [0.06718048],\n",
       "        [0.06712084],\n",
       "        [0.06691046],\n",
       "        [0.06666704],\n",
       "        [0.06644257],\n",
       "        [0.06625837],\n",
       "        [0.06611526],\n",
       "        [0.06601156]],\n",
       "\n",
       "       [[0.06648354],\n",
       "        [0.06541643],\n",
       "        [0.06732673],\n",
       "        [0.06637679],\n",
       "        [0.06780747],\n",
       "        [0.06751113],\n",
       "        [0.06745288],\n",
       "        [0.06719667],\n",
       "        [0.06691219],\n",
       "        [0.06665324],\n",
       "        [0.06643742],\n",
       "        [0.0662666 ],\n",
       "        [0.06613789],\n",
       "        [0.06604363],\n",
       "        [0.06597736]],\n",
       "\n",
       "       [[0.06689923],\n",
       "        [0.06582543],\n",
       "        [0.06532116],\n",
       "        [0.0672071 ],\n",
       "        [0.06635242],\n",
       "        [0.06683023],\n",
       "        [0.06725404],\n",
       "        [0.06728797],\n",
       "        [0.06716699],\n",
       "        [0.06699309],\n",
       "        [0.06681751],\n",
       "        [0.06666466],\n",
       "        [0.06654263],\n",
       "        [0.06645131],\n",
       "        [0.06638625]],\n",
       "\n",
       "       [[0.06642362],\n",
       "        [0.06535744],\n",
       "        [0.06593864],\n",
       "        [0.06565008],\n",
       "        [0.06552044],\n",
       "        [0.06772722],\n",
       "        [0.06859746],\n",
       "        [0.06789436],\n",
       "        [0.06759942],\n",
       "        [0.06721767],\n",
       "        [0.06687144],\n",
       "        [0.06658772],\n",
       "        [0.06636153],\n",
       "        [0.06619029],\n",
       "        [0.06606264]],\n",
       "\n",
       "       [[0.06633186],\n",
       "        [0.06672028],\n",
       "        [0.06604144],\n",
       "        [0.06574859],\n",
       "        [0.06549901],\n",
       "        [0.0676893 ],\n",
       "        [0.06834751],\n",
       "        [0.06764334],\n",
       "        [0.06739489],\n",
       "        [0.06705232],\n",
       "        [0.06673589],\n",
       "        [0.06647073],\n",
       "        [0.06625844],\n",
       "        [0.06609409],\n",
       "        [0.06597237]],\n",
       "\n",
       "       [[0.06646971],\n",
       "        [0.06810435],\n",
       "        [0.06802276],\n",
       "        [0.06590424],\n",
       "        [0.06547538],\n",
       "        [0.06610014],\n",
       "        [0.06700975],\n",
       "        [0.06736139],\n",
       "        [0.06720383],\n",
       "        [0.06692056],\n",
       "        [0.06664503],\n",
       "        [0.06641571],\n",
       "        [0.06623907],\n",
       "        [0.06610963],\n",
       "        [0.06601845]],\n",
       "\n",
       "       [[0.0663753 ],\n",
       "        [0.06728569],\n",
       "        [0.06602007],\n",
       "        [0.06698469],\n",
       "        [0.06797755],\n",
       "        [0.06749362],\n",
       "        [0.06735951],\n",
       "        [0.06706608],\n",
       "        [0.06676507],\n",
       "        [0.0665012 ],\n",
       "        [0.06628692],\n",
       "        [0.06612127],\n",
       "        [0.06599978],\n",
       "        [0.06591137],\n",
       "        [0.0658519 ]],\n",
       "\n",
       "       [[0.06633715],\n",
       "        [0.06527237],\n",
       "        [0.06717849],\n",
       "        [0.06650441],\n",
       "        [0.06776913],\n",
       "        [0.06811801],\n",
       "        [0.06740542],\n",
       "        [0.06721597],\n",
       "        [0.06693669],\n",
       "        [0.06666601],\n",
       "        [0.06643056],\n",
       "        [0.06623642],\n",
       "        [0.06608228],\n",
       "        [0.06596638],\n",
       "        [0.06588066]],\n",
       "\n",
       "       [[0.06645966],\n",
       "        [0.06705536],\n",
       "        [0.06685115],\n",
       "        [0.06702424],\n",
       "        [0.06757656],\n",
       "        [0.06699977],\n",
       "        [0.06591993],\n",
       "        [0.06639257],\n",
       "        [0.06683089],\n",
       "        [0.06686987],\n",
       "        [0.06674504],\n",
       "        [0.06656326],\n",
       "        [0.06638204],\n",
       "        [0.06622661],\n",
       "        [0.06610305]],\n",
       "\n",
       "       [[0.06612137],\n",
       "        [0.06671403],\n",
       "        [0.06818797],\n",
       "        [0.06752808],\n",
       "        [0.06822608],\n",
       "        [0.06749779],\n",
       "        [0.06721223],\n",
       "        [0.06684712],\n",
       "        [0.06651714],\n",
       "        [0.06624489],\n",
       "        [0.06603076],\n",
       "        [0.06586666],\n",
       "        [0.065745  ],\n",
       "        [0.06565985],\n",
       "        [0.06560101]],\n",
       "\n",
       "       [[0.06644747],\n",
       "        [0.06582745],\n",
       "        [0.06698416],\n",
       "        [0.06807046],\n",
       "        [0.06763846],\n",
       "        [0.06752928],\n",
       "        [0.06724404],\n",
       "        [0.06693774],\n",
       "        [0.06666121],\n",
       "        [0.06643182],\n",
       "        [0.06625003],\n",
       "        [0.06611472],\n",
       "        [0.06601631],\n",
       "        [0.06594692],\n",
       "        [0.06589994]],\n",
       "\n",
       "       [[0.0664172 ],\n",
       "        [0.06619272],\n",
       "        [0.06713048],\n",
       "        [0.06752659],\n",
       "        [0.06702027],\n",
       "        [0.06733549],\n",
       "        [0.06741197],\n",
       "        [0.06715064],\n",
       "        [0.06683929],\n",
       "        [0.0665624 ],\n",
       "        [0.06634014],\n",
       "        [0.06617106],\n",
       "        [0.0660459 ],\n",
       "        [0.06595878],\n",
       "        [0.06589707]],\n",
       "\n",
       "       [[0.06640752],\n",
       "        [0.0669792 ],\n",
       "        [0.06637715],\n",
       "        [0.06739996],\n",
       "        [0.06640644],\n",
       "        [0.06651471],\n",
       "        [0.06716718],\n",
       "        [0.0674079 ],\n",
       "        [0.0672079 ],\n",
       "        [0.06690714],\n",
       "        [0.06661996],\n",
       "        [0.06638166],\n",
       "        [0.06619744],\n",
       "        [0.06606167],\n",
       "        [0.0659642 ]],\n",
       "\n",
       "       [[0.06632336],\n",
       "        [0.06666729],\n",
       "        [0.06758571],\n",
       "        [0.06834923],\n",
       "        [0.06767766],\n",
       "        [0.06742381],\n",
       "        [0.06706832],\n",
       "        [0.06673822],\n",
       "        [0.06646277],\n",
       "        [0.06624316],\n",
       "        [0.06607641],\n",
       "        [0.06595296],\n",
       "        [0.06586424],\n",
       "        [0.06580463],\n",
       "        [0.06576228]],\n",
       "\n",
       "       [[0.06661635],\n",
       "        [0.06875374],\n",
       "        [0.06700375],\n",
       "        [0.06618176],\n",
       "        [0.0656966 ],\n",
       "        [0.06644023],\n",
       "        [0.06698788],\n",
       "        [0.06706239],\n",
       "        [0.06694423],\n",
       "        [0.0667607 ],\n",
       "        [0.06657205],\n",
       "        [0.06640932],\n",
       "        [0.06627819],\n",
       "        [0.06618109],\n",
       "        [0.06611171]],\n",
       "\n",
       "       [[0.06615001],\n",
       "        [0.06546477],\n",
       "        [0.06702103],\n",
       "        [0.06743102],\n",
       "        [0.06815685],\n",
       "        [0.06798438],\n",
       "        [0.06655771],\n",
       "        [0.06673709],\n",
       "        [0.0669364 ],\n",
       "        [0.06680764],\n",
       "        [0.06657432],\n",
       "        [0.06632894],\n",
       "        [0.06611188],\n",
       "        [0.06593526],\n",
       "        [0.06580269]],\n",
       "\n",
       "       [[0.06668648],\n",
       "        [0.06791499],\n",
       "        [0.06680683],\n",
       "        [0.06665859],\n",
       "        [0.06506822],\n",
       "        [0.06683787],\n",
       "        [0.06685018],\n",
       "        [0.06706169],\n",
       "        [0.06701431],\n",
       "        [0.06687637],\n",
       "        [0.06671279],\n",
       "        [0.06655461],\n",
       "        [0.06641928],\n",
       "        [0.0663101 ],\n",
       "        [0.06622764]],\n",
       "\n",
       "       [[0.06655202],\n",
       "        [0.06693482],\n",
       "        [0.06684163],\n",
       "        [0.0667331 ],\n",
       "        [0.06731434],\n",
       "        [0.06752035],\n",
       "        [0.06730402],\n",
       "        [0.0669985 ],\n",
       "        [0.06671602],\n",
       "        [0.06648612],\n",
       "        [0.06630926],\n",
       "        [0.0661809 ],\n",
       "        [0.06609082],\n",
       "        [0.06602985],\n",
       "        [0.06598831]],\n",
       "\n",
       "       [[0.06638186],\n",
       "        [0.06531636],\n",
       "        [0.06678046],\n",
       "        [0.06766305],\n",
       "        [0.06830538],\n",
       "        [0.06760066],\n",
       "        [0.06736455],\n",
       "        [0.06704828],\n",
       "        [0.0667554 ],\n",
       "        [0.06650649],\n",
       "        [0.06630369],\n",
       "        [0.0661444 ],\n",
       "        [0.06602278],\n",
       "        [0.0659342 ],\n",
       "        [0.06587239]],\n",
       "\n",
       "       [[0.06695298],\n",
       "        [0.06587831],\n",
       "        [0.06540209],\n",
       "        [0.06612067],\n",
       "        [0.06497968],\n",
       "        [0.06728055],\n",
       "        [0.06744297],\n",
       "        [0.06761266],\n",
       "        [0.06747123],\n",
       "        [0.06724841],\n",
       "        [0.06702857],\n",
       "        [0.06683725],\n",
       "        [0.06668653],\n",
       "        [0.06657059],\n",
       "        [0.06648745]],\n",
       "\n",
       "       [[0.06667386],\n",
       "        [0.06560367],\n",
       "        [0.06631177],\n",
       "        [0.06825969],\n",
       "        [0.06668418],\n",
       "        [0.06686515],\n",
       "        [0.06713987],\n",
       "        [0.06709906],\n",
       "        [0.06694052],\n",
       "        [0.06675306],\n",
       "        [0.0665739 ],\n",
       "        [0.06642193],\n",
       "        [0.06630374],\n",
       "        [0.06621565],\n",
       "        [0.06615396]],\n",
       "\n",
       "       [[0.0659949 ],\n",
       "        [0.06764407],\n",
       "        [0.06806318],\n",
       "        [0.06749235],\n",
       "        [0.06653019],\n",
       "        [0.06602695],\n",
       "        [0.06755158],\n",
       "        [0.06716969],\n",
       "        [0.06702295],\n",
       "        [0.0667075 ],\n",
       "        [0.06639247],\n",
       "        [0.06612439],\n",
       "        [0.06590956],\n",
       "        [0.06574555],\n",
       "        [0.06562459]],\n",
       "\n",
       "       [[0.06648427],\n",
       "        [0.06553897],\n",
       "        [0.06722913],\n",
       "        [0.06647979],\n",
       "        [0.06785306],\n",
       "        [0.06753796],\n",
       "        [0.06746512],\n",
       "        [0.06718802],\n",
       "        [0.06688663],\n",
       "        [0.06661776],\n",
       "        [0.06640039],\n",
       "        [0.066231  ],\n",
       "        [0.06610732],\n",
       "        [0.06602056],\n",
       "        [0.06596004]],\n",
       "\n",
       "       [[0.06630906],\n",
       "        [0.06588989],\n",
       "        [0.06605174],\n",
       "        [0.06740901],\n",
       "        [0.06840879],\n",
       "        [0.0678076 ],\n",
       "        [0.06756156],\n",
       "        [0.06718718],\n",
       "        [0.06682936],\n",
       "        [0.06652649],\n",
       "        [0.06628458],\n",
       "        [0.0661002 ],\n",
       "        [0.06596456],\n",
       "        [0.06586843],\n",
       "        [0.06580152]],\n",
       "\n",
       "       [[0.06652062],\n",
       "        [0.0654529 ],\n",
       "        [0.06617175],\n",
       "        [0.06647912],\n",
       "        [0.0675991 ],\n",
       "        [0.06791511],\n",
       "        [0.06732344],\n",
       "        [0.06724256],\n",
       "        [0.06703477],\n",
       "        [0.06680471],\n",
       "        [0.06658978],\n",
       "        [0.06640575],\n",
       "        [0.0662569 ],\n",
       "        [0.06614345],\n",
       "        [0.06606007]],\n",
       "\n",
       "       [[0.06667062],\n",
       "        [0.06757171],\n",
       "        [0.06827511],\n",
       "        [0.06607399],\n",
       "        [0.0658111 ],\n",
       "        [0.06647187],\n",
       "        [0.06694045],\n",
       "        [0.06698737],\n",
       "        [0.06687491],\n",
       "        [0.06671168],\n",
       "        [0.06654947],\n",
       "        [0.06640629],\n",
       "        [0.066295  ],\n",
       "        [0.06621047],\n",
       "        [0.06615001]],\n",
       "\n",
       "       [[0.06626896],\n",
       "        [0.06629191],\n",
       "        [0.06556097],\n",
       "        [0.06703173],\n",
       "        [0.06682495],\n",
       "        [0.06702448],\n",
       "        [0.0679826 ],\n",
       "        [0.06745704],\n",
       "        [0.06729835],\n",
       "        [0.06699477],\n",
       "        [0.06669033],\n",
       "        [0.06642298],\n",
       "        [0.06620535],\n",
       "        [0.06603583],\n",
       "        [0.06590972]],\n",
       "\n",
       "       [[0.06601942],\n",
       "        [0.06540336],\n",
       "        [0.06622732],\n",
       "        [0.06716696],\n",
       "        [0.06817884],\n",
       "        [0.06615774],\n",
       "        [0.06721034],\n",
       "        [0.06798375],\n",
       "        [0.06734199],\n",
       "        [0.067114  ],\n",
       "        [0.06677865],\n",
       "        [0.06645959],\n",
       "        [0.06618876],\n",
       "        [0.06596918],\n",
       "        [0.06580014]],\n",
       "\n",
       "       [[0.06628259],\n",
       "        [0.06720039],\n",
       "        [0.06769038],\n",
       "        [0.06734212],\n",
       "        [0.06739543],\n",
       "        [0.0662677 ],\n",
       "        [0.06663933],\n",
       "        [0.06696421],\n",
       "        [0.06690304],\n",
       "        [0.06670288],\n",
       "        [0.06647236],\n",
       "        [0.06625959],\n",
       "        [0.06608358],\n",
       "        [0.06594796],\n",
       "        [0.06584845]],\n",
       "\n",
       "       [[0.06627772],\n",
       "        [0.06565928],\n",
       "        [0.0665164 ],\n",
       "        [0.06817528],\n",
       "        [0.06695271],\n",
       "        [0.06803577],\n",
       "        [0.0674384 ],\n",
       "        [0.06721566],\n",
       "        [0.06689446],\n",
       "        [0.06660034],\n",
       "        [0.0663551 ],\n",
       "        [0.06615967],\n",
       "        [0.06600872],\n",
       "        [0.0658959 ],\n",
       "        [0.06581457]],\n",
       "\n",
       "       [[0.06654823],\n",
       "        [0.06746969],\n",
       "        [0.06725282],\n",
       "        [0.06657952],\n",
       "        [0.06696353],\n",
       "        [0.06723574],\n",
       "        [0.06714317],\n",
       "        [0.06693533],\n",
       "        [0.06670749],\n",
       "        [0.06649964],\n",
       "        [0.06633069],\n",
       "        [0.06620008],\n",
       "        [0.06610486],\n",
       "        [0.06603704],\n",
       "        [0.06599219]],\n",
       "\n",
       "       [[0.06635027],\n",
       "        [0.06585891],\n",
       "        [0.06734151],\n",
       "        [0.06840976],\n",
       "        [0.06783023],\n",
       "        [0.06758351],\n",
       "        [0.06720892],\n",
       "        [0.06685432],\n",
       "        [0.06655592],\n",
       "        [0.0663193 ],\n",
       "        [0.06613717],\n",
       "        [0.06600353],\n",
       "        [0.06590741],\n",
       "        [0.06584191],\n",
       "        [0.06579733]],\n",
       "\n",
       "       [[0.06645701],\n",
       "        [0.0653903 ],\n",
       "        [0.06686687],\n",
       "        [0.06597578],\n",
       "        [0.06725587],\n",
       "        [0.06802469],\n",
       "        [0.06748069],\n",
       "        [0.06735418],\n",
       "        [0.06709351],\n",
       "        [0.06682215],\n",
       "        [0.06658025],\n",
       "        [0.06637824],\n",
       "        [0.06621761],\n",
       "        [0.06609579],\n",
       "        [0.06600707]],\n",
       "\n",
       "       [[0.06689229],\n",
       "        [0.06619936],\n",
       "        [0.06637008],\n",
       "        [0.06600244],\n",
       "        [0.06672066],\n",
       "        [0.06722976],\n",
       "        [0.06728452],\n",
       "        [0.06716152],\n",
       "        [0.06698476],\n",
       "        [0.06680866],\n",
       "        [0.0666549 ],\n",
       "        [0.0665335 ],\n",
       "        [0.06644344],\n",
       "        [0.06637972],\n",
       "        [0.06633436]],\n",
       "\n",
       "       [[0.06666047],\n",
       "        [0.06643327],\n",
       "        [0.06575075],\n",
       "        [0.06743711],\n",
       "        [0.06734551],\n",
       "        [0.0674295 ],\n",
       "        [0.06726383],\n",
       "        [0.06703315],\n",
       "        [0.06680397],\n",
       "        [0.06660271],\n",
       "        [0.06643946],\n",
       "        [0.0663147 ],\n",
       "        [0.06622115],\n",
       "        [0.06615543],\n",
       "        [0.06610895]],\n",
       "\n",
       "       [[0.0661459 ],\n",
       "        [0.06756706],\n",
       "        [0.06732887],\n",
       "        [0.06781986],\n",
       "        [0.06671337],\n",
       "        [0.06748562],\n",
       "        [0.06700951],\n",
       "        [0.06694715],\n",
       "        [0.0667331 ],\n",
       "        [0.06649311],\n",
       "        [0.06626778],\n",
       "        [0.06607382],\n",
       "        [0.06591565],\n",
       "        [0.0657943 ],\n",
       "        [0.06570493]]], dtype=float32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.mig\n",
    "# Веса внимания\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "zt8p4qy3rsc44koq8pv24r",
    "execution_id": "8f3e9249-74f8-4d1d-89a9-e3eed1705d80",
    "id": "NgynZtVxvb15"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "cellId": "vrwror8ejoakwblde02z8m",
    "execution_id": "2d607204-fedb-4bfc-956a-e91a6b039948",
    "id": "g4qjLc_XvXpP"
   },
   "outputs": [],
   "source": [
    "#!g2.mig\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        # Используем слой внимания\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # Enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        # Получаем выходы слоя внимания (из скрытого состояния и выхода энкодера)\n",
    "        # Context_vector shape == (batch_size, hidden_size)\n",
    "        # Attention_weights shape == (batch_size, max_len, 1)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        # Соединяем выход эмбеддинга с вектором контекста и подаем навход RNN\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # Passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # (batch_size, 1, hidden_size) --> output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # Output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "cellId": "q5peohf6t7wimw3xl0z8e",
    "execution_id": "f9b99345-0233-4eca-85ca-4ad3fcd6510c",
    "id": "8Lj7x61EvrmT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 7335)\n"
     ]
    }
   ],
   "source": [
    "#!g2.mig\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "# Применяем декодер к случайному батчу из равномерного распределения (батч,1) и выходам энкодера\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "cellId": "b8qjrnmp1k2af4lbhts5u",
    "execution_id": "6812f568-0406-4410-843a-8e41fd144445",
    "id": "gqaJSa1pvxE2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 7335), dtype=float32, numpy=\n",
       "array([[-0.00431752,  0.00183496, -0.002612  , ..., -0.00067051,\n",
       "        -0.00290763,  0.00045797],\n",
       "       [-0.004483  ,  0.00149953, -0.00244687, ..., -0.00018436,\n",
       "        -0.00315358,  0.00072791],\n",
       "       [-0.00474681,  0.00121364, -0.00279857, ..., -0.00051203,\n",
       "        -0.00280373,  0.00027528],\n",
       "       ...,\n",
       "       [-0.00436075,  0.00140361, -0.00272169, ..., -0.0004665 ,\n",
       "        -0.00391093,  0.00045581],\n",
       "       [-0.00442866,  0.00139743, -0.00292976, ..., -0.00032783,\n",
       "        -0.00259674,  0.00030078],\n",
       "       [-0.00407641,  0.00242041, -0.00234986, ..., -0.00041125,\n",
       "        -0.00371192,  0.00056452]], dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.mig\n",
    "sample_decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "761f2hbhei32wwgdo5rgrg",
    "execution_id": "839f6a94-047e-4f50-abc1-2d172b0f4626",
    "id": "5uUKUQQsv1OD"
   },
   "source": [
    "### Сheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "cellId": "b4ycd97b8ed7w5tx8w6z5",
    "execution_id": "7c64b801-a0d8-474c-b248-5cf29bd17f90",
    "id": "pnuxIWBFvziN"
   },
   "outputs": [],
   "source": [
    "#!g2.mig\n",
    "checkpoint_dir = './training_attention_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "8l2kta2cu8irnw0nfgvs9",
    "execution_id": "8b6cd4ac-0e4d-4b3e-9627-88133c3ac6c3",
    "id": "UI2KWyNav5hb"
   },
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "cellId": "b87jl4w70vv77ahcv1fud4",
    "execution_id": "0da81c69-f85a-4ffd-b098-72b1f85b59cd",
    "id": "sSGou04Zv5B6"
   },
   "outputs": [],
   "source": [
    "#!g2.mig\n",
    "@tf.function\n",
    "def train_step_att(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Получаем выходы encoder\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        # Помещаем выходное скрытое состояние энкодера в скрытое состояние decoder\n",
    "        dec_hidden = enc_hidden\n",
    "        # Формируем вход декодера:\n",
    "             # Берем список длины батч из индексов тега <start>\n",
    "             # Приписываем списку размерность 1 сзади\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - выводим target в качестве следующего входа\n",
    "        for t in range(1, targ.shape[1]):\n",
    "          # Помещаем enc_output, dec_input, dec_hidden в decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            # Считаем функцию потерь \n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            # Используем teacher forcing (приписываем списку размерность 1 сзади)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    # Переменные\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    # Вычисляем градиенты loss по variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    # Оптимизатор применяет подсчитанные градиенты\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "cellId": "dk4oxpeu7la5xdt7c66w65",
    "execution_id": "d0dd4f92-6e59-4dee-ab1d-9cb4d05cd86d",
    "id": "0CfQ1gT5wIcS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.5261\n",
      "Epoch 1 Batch 100 Loss 1.9116\n",
      "Epoch 1 Batch 200 Loss 1.5853\n",
      "Epoch 1 Batch 300 Loss 1.5470\n",
      "Epoch 1 Batch 400 Loss 1.4870\n",
      "Epoch 1 Batch 500 Loss 1.3588\n",
      "Epoch 1 Batch 600 Loss 1.2534\n",
      "Epoch 1 Batch 700 Loss 1.1858\n",
      "Epoch 1 Batch 800 Loss 1.2579\n",
      "Epoch 1 Batch 900 Loss 1.1737\n",
      "Epoch 1 Batch 1000 Loss 1.0625\n",
      "Epoch 1 Batch 1100 Loss 1.0047\n",
      "Epoch 1 Batch 1200 Loss 1.0235\n",
      "Epoch 1 Loss 1.3939\n",
      "Time taken for 1 epoch 117.6727466583252 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.9361\n",
      "Epoch 2 Batch 100 Loss 0.9212\n",
      "Epoch 2 Batch 200 Loss 0.9018\n",
      "Epoch 2 Batch 300 Loss 0.8291\n",
      "Epoch 2 Batch 400 Loss 0.8947\n",
      "Epoch 2 Batch 500 Loss 0.7480\n",
      "Epoch 2 Batch 600 Loss 0.7704\n",
      "Epoch 2 Batch 700 Loss 0.7566\n",
      "Epoch 2 Batch 800 Loss 0.7194\n",
      "Epoch 2 Batch 900 Loss 0.7015\n",
      "Epoch 2 Batch 1000 Loss 0.6712\n",
      "Epoch 2 Batch 1100 Loss 0.6802\n",
      "Epoch 2 Batch 1200 Loss 0.6101\n",
      "Epoch 2 Loss 0.7601\n",
      "Time taken for 1 epoch 111.00780248641968 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.5206\n",
      "Epoch 3 Batch 100 Loss 0.4091\n",
      "Epoch 3 Batch 200 Loss 0.4261\n",
      "Epoch 3 Batch 300 Loss 0.5173\n",
      "Epoch 3 Batch 400 Loss 0.4547\n",
      "Epoch 3 Batch 500 Loss 0.4092\n",
      "Epoch 3 Batch 600 Loss 0.4431\n",
      "Epoch 3 Batch 700 Loss 0.3939\n",
      "Epoch 3 Batch 800 Loss 0.4528\n",
      "Epoch 3 Batch 900 Loss 0.4266\n",
      "Epoch 3 Batch 1000 Loss 0.4304\n",
      "Epoch 3 Batch 1100 Loss 0.4189\n",
      "Epoch 3 Batch 1200 Loss 0.3975\n",
      "Epoch 3 Loss 0.4272\n",
      "Time taken for 1 epoch 108.50668811798096 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.1966\n",
      "Epoch 4 Batch 100 Loss 0.2640\n",
      "Epoch 4 Batch 200 Loss 0.2322\n",
      "Epoch 4 Batch 300 Loss 0.2186\n",
      "Epoch 4 Batch 400 Loss 0.2237\n",
      "Epoch 4 Batch 500 Loss 0.2281\n",
      "Epoch 4 Batch 600 Loss 0.2365\n",
      "Epoch 4 Batch 700 Loss 0.3206\n",
      "Epoch 4 Batch 800 Loss 0.2245\n",
      "Epoch 4 Batch 900 Loss 0.2281\n",
      "Epoch 4 Batch 1000 Loss 0.2107\n",
      "Epoch 4 Batch 1100 Loss 0.2188\n",
      "Epoch 4 Batch 1200 Loss 0.2154\n",
      "Epoch 4 Loss 0.2517\n",
      "Time taken for 1 epoch 110.91339230537415 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.1154\n",
      "Epoch 5 Batch 100 Loss 0.1535\n",
      "Epoch 5 Batch 200 Loss 0.1289\n",
      "Epoch 5 Batch 300 Loss 0.1367\n",
      "Epoch 5 Batch 400 Loss 0.1233\n",
      "Epoch 5 Batch 500 Loss 0.1580\n",
      "Epoch 5 Batch 600 Loss 0.2410\n",
      "Epoch 5 Batch 700 Loss 0.2102\n",
      "Epoch 5 Batch 800 Loss 0.1994\n",
      "Epoch 5 Batch 900 Loss 0.1654\n",
      "Epoch 5 Batch 1000 Loss 0.2152\n",
      "Epoch 5 Batch 1100 Loss 0.1877\n",
      "Epoch 5 Batch 1200 Loss 0.1465\n",
      "Epoch 5 Loss 0.1630\n",
      "Time taken for 1 epoch 108.5058057308197 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.0948\n",
      "Epoch 6 Batch 100 Loss 0.1035\n",
      "Epoch 6 Batch 200 Loss 0.1244\n",
      "Epoch 6 Batch 300 Loss 0.0718\n",
      "Epoch 6 Batch 400 Loss 0.0953\n",
      "Epoch 6 Batch 500 Loss 0.0822\n",
      "Epoch 6 Batch 600 Loss 0.1192\n",
      "Epoch 6 Batch 700 Loss 0.1419\n",
      "Epoch 6 Batch 800 Loss 0.1590\n",
      "Epoch 6 Batch 900 Loss 0.1533\n",
      "Epoch 6 Batch 1000 Loss 0.0941\n",
      "Epoch 6 Batch 1100 Loss 0.1137\n",
      "Epoch 6 Batch 1200 Loss 0.1682\n",
      "Epoch 6 Loss 0.1188\n",
      "Time taken for 1 epoch 111.0689868927002 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.1065\n",
      "Epoch 7 Batch 100 Loss 0.0625\n",
      "Epoch 7 Batch 200 Loss 0.0709\n",
      "Epoch 7 Batch 300 Loss 0.0832\n",
      "Epoch 7 Batch 400 Loss 0.0661\n",
      "Epoch 7 Batch 500 Loss 0.0732\n",
      "Epoch 7 Batch 600 Loss 0.0781\n",
      "Epoch 7 Batch 700 Loss 0.0899\n",
      "Epoch 7 Batch 800 Loss 0.1112\n",
      "Epoch 7 Batch 900 Loss 0.0814\n",
      "Epoch 7 Batch 1000 Loss 0.0988\n",
      "Epoch 7 Batch 1100 Loss 0.1302\n",
      "Epoch 7 Batch 1200 Loss 0.1057\n",
      "Epoch 7 Loss 0.0972\n",
      "Time taken for 1 epoch 108.49581718444824 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0643\n",
      "Epoch 8 Batch 100 Loss 0.0778\n",
      "Epoch 8 Batch 200 Loss 0.0679\n",
      "Epoch 8 Batch 300 Loss 0.0725\n",
      "Epoch 8 Batch 400 Loss 0.0686\n",
      "Epoch 8 Batch 500 Loss 0.0977\n",
      "Epoch 8 Batch 600 Loss 0.0772\n",
      "Epoch 8 Batch 700 Loss 0.0785\n",
      "Epoch 8 Batch 800 Loss 0.0822\n",
      "Epoch 8 Batch 900 Loss 0.1075\n",
      "Epoch 8 Batch 1000 Loss 0.0713\n",
      "Epoch 8 Batch 1100 Loss 0.1119\n",
      "Epoch 8 Batch 1200 Loss 0.1179\n",
      "Epoch 8 Loss 0.0843\n",
      "Time taken for 1 epoch 111.08104395866394 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0984\n",
      "Epoch 9 Batch 100 Loss 0.0727\n",
      "Epoch 9 Batch 200 Loss 0.0627\n",
      "Epoch 9 Batch 300 Loss 0.0747\n",
      "Epoch 9 Batch 400 Loss 0.1078\n",
      "Epoch 9 Batch 500 Loss 0.0940\n",
      "Epoch 9 Batch 600 Loss 0.0695\n",
      "Epoch 9 Batch 700 Loss 0.0695\n",
      "Epoch 9 Batch 800 Loss 0.1043\n",
      "Epoch 9 Batch 900 Loss 0.0946\n",
      "Epoch 9 Batch 1000 Loss 0.1155\n",
      "Epoch 9 Batch 1100 Loss 0.0708\n",
      "Epoch 9 Batch 1200 Loss 0.0778\n",
      "Epoch 9 Loss 0.0768\n",
      "Time taken for 1 epoch 108.50690150260925 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0596\n",
      "Epoch 10 Batch 100 Loss 0.0514\n",
      "Epoch 10 Batch 200 Loss 0.0602\n",
      "Epoch 10 Batch 300 Loss 0.0615\n",
      "Epoch 10 Batch 400 Loss 0.0615\n",
      "Epoch 10 Batch 500 Loss 0.0458\n",
      "Epoch 10 Batch 600 Loss 0.0801\n",
      "Epoch 10 Batch 700 Loss 0.0781\n",
      "Epoch 10 Batch 800 Loss 0.0497\n",
      "Epoch 10 Batch 900 Loss 0.0617\n",
      "Epoch 10 Batch 1000 Loss 0.0870\n",
      "Epoch 10 Batch 1100 Loss 0.0824\n",
      "Epoch 10 Batch 1200 Loss 0.0695\n",
      "Epoch 10 Loss 0.0714\n",
      "Time taken for 1 epoch 111.08818244934082 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.0514\n",
      "Epoch 11 Batch 100 Loss 0.0970\n",
      "Epoch 11 Batch 200 Loss 0.0582\n",
      "Epoch 11 Batch 300 Loss 0.0496\n",
      "Epoch 11 Batch 400 Loss 0.0828\n",
      "Epoch 11 Batch 500 Loss 0.0431\n",
      "Epoch 11 Batch 600 Loss 0.0348\n",
      "Epoch 11 Batch 700 Loss 0.0902\n",
      "Epoch 11 Batch 800 Loss 0.0751\n",
      "Epoch 11 Batch 900 Loss 0.0742\n",
      "Epoch 11 Batch 1000 Loss 0.0842\n",
      "Epoch 11 Batch 1100 Loss 0.0916\n",
      "Epoch 11 Batch 1200 Loss 0.0785\n",
      "Epoch 11 Loss 0.0684\n",
      "Time taken for 1 epoch 108.51057600975037 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.0293\n",
      "Epoch 12 Batch 100 Loss 0.0502\n",
      "Epoch 12 Batch 200 Loss 0.0604\n",
      "Epoch 12 Batch 300 Loss 0.0455\n",
      "Epoch 12 Batch 400 Loss 0.0383\n",
      "Epoch 12 Batch 500 Loss 0.0659\n",
      "Epoch 12 Batch 600 Loss 0.0380\n",
      "Epoch 12 Batch 700 Loss 0.0761\n",
      "Epoch 12 Batch 800 Loss 0.1045\n",
      "Epoch 12 Batch 900 Loss 0.1046\n",
      "Epoch 12 Batch 1000 Loss 0.0681\n",
      "Epoch 12 Batch 1100 Loss 0.0750\n",
      "Epoch 12 Batch 1200 Loss 0.0535\n",
      "Epoch 12 Loss 0.0640\n",
      "Time taken for 1 epoch 110.98354291915894 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.0703\n",
      "Epoch 13 Batch 100 Loss 0.0712\n",
      "Epoch 13 Batch 200 Loss 0.0618\n",
      "Epoch 13 Batch 300 Loss 0.0625\n",
      "Epoch 13 Batch 400 Loss 0.0632\n",
      "Epoch 13 Batch 500 Loss 0.0440\n",
      "Epoch 13 Batch 600 Loss 0.0558\n",
      "Epoch 13 Batch 700 Loss 0.0825\n",
      "Epoch 13 Batch 800 Loss 0.0592\n",
      "Epoch 13 Batch 900 Loss 0.0671\n",
      "Epoch 13 Batch 1000 Loss 0.0558\n",
      "Epoch 13 Batch 1100 Loss 0.0485\n",
      "Epoch 13 Batch 1200 Loss 0.1118\n",
      "Epoch 13 Loss 0.0621\n",
      "Time taken for 1 epoch 108.49928426742554 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.0410\n",
      "Epoch 14 Batch 100 Loss 0.0270\n",
      "Epoch 14 Batch 200 Loss 0.0355\n",
      "Epoch 14 Batch 300 Loss 0.0599\n",
      "Epoch 14 Batch 400 Loss 0.0538\n",
      "Epoch 14 Batch 500 Loss 0.0648\n",
      "Epoch 14 Batch 600 Loss 0.0360\n",
      "Epoch 14 Batch 700 Loss 0.0896\n",
      "Epoch 14 Batch 800 Loss 0.0579\n",
      "Epoch 14 Batch 900 Loss 0.0638\n",
      "Epoch 14 Batch 1000 Loss 0.0846\n",
      "Epoch 14 Batch 1100 Loss 0.0461\n",
      "Epoch 14 Batch 1200 Loss 0.0842\n",
      "Epoch 14 Loss 0.0590\n",
      "Time taken for 1 epoch 110.8230447769165 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.0511\n",
      "Epoch 15 Batch 100 Loss 0.0348\n",
      "Epoch 15 Batch 200 Loss 0.0472\n",
      "Epoch 15 Batch 300 Loss 0.0503\n",
      "Epoch 15 Batch 400 Loss 0.0473\n",
      "Epoch 15 Batch 500 Loss 0.0386\n",
      "Epoch 15 Batch 600 Loss 0.0647\n",
      "Epoch 15 Batch 700 Loss 0.0438\n",
      "Epoch 15 Batch 800 Loss 0.0682\n",
      "Epoch 15 Batch 900 Loss 0.0696\n",
      "Epoch 15 Batch 1000 Loss 0.1112\n",
      "Epoch 15 Batch 1100 Loss 0.0627\n",
      "Epoch 15 Batch 1200 Loss 0.0642\n",
      "Epoch 15 Loss 0.0580\n",
      "Time taken for 1 epoch 108.4991192817688 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.0345\n",
      "Epoch 16 Batch 100 Loss 0.0274\n",
      "Epoch 16 Batch 200 Loss 0.0221\n",
      "Epoch 16 Batch 300 Loss 0.0399\n",
      "Epoch 16 Batch 400 Loss 0.0293\n",
      "Epoch 16 Batch 500 Loss 0.0389\n",
      "Epoch 16 Batch 600 Loss 0.0627\n",
      "Epoch 16 Batch 700 Loss 0.0749\n",
      "Epoch 16 Batch 800 Loss 0.0785\n",
      "Epoch 16 Batch 900 Loss 0.0478\n",
      "Epoch 16 Batch 1000 Loss 0.0585\n",
      "Epoch 16 Batch 1100 Loss 0.0751\n",
      "Epoch 16 Batch 1200 Loss 0.0592\n",
      "Epoch 16 Loss 0.0564\n",
      "Time taken for 1 epoch 111.08307385444641 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.0670\n",
      "Epoch 17 Batch 100 Loss 0.0465\n",
      "Epoch 17 Batch 200 Loss 0.0580\n",
      "Epoch 17 Batch 300 Loss 0.0487\n",
      "Epoch 17 Batch 400 Loss 0.0419\n",
      "Epoch 17 Batch 500 Loss 0.0802\n",
      "Epoch 17 Batch 600 Loss 0.0374\n",
      "Epoch 17 Batch 700 Loss 0.0600\n",
      "Epoch 17 Batch 800 Loss 0.0708\n",
      "Epoch 17 Batch 900 Loss 0.0617\n",
      "Epoch 17 Batch 1000 Loss 0.0765\n",
      "Epoch 17 Batch 1100 Loss 0.0382\n",
      "Epoch 17 Batch 1200 Loss 0.0786\n",
      "Epoch 17 Loss 0.0545\n",
      "Time taken for 1 epoch 108.52142667770386 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.0302\n",
      "Epoch 18 Batch 100 Loss 0.0259\n",
      "Epoch 18 Batch 200 Loss 0.0453\n",
      "Epoch 18 Batch 300 Loss 0.0524\n",
      "Epoch 18 Batch 400 Loss 0.0740\n",
      "Epoch 18 Batch 500 Loss 0.0790\n",
      "Epoch 18 Batch 600 Loss 0.0615\n",
      "Epoch 18 Batch 700 Loss 0.0361\n",
      "Epoch 18 Batch 800 Loss 0.0724\n",
      "Epoch 18 Batch 900 Loss 0.0536\n",
      "Epoch 18 Batch 1000 Loss 0.0650\n",
      "Epoch 18 Batch 1100 Loss 0.0617\n",
      "Epoch 18 Batch 1200 Loss 0.0413\n",
      "Epoch 18 Loss 0.0523\n",
      "Time taken for 1 epoch 111.20264053344727 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.0453\n",
      "Epoch 19 Batch 100 Loss 0.0450\n",
      "Epoch 19 Batch 200 Loss 0.0299\n",
      "Epoch 19 Batch 300 Loss 0.0478\n",
      "Epoch 19 Batch 400 Loss 0.0505\n",
      "Epoch 19 Batch 500 Loss 0.0716\n",
      "Epoch 19 Batch 600 Loss 0.0710\n",
      "Epoch 19 Batch 700 Loss 0.0504\n",
      "Epoch 19 Batch 800 Loss 0.0560\n",
      "Epoch 19 Batch 900 Loss 0.0422\n",
      "Epoch 19 Batch 1000 Loss 0.0514\n",
      "Epoch 19 Batch 1100 Loss 0.0464\n",
      "Epoch 19 Batch 1200 Loss 0.0588\n",
      "Epoch 19 Loss 0.0516\n",
      "Time taken for 1 epoch 108.51471853256226 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.0486\n",
      "Epoch 20 Batch 100 Loss 0.0318\n",
      "Epoch 20 Batch 200 Loss 0.0377\n",
      "Epoch 20 Batch 300 Loss 0.0691\n",
      "Epoch 20 Batch 400 Loss 0.0393\n",
      "Epoch 20 Batch 500 Loss 0.0712\n",
      "Epoch 20 Batch 600 Loss 0.0350\n",
      "Epoch 20 Batch 700 Loss 0.0402\n",
      "Epoch 20 Batch 800 Loss 0.0422\n",
      "Epoch 20 Batch 900 Loss 0.0389\n",
      "Epoch 20 Batch 1000 Loss 0.0587\n",
      "Epoch 20 Batch 1100 Loss 0.0742\n",
      "Epoch 20 Batch 1200 Loss 0.0555\n",
      "Epoch 20 Loss 0.0511\n",
      "Time taken for 1 epoch 110.88857007026672 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.0232\n",
      "Epoch 21 Batch 100 Loss 0.0305\n",
      "Epoch 21 Batch 200 Loss 0.0231\n",
      "Epoch 21 Batch 300 Loss 0.0395\n",
      "Epoch 21 Batch 400 Loss 0.0561\n",
      "Epoch 21 Batch 500 Loss 0.0581\n",
      "Epoch 21 Batch 600 Loss 0.0372\n",
      "Epoch 21 Batch 700 Loss 0.0552\n",
      "Epoch 21 Batch 800 Loss 0.0598\n",
      "Epoch 21 Batch 900 Loss 0.0546\n",
      "Epoch 21 Batch 1000 Loss 0.0394\n",
      "Epoch 21 Batch 1100 Loss 0.0781\n",
      "Epoch 21 Batch 1200 Loss 0.0622\n",
      "Epoch 21 Loss 0.0490\n",
      "Time taken for 1 epoch 108.50640654563904 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.0539\n",
      "Epoch 22 Batch 100 Loss 0.0342\n",
      "Epoch 22 Batch 200 Loss 0.0367\n",
      "Epoch 22 Batch 300 Loss 0.0250\n",
      "Epoch 22 Batch 400 Loss 0.0254\n",
      "Epoch 22 Batch 500 Loss 0.0511\n",
      "Epoch 22 Batch 600 Loss 0.0298\n",
      "Epoch 22 Batch 700 Loss 0.0376\n",
      "Epoch 22 Batch 800 Loss 0.0661\n",
      "Epoch 22 Batch 900 Loss 0.0525\n",
      "Epoch 22 Batch 1000 Loss 0.0706\n",
      "Epoch 22 Batch 1100 Loss 0.0495\n",
      "Epoch 22 Batch 1200 Loss 0.0613\n",
      "Epoch 22 Loss 0.0484\n",
      "Time taken for 1 epoch 111.0693244934082 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.0357\n",
      "Epoch 23 Batch 100 Loss 0.0455\n",
      "Epoch 23 Batch 200 Loss 0.0371\n",
      "Epoch 23 Batch 300 Loss 0.0403\n",
      "Epoch 23 Batch 400 Loss 0.0392\n",
      "Epoch 23 Batch 500 Loss 0.0475\n",
      "Epoch 23 Batch 600 Loss 0.0648\n",
      "Epoch 23 Batch 700 Loss 0.0716\n",
      "Epoch 23 Batch 800 Loss 0.0645\n",
      "Epoch 23 Batch 900 Loss 0.0483\n",
      "Epoch 23 Batch 1000 Loss 0.0847\n",
      "Epoch 23 Batch 1100 Loss 0.0583\n",
      "Epoch 23 Batch 1200 Loss 0.0497\n",
      "Epoch 23 Loss 0.0479\n",
      "Time taken for 1 epoch 108.52754831314087 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.0424\n",
      "Epoch 24 Batch 100 Loss 0.0381\n",
      "Epoch 24 Batch 200 Loss 0.0410\n",
      "Epoch 24 Batch 300 Loss 0.0482\n",
      "Epoch 24 Batch 400 Loss 0.0531\n",
      "Epoch 24 Batch 500 Loss 0.0263\n",
      "Epoch 24 Batch 600 Loss 0.0266\n",
      "Epoch 24 Batch 700 Loss 0.0286\n",
      "Epoch 24 Batch 800 Loss 0.0286\n",
      "Epoch 24 Batch 900 Loss 0.0357\n",
      "Epoch 24 Batch 1000 Loss 0.0454\n",
      "Epoch 24 Batch 1100 Loss 0.0507\n",
      "Epoch 24 Batch 1200 Loss 0.0523\n",
      "Epoch 24 Loss 0.0466\n",
      "Time taken for 1 epoch 111.11248922348022 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.0322\n",
      "Epoch 25 Batch 100 Loss 0.0605\n",
      "Epoch 25 Batch 200 Loss 0.0406\n",
      "Epoch 25 Batch 300 Loss 0.0395\n",
      "Epoch 25 Batch 400 Loss 0.0344\n",
      "Epoch 25 Batch 500 Loss 0.0400\n",
      "Epoch 25 Batch 600 Loss 0.0731\n",
      "Epoch 25 Batch 700 Loss 0.0312\n",
      "Epoch 25 Batch 800 Loss 0.0565\n",
      "Epoch 25 Batch 900 Loss 0.0329\n",
      "Epoch 25 Batch 1000 Loss 0.0383\n",
      "Epoch 25 Batch 1100 Loss 0.0782\n",
      "Epoch 25 Batch 1200 Loss 0.0695\n",
      "Epoch 25 Loss 0.0459\n",
      "Time taken for 1 epoch 108.51891303062439 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.0312\n",
      "Epoch 26 Batch 100 Loss 0.0346\n",
      "Epoch 26 Batch 200 Loss 0.0392\n",
      "Epoch 26 Batch 300 Loss 0.0361\n",
      "Epoch 26 Batch 400 Loss 0.0532\n",
      "Epoch 26 Batch 500 Loss 0.0335\n",
      "Epoch 26 Batch 600 Loss 0.0372\n",
      "Epoch 26 Batch 700 Loss 0.0717\n",
      "Epoch 26 Batch 800 Loss 0.0484\n",
      "Epoch 26 Batch 900 Loss 0.0473\n",
      "Epoch 26 Batch 1000 Loss 0.0512\n",
      "Epoch 26 Batch 1100 Loss 0.0473\n",
      "Epoch 26 Batch 1200 Loss 0.0555\n",
      "Epoch 26 Loss 0.0453\n",
      "Time taken for 1 epoch 110.88924217224121 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.0371\n",
      "Epoch 27 Batch 100 Loss 0.0421\n",
      "Epoch 27 Batch 200 Loss 0.0579\n",
      "Epoch 27 Batch 300 Loss 0.0219\n",
      "Epoch 27 Batch 400 Loss 0.0406\n",
      "Epoch 27 Batch 500 Loss 0.0474\n",
      "Epoch 27 Batch 600 Loss 0.0518\n",
      "Epoch 27 Batch 700 Loss 0.0455\n",
      "Epoch 27 Batch 800 Loss 0.0908\n",
      "Epoch 27 Batch 900 Loss 0.0635\n",
      "Epoch 27 Batch 1000 Loss 0.0457\n",
      "Epoch 27 Batch 1100 Loss 0.0531\n",
      "Epoch 27 Batch 1200 Loss 0.0394\n",
      "Epoch 27 Loss 0.0450\n",
      "Time taken for 1 epoch 108.50854110717773 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.0216\n",
      "Epoch 28 Batch 100 Loss 0.0348\n",
      "Epoch 28 Batch 200 Loss 0.0531\n",
      "Epoch 28 Batch 300 Loss 0.0268\n",
      "Epoch 28 Batch 400 Loss 0.0459\n",
      "Epoch 28 Batch 500 Loss 0.0511\n",
      "Epoch 28 Batch 600 Loss 0.0903\n",
      "Epoch 28 Batch 700 Loss 0.0491\n",
      "Epoch 28 Batch 800 Loss 0.0536\n",
      "Epoch 28 Batch 900 Loss 0.0449\n",
      "Epoch 28 Batch 1000 Loss 0.0524\n",
      "Epoch 28 Batch 1100 Loss 0.0564\n",
      "Epoch 28 Batch 1200 Loss 0.0533\n",
      "Epoch 28 Loss 0.0437\n",
      "Time taken for 1 epoch 110.9059705734253 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.0266\n",
      "Epoch 29 Batch 100 Loss 0.0392\n",
      "Epoch 29 Batch 200 Loss 0.0266\n",
      "Epoch 29 Batch 300 Loss 0.0473\n",
      "Epoch 29 Batch 400 Loss 0.0470\n",
      "Epoch 29 Batch 500 Loss 0.0494\n",
      "Epoch 29 Batch 600 Loss 0.0359\n",
      "Epoch 29 Batch 700 Loss 0.0365\n",
      "Epoch 29 Batch 800 Loss 0.0364\n",
      "Epoch 29 Batch 900 Loss 0.0393\n",
      "Epoch 29 Batch 1000 Loss 0.0387\n",
      "Epoch 29 Batch 1100 Loss 0.0208\n",
      "Epoch 29 Batch 1200 Loss 0.0511\n",
      "Epoch 29 Loss 0.0430\n",
      "Time taken for 1 epoch 108.50730085372925 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.0452\n",
      "Epoch 30 Batch 100 Loss 0.0243\n",
      "Epoch 30 Batch 200 Loss 0.0226\n",
      "Epoch 30 Batch 300 Loss 0.0475\n",
      "Epoch 30 Batch 400 Loss 0.0273\n",
      "Epoch 30 Batch 500 Loss 0.0574\n",
      "Epoch 30 Batch 600 Loss 0.0356\n",
      "Epoch 30 Batch 700 Loss 0.0474\n",
      "Epoch 30 Batch 800 Loss 0.0434\n",
      "Epoch 30 Batch 900 Loss 0.0445\n",
      "Epoch 30 Batch 1000 Loss 0.0399\n",
      "Epoch 30 Batch 1100 Loss 0.0426\n",
      "Epoch 30 Batch 1200 Loss 0.0947\n",
      "Epoch 30 Loss 0.0425\n",
      "Time taken for 1 epoch 110.91969728469849 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.0280\n",
      "Epoch 31 Batch 100 Loss 0.0472\n",
      "Epoch 31 Batch 200 Loss 0.0399\n",
      "Epoch 31 Batch 300 Loss 0.0462\n",
      "Epoch 31 Batch 400 Loss 0.0319\n",
      "Epoch 31 Batch 500 Loss 0.0201\n",
      "Epoch 31 Batch 600 Loss 0.0515\n",
      "Epoch 31 Batch 700 Loss 0.0421\n",
      "Epoch 31 Batch 800 Loss 0.0370\n",
      "Epoch 31 Batch 900 Loss 0.0344\n",
      "Epoch 31 Batch 1000 Loss 0.0618\n",
      "Epoch 31 Batch 1100 Loss 0.0241\n",
      "Epoch 31 Batch 1200 Loss 0.0523\n",
      "Epoch 31 Loss 0.0418\n",
      "Time taken for 1 epoch 108.49033308029175 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.0335\n",
      "Epoch 32 Batch 100 Loss 0.0350\n",
      "Epoch 32 Batch 200 Loss 0.0485\n",
      "Epoch 32 Batch 300 Loss 0.0346\n",
      "Epoch 32 Batch 400 Loss 0.0465\n",
      "Epoch 32 Batch 500 Loss 0.0306\n",
      "Epoch 32 Batch 600 Loss 0.0504\n",
      "Epoch 32 Batch 700 Loss 0.0368\n",
      "Epoch 32 Batch 800 Loss 0.0513\n",
      "Epoch 32 Batch 900 Loss 0.0516\n",
      "Epoch 32 Batch 1000 Loss 0.0424\n",
      "Epoch 32 Batch 1100 Loss 0.0629\n",
      "Epoch 32 Batch 1200 Loss 0.0635\n",
      "Epoch 32 Loss 0.0412\n",
      "Time taken for 1 epoch 110.89130878448486 sec\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.0301\n",
      "Epoch 33 Batch 100 Loss 0.0279\n",
      "Epoch 33 Batch 200 Loss 0.0189\n",
      "Epoch 33 Batch 300 Loss 0.0442\n",
      "Epoch 33 Batch 400 Loss 0.0310\n",
      "Epoch 33 Batch 500 Loss 0.0486\n",
      "Epoch 33 Batch 600 Loss 0.0444\n",
      "Epoch 33 Batch 700 Loss 0.0421\n",
      "Epoch 33 Batch 800 Loss 0.0268\n",
      "Epoch 33 Batch 900 Loss 0.0338\n",
      "Epoch 33 Batch 1000 Loss 0.0354\n",
      "Epoch 33 Batch 1100 Loss 0.0839\n",
      "Epoch 33 Batch 1200 Loss 0.0416\n",
      "Epoch 33 Loss 0.0407\n",
      "Time taken for 1 epoch 108.4907431602478 sec\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.0343\n",
      "Epoch 34 Batch 100 Loss 0.0315\n",
      "Epoch 34 Batch 200 Loss 0.0288\n",
      "Epoch 34 Batch 300 Loss 0.0251\n",
      "Epoch 34 Batch 400 Loss 0.0395\n",
      "Epoch 34 Batch 500 Loss 0.0421\n",
      "Epoch 34 Batch 600 Loss 0.0381\n",
      "Epoch 34 Batch 700 Loss 0.0426\n",
      "Epoch 34 Batch 800 Loss 0.0493\n",
      "Epoch 34 Batch 900 Loss 0.0676\n",
      "Epoch 34 Batch 1000 Loss 0.0395\n",
      "Epoch 34 Batch 1100 Loss 0.0415\n",
      "Epoch 34 Batch 1200 Loss 0.0398\n",
      "Epoch 34 Loss 0.0402\n",
      "Time taken for 1 epoch 111.03901791572571 sec\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.0432\n",
      "Epoch 35 Batch 100 Loss 0.0459\n",
      "Epoch 35 Batch 200 Loss 0.0284\n",
      "Epoch 35 Batch 300 Loss 0.0341\n",
      "Epoch 35 Batch 400 Loss 0.0306\n",
      "Epoch 35 Batch 500 Loss 0.0618\n",
      "Epoch 35 Batch 600 Loss 0.0654\n",
      "Epoch 35 Batch 700 Loss 0.0265\n",
      "Epoch 35 Batch 800 Loss 0.0603\n",
      "Epoch 35 Batch 900 Loss 0.0371\n",
      "Epoch 35 Batch 1000 Loss 0.0409\n",
      "Epoch 35 Batch 1100 Loss 0.0425\n",
      "Epoch 35 Batch 1200 Loss 0.0559\n",
      "Epoch 35 Loss 0.0398\n",
      "Time taken for 1 epoch 108.49951100349426 sec\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.0329\n",
      "Epoch 36 Batch 100 Loss 0.0441\n",
      "Epoch 36 Batch 200 Loss 0.0359\n",
      "Epoch 36 Batch 300 Loss 0.0383\n",
      "Epoch 36 Batch 400 Loss 0.0357\n",
      "Epoch 36 Batch 500 Loss 0.0593\n",
      "Epoch 36 Batch 600 Loss 0.0550\n",
      "Epoch 36 Batch 700 Loss 0.0294\n",
      "Epoch 36 Batch 800 Loss 0.0398\n",
      "Epoch 36 Batch 900 Loss 0.0412\n",
      "Epoch 36 Batch 1000 Loss 0.0396\n",
      "Epoch 36 Batch 1100 Loss 0.0550\n",
      "Epoch 36 Batch 1200 Loss 0.0588\n",
      "Epoch 36 Loss 0.0401\n",
      "Time taken for 1 epoch 110.78535032272339 sec\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.0394\n",
      "Epoch 37 Batch 100 Loss 0.0193\n",
      "Epoch 37 Batch 200 Loss 0.0296\n",
      "Epoch 37 Batch 300 Loss 0.0187\n",
      "Epoch 37 Batch 400 Loss 0.0387\n",
      "Epoch 37 Batch 500 Loss 0.0481\n",
      "Epoch 37 Batch 600 Loss 0.0318\n",
      "Epoch 37 Batch 700 Loss 0.0538\n",
      "Epoch 37 Batch 800 Loss 0.0291\n",
      "Epoch 37 Batch 900 Loss 0.0491\n",
      "Epoch 37 Batch 1000 Loss 0.0212\n",
      "Epoch 37 Batch 1100 Loss 0.0438\n",
      "Epoch 37 Batch 1200 Loss 0.0581\n",
      "Epoch 37 Loss 0.0390\n",
      "Time taken for 1 epoch 108.5079894065857 sec\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.0294\n",
      "Epoch 38 Batch 100 Loss 0.0356\n",
      "Epoch 38 Batch 200 Loss 0.0441\n",
      "Epoch 38 Batch 300 Loss 0.0333\n",
      "Epoch 38 Batch 400 Loss 0.0244\n",
      "Epoch 38 Batch 500 Loss 0.0517\n",
      "Epoch 38 Batch 600 Loss 0.0193\n",
      "Epoch 38 Batch 700 Loss 0.0470\n",
      "Epoch 38 Batch 800 Loss 0.0178\n",
      "Epoch 38 Batch 900 Loss 0.0276\n",
      "Epoch 38 Batch 1000 Loss 0.0527\n",
      "Epoch 38 Batch 1100 Loss 0.0460\n",
      "Epoch 38 Batch 1200 Loss 0.0689\n",
      "Epoch 38 Loss 0.0382\n",
      "Time taken for 1 epoch 111.12361121177673 sec\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.0210\n",
      "Epoch 39 Batch 100 Loss 0.0173\n",
      "Epoch 39 Batch 200 Loss 0.0300\n",
      "Epoch 39 Batch 300 Loss 0.0191\n",
      "Epoch 39 Batch 400 Loss 0.0275\n",
      "Epoch 39 Batch 500 Loss 0.0348\n",
      "Epoch 39 Batch 600 Loss 0.0498\n",
      "Epoch 39 Batch 700 Loss 0.0465\n",
      "Epoch 39 Batch 800 Loss 0.0251\n",
      "Epoch 39 Batch 900 Loss 0.0342\n",
      "Epoch 39 Batch 1000 Loss 0.0430\n",
      "Epoch 39 Batch 1100 Loss 0.0449\n",
      "Epoch 39 Batch 1200 Loss 0.0359\n",
      "Epoch 39 Loss 0.0382\n",
      "Time taken for 1 epoch 108.49563360214233 sec\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.0461\n",
      "Epoch 40 Batch 100 Loss 0.0197\n",
      "Epoch 40 Batch 200 Loss 0.0495\n",
      "Epoch 40 Batch 300 Loss 0.0413\n",
      "Epoch 40 Batch 400 Loss 0.0511\n",
      "Epoch 40 Batch 500 Loss 0.0333\n",
      "Epoch 40 Batch 600 Loss 0.0349\n",
      "Epoch 40 Batch 700 Loss 0.0295\n",
      "Epoch 40 Batch 800 Loss 0.0487\n",
      "Epoch 40 Batch 900 Loss 0.0549\n",
      "Epoch 40 Batch 1000 Loss 0.0376\n",
      "Epoch 40 Batch 1100 Loss 0.0661\n",
      "Epoch 40 Batch 1200 Loss 0.0439\n",
      "Epoch 40 Loss 0.0387\n",
      "Time taken for 1 epoch 111.02465534210205 sec\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.0218\n",
      "Epoch 41 Batch 100 Loss 0.0201\n",
      "Epoch 41 Batch 200 Loss 0.0213\n",
      "Epoch 41 Batch 300 Loss 0.0359\n",
      "Epoch 41 Batch 400 Loss 0.0306\n",
      "Epoch 41 Batch 500 Loss 0.0427\n",
      "Epoch 41 Batch 600 Loss 0.0311\n",
      "Epoch 41 Batch 700 Loss 0.0522\n",
      "Epoch 41 Batch 800 Loss 0.0523\n",
      "Epoch 41 Batch 900 Loss 0.0671\n",
      "Epoch 41 Batch 1000 Loss 0.0314\n",
      "Epoch 41 Batch 1100 Loss 0.0493\n",
      "Epoch 41 Batch 1200 Loss 0.0294\n",
      "Epoch 41 Loss 0.0378\n",
      "Time taken for 1 epoch 108.48320865631104 sec\n",
      "\n",
      "Epoch 42 Batch 0 Loss 0.0271\n",
      "Epoch 42 Batch 100 Loss 0.0234\n",
      "Epoch 42 Batch 200 Loss 0.0300\n",
      "Epoch 42 Batch 300 Loss 0.0181\n",
      "Epoch 42 Batch 400 Loss 0.0556\n",
      "Epoch 42 Batch 500 Loss 0.0226\n",
      "Epoch 42 Batch 600 Loss 0.0331\n",
      "Epoch 42 Batch 700 Loss 0.0410\n",
      "Epoch 42 Batch 800 Loss 0.0483\n",
      "Epoch 42 Batch 900 Loss 0.0337\n",
      "Epoch 42 Batch 1000 Loss 0.0309\n",
      "Epoch 42 Batch 1100 Loss 0.0395\n",
      "Epoch 42 Batch 1200 Loss 0.0351\n",
      "Epoch 42 Loss 0.0373\n",
      "Time taken for 1 epoch 110.95476150512695 sec\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.0262\n",
      "Epoch 43 Batch 100 Loss 0.0254\n",
      "Epoch 43 Batch 200 Loss 0.0228\n",
      "Epoch 43 Batch 300 Loss 0.0292\n",
      "Epoch 43 Batch 400 Loss 0.0376\n",
      "Epoch 43 Batch 500 Loss 0.0438\n",
      "Epoch 43 Batch 600 Loss 0.0677\n",
      "Epoch 43 Batch 700 Loss 0.0440\n",
      "Epoch 43 Batch 800 Loss 0.0277\n",
      "Epoch 43 Batch 900 Loss 0.0331\n",
      "Epoch 43 Batch 1000 Loss 0.0520\n",
      "Epoch 43 Batch 1100 Loss 0.0473\n",
      "Epoch 43 Batch 1200 Loss 0.0428\n",
      "Epoch 43 Loss 0.0373\n",
      "Time taken for 1 epoch 108.48960638046265 sec\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.0230\n",
      "Epoch 44 Batch 100 Loss 0.0465\n",
      "Epoch 44 Batch 200 Loss 0.0314\n",
      "Epoch 44 Batch 300 Loss 0.0353\n",
      "Epoch 44 Batch 400 Loss 0.0263\n",
      "Epoch 44 Batch 500 Loss 0.0612\n",
      "Epoch 44 Batch 600 Loss 0.0359\n",
      "Epoch 44 Batch 700 Loss 0.0325\n",
      "Epoch 44 Batch 800 Loss 0.0303\n",
      "Epoch 44 Batch 900 Loss 0.0362\n",
      "Epoch 44 Batch 1000 Loss 0.0367\n",
      "Epoch 44 Batch 1100 Loss 0.0398\n",
      "Epoch 44 Batch 1200 Loss 0.0454\n",
      "Epoch 44 Loss 0.0370\n",
      "Time taken for 1 epoch 111.08355569839478 sec\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.0256\n",
      "Epoch 45 Batch 100 Loss 0.0262\n",
      "Epoch 45 Batch 200 Loss 0.0583\n",
      "Epoch 45 Batch 300 Loss 0.0328\n",
      "Epoch 45 Batch 400 Loss 0.0223\n",
      "Epoch 45 Batch 500 Loss 0.0222\n",
      "Epoch 45 Batch 600 Loss 0.0500\n",
      "Epoch 45 Batch 700 Loss 0.0367\n",
      "Epoch 45 Batch 800 Loss 0.0506\n",
      "Epoch 45 Batch 900 Loss 0.0295\n",
      "Epoch 45 Batch 1000 Loss 0.0419\n",
      "Epoch 45 Batch 1100 Loss 0.0329\n",
      "Epoch 45 Batch 1200 Loss 0.0231\n",
      "Epoch 45 Loss 0.0359\n",
      "Time taken for 1 epoch 108.49850463867188 sec\n",
      "\n",
      "Epoch 46 Batch 0 Loss 0.0228\n",
      "Epoch 46 Batch 100 Loss 0.0362\n",
      "Epoch 46 Batch 200 Loss 0.0107\n",
      "Epoch 46 Batch 300 Loss 0.0227\n",
      "Epoch 46 Batch 400 Loss 0.0491\n",
      "Epoch 46 Batch 500 Loss 0.0137\n",
      "Epoch 46 Batch 600 Loss 0.0384\n",
      "Epoch 46 Batch 700 Loss 0.0324\n",
      "Epoch 46 Batch 800 Loss 0.0289\n",
      "Epoch 46 Batch 900 Loss 0.0472\n",
      "Epoch 46 Batch 1000 Loss 0.0408\n",
      "Epoch 46 Batch 1100 Loss 0.0468\n",
      "Epoch 46 Batch 1200 Loss 0.0527\n",
      "Epoch 46 Loss 0.0359\n",
      "Time taken for 1 epoch 110.96068406105042 sec\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.0320\n",
      "Epoch 47 Batch 100 Loss 0.0139\n",
      "Epoch 47 Batch 200 Loss 0.0156\n",
      "Epoch 47 Batch 300 Loss 0.0614\n",
      "Epoch 47 Batch 400 Loss 0.0415\n",
      "Epoch 47 Batch 500 Loss 0.0444\n",
      "Epoch 47 Batch 600 Loss 0.0565\n",
      "Epoch 47 Batch 700 Loss 0.0313\n",
      "Epoch 47 Batch 800 Loss 0.0327\n",
      "Epoch 47 Batch 900 Loss 0.0541\n",
      "Epoch 47 Batch 1000 Loss 0.0288\n",
      "Epoch 47 Batch 1100 Loss 0.0359\n",
      "Epoch 47 Batch 1200 Loss 0.0583\n",
      "Epoch 47 Loss 0.0363\n",
      "Time taken for 1 epoch 108.49126243591309 sec\n",
      "\n",
      "Epoch 48 Batch 0 Loss 0.0280\n",
      "Epoch 48 Batch 100 Loss 0.0299\n",
      "Epoch 48 Batch 200 Loss 0.0275\n",
      "Epoch 48 Batch 300 Loss 0.0473\n",
      "Epoch 48 Batch 400 Loss 0.0278\n",
      "Epoch 48 Batch 500 Loss 0.0323\n",
      "Epoch 48 Batch 600 Loss 0.0432\n",
      "Epoch 48 Batch 700 Loss 0.0523\n",
      "Epoch 48 Batch 800 Loss 0.0664\n",
      "Epoch 48 Batch 900 Loss 0.0430\n",
      "Epoch 48 Batch 1000 Loss 0.0468\n",
      "Epoch 48 Batch 1100 Loss 0.0422\n",
      "Epoch 48 Batch 1200 Loss 0.0403\n",
      "Epoch 48 Loss 0.0354\n",
      "Time taken for 1 epoch 111.06852626800537 sec\n",
      "\n",
      "Epoch 49 Batch 0 Loss 0.0354\n",
      "Epoch 49 Batch 100 Loss 0.0153\n",
      "Epoch 49 Batch 200 Loss 0.0140\n",
      "Epoch 49 Batch 300 Loss 0.0324\n",
      "Epoch 49 Batch 400 Loss 0.0339\n",
      "Epoch 49 Batch 500 Loss 0.0300\n",
      "Epoch 49 Batch 600 Loss 0.0305\n",
      "Epoch 49 Batch 700 Loss 0.0414\n",
      "Epoch 49 Batch 800 Loss 0.0255\n",
      "Epoch 49 Batch 900 Loss 0.0284\n",
      "Epoch 49 Batch 1000 Loss 0.0429\n",
      "Epoch 49 Batch 1100 Loss 0.0350\n",
      "Epoch 49 Batch 1200 Loss 0.0736\n",
      "Epoch 49 Loss 0.0352\n",
      "Time taken for 1 epoch 108.50100064277649 sec\n",
      "\n",
      "Epoch 50 Batch 0 Loss 0.0222\n",
      "Epoch 50 Batch 100 Loss 0.0296\n",
      "Epoch 50 Batch 200 Loss 0.0275\n",
      "Epoch 50 Batch 300 Loss 0.0313\n",
      "Epoch 50 Batch 400 Loss 0.0287\n",
      "Epoch 50 Batch 500 Loss 0.0431\n",
      "Epoch 50 Batch 600 Loss 0.0322\n",
      "Epoch 50 Batch 700 Loss 0.0421\n",
      "Epoch 50 Batch 800 Loss 0.0436\n",
      "Epoch 50 Batch 900 Loss 0.0364\n",
      "Epoch 50 Batch 1000 Loss 0.0412\n",
      "Epoch 50 Batch 1100 Loss 0.0421\n",
      "Epoch 50 Batch 1200 Loss 0.0290\n",
      "Epoch 50 Loss 0.0354\n",
      "Time taken for 1 epoch 111.29138541221619 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!g2.mig\n",
    "EPOCHS = 50\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    # Инициализируем входное скрытое состояние (из нулей) размера (батч, кол-во рекуррентных ячеек)\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        # Делаем шаг обучения\n",
    "        batch_loss = train_step_att(inp, targ, enc_hidden)\n",
    "        # Считаем ошибку\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                       batch,\n",
    "                                                       batch_loss.numpy()))\n",
    "    # Сохраняем модель каждые 2 эпохи\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "of01fwxkufgllpb0i4rsjh",
    "execution_id": "577d82f2-97f2-4a09-9a91-4c5d9d1a3fc9",
    "id": "s4i9Xyt1wXR5"
   },
   "source": [
    "### Перевод"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "cellId": "43zaqm7hjgfbs9bx4bokij",
    "execution_id": "98404536-2051-436f-82f1-e54ef147522c",
    "id": "5RCG1N03wZUC"
   },
   "outputs": [],
   "source": [
    "#!g2.mig\n",
    "def evaluate_att(sentence):\n",
    "    # Строим матрицу внимания из нулей размера (макс длина таргета, макс длина входа)  \n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    # Препоцессим предложение\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    # Разбиваем предложение по пробелам и составляем список индексов каждого слова\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    # Добиваем inputs нулями справа до максимальной длины входного текста\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "    # Преобразуем inputs в тензор\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "    # Инициализируем входной хидден из нулей размера (1, units)\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    # Подаем inputs и hidden в encoder\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    # Инициализируем входной хидден декодера -- выходной хидден энкодера\n",
    "    dec_hidden = enc_hidden\n",
    "    # Вход декодера -- список [индекс start] размера(1,1)  \n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        # Получаем выход декодера\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "\n",
    "        # Сохраняем веса внимания, чтобы позже визуализировать\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "        # Заканчиваем на токене end\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "    # Предсказанный predicted ID подаем обратно в декодер (размер (1,1))\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "cellId": "77sg1gw923davynhj8bd",
    "execution_id": "4aa020bf-411e-49b1-94c0-6821541b9850",
    "id": "JnHZmvrIwoIT"
   },
   "outputs": [],
   "source": [
    "#!g2.mig\n",
    "# Визуализация весов внимания\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "cellId": "zej5xqtichpydeu82bkbfq",
    "execution_id": "b24ec39b-4e1a-464f-9d67-ee0e65e4b961",
    "id": "-Eskl3Wiwr2T"
   },
   "outputs": [],
   "source": [
    "#!g2.mig\n",
    "def translate_att(sentence):\n",
    "    result, sentence, attention_plot = evaluate_att(sentence)\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "cellId": "twpk88gvypoz24hrrb09q",
    "execution_id": "17999ada-cef5-4eb8-9d8b-d27bef6fea0a",
    "id": "LKPTdh7Gwvue"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f3d682de100>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.mig\n",
    "# Восстанавливаем последнюю контрольную точку\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "cellId": "bc6taxmpzi6ontzmr4chl",
    "execution_id": "ab50a357-210f-46e0-8ad0-874fdbb41f38",
    "id": "hxlaYE2Sw8tI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> хорошая погода <end>\n",
      "Predicted translation: it's good weekend . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-51-5f13abce0f10>:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
      "<ipython-input-51-5f13abce0f10>:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g2.mig\n",
    "translate_att('Хорошая погода')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "cellId": "5avkkwski54u2zsl07qjg",
    "id": "gbD4FLVExJTz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 16:30:25.135736: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-20 16:30:27.736502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2629 MB memory:  -> device: 0, name: GRID A100X-1-5C MIG 1g.5gb, pci bus id: 0000:8c:00.0, compute capability: 8.0\n",
      "2023-02-20 16:30:47.970425: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8101\n",
      "2023-02-20 16:30:51.952032: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> я иду в школу <end>\n",
      "Predicted translation: i'm going to school . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-51-5f13abce0f10>:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "<ipython-input-51-5f13abce0f10>:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g2.mig\n",
    "translate_att('Я иду в школу')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "cellId": "s7ang5n53jnthegkx354",
    "id": "IXFgpKUOxJTz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> мама приготовила вкусный обед для нашей большой семьи <end>\n",
      "Predicted translation: my we've got fun of time . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-51-5f13abce0f10>:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "<ipython-input-51-5f13abce0f10>:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g2.mig\n",
    "translate_att('Мама приготовила вкусный обед для нашей большой семьи')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "cellId": "uhen1zhn5mu32f2to5r5",
    "id": "ehEVe_DixJTz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> мне здесь нравится , но я очень скучаю по дому . там остались мама и папа . <end>\n",
      "Predicted translation: i'm not really safe there , but big of us very \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-51-5f13abce0f10>:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "<ipython-input-51-5f13abce0f10>:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g2.mig\n",
    "translate_att(u'Мне здесь нравится, но я очень скучаю по дому. Там остались мама и папа.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ezffz9pgng8o6e20xqz9",
    "execution_id": "b009c89b-2c2c-430b-9484-89db4b0312ab",
    "id": "9nfqIqQKxPQY"
   },
   "source": [
    "### Вывод:\n",
    "\n",
    "Перевод модели со вниманием также имеет неточности."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "6768b24d-6c2a-4a16-b621-1f5801ce29a1",
  "notebookPath": "10_Машинный_перевод/ДЗ_10.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

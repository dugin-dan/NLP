{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding word2vec fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача поиск похожих по эмбеддингам\n",
    "\n",
    "Скачиваем датасет (источник): положительные, отрицательные.\n",
    "\n",
    "или можно через ноутбук\n",
    "\n",
    "!wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv\n",
    "!wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv\n",
    "\n",
    "что надо сделать\n",
    "1. объединить в одну выборку\n",
    "2. на основе word2vec/fasttext/glove/слоя Embedding реализовать метод поиска ближайших твитов\n",
    "(на вход метода должен приходить запрос (какой-то твит, вопрос) и количество вариантов вывода к примеру 5-ть, ваш метод должен возвращать 5-ть ближайших твитов к этому запросу)\n",
    "3. Проверить насколько хорошо работают подходы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общая подготовка (импорт библиотек, модулей, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stop_words\n",
      "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
      "Building wheels for collected packages: stop-words\n",
      "  Building wheel for stop-words (setup.py): started\n",
      "  Building wheel for stop-words (setup.py): finished with status 'done'\n",
      "  Created wheel for stop-words: filename=stop_words-2018.7.23-py3-none-any.whl size=32920 sha256=f23a06106fd5ea560f4666b801ca3867e227d48d0add53a5ede8cb3d29d2fde7\n",
      "  Stored in directory: c:\\users\\данила\\appdata\\local\\pip\\cache\\wheels\\eb\\03\\0d\\3bd31c983789aeb0b4d5e2ca48590288d9db1586cf5f225062\n",
      "Successfully built stop-words\n",
      "Installing collected packages: stop-words\n",
      "Successfully installed stop-words-2018.7.23\n"
     ]
    }
   ],
   "source": [
    "!pip install stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting annoy\n",
      "  Downloading annoy-1.17.0.tar.gz (646 kB)\n",
      "Building wheels for collected packages: annoy\n",
      "  Building wheel for annoy (setup.py): started\n",
      "  Building wheel for annoy (setup.py): finished with status 'done'\n",
      "  Created wheel for annoy: filename=annoy-1.17.0-cp38-cp38-win_amd64.whl size=53013 sha256=35a649d41387c3e61bbea725ec10f6ca97c26809aade023dfb07476fe1bb5695\n",
      "  Stored in directory: c:\\users\\данила\\appdata\\local\\pip\\cache\\wheels\\77\\01\\de\\4421524f9997a25dfa7291121565d12ef514154945e80e907a\n",
      "Successfully built annoy\n",
      "Installing collected packages: annoy\n",
      "Successfully installed annoy-1.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from stop_words import get_stop_words\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "import annoy\n",
    "import numpy as np\n",
    "import gensim.models\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка и подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111918</th>\n",
       "      <td>Но не каждый хочет что то исправлять:( http://...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111919</th>\n",
       "      <td>скучаю так :-( только @taaannyaaa вправляет мо...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111920</th>\n",
       "      <td>Вот и в школу, в говно это идти уже надо(</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111921</th>\n",
       "      <td>RT @_Them__: @LisaBeroud Тауриэль, не грусти :...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111922</th>\n",
       "      <td>Такси везет меня на работу. Раздумываю приплат...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text     label\n",
       "111918  Но не каждый хочет что то исправлять:( http://...  negative\n",
       "111919  скучаю так :-( только @taaannyaaa вправляет мо...  negative\n",
       "111920          Вот и в школу, в говно это идти уже надо(  negative\n",
       "111921  RT @_Them__: @LisaBeroud Тауриэль, не грусти :...  negative\n",
       "111922  Такси везет меня на работу. Раздумываю приплат...  negative"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# считываем данные и заполняем общий датасет\n",
    "positive = pd.read_csv('positive.csv', sep=';', usecols=[3], names=['text'])\n",
    "positive['label'] = ['positive'] * len(positive)\n",
    "negative = pd.read_csv('negative.csv', sep=';', usecols=[3], names=['text'])\n",
    "negative['label'] = ['negative'] * len(negative)\n",
    "df = positive.append(negative)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#разбиваем данные на train и test\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.text, df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "morpher = MorphAnalyzer()\n",
    "sw = set(get_stop_words(\"ru\"))\n",
    "exclude = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция преобразования твита (лемматизация, удаление стоп-слов и пунктуации)\n",
    "def preprocess_txt(line):\n",
    "    spls = \"\".join(i for i in line.strip() if i not in exclude).split()\n",
    "    spls = [morpher.parse(i.lower())[0].normal_form for i in spls]\n",
    "    spls = [i for i in spls if i not in sw and i != \"\"]\n",
    "    return spls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Процедура подготовки данных к обучению\n",
    "assert True\n",
    "\n",
    "# Preprocess for models fitting\n",
    "tweet_tokens = []\n",
    "c = 0\n",
    "for text in x_train:\n",
    "    tweet_prep = preprocess_txt(text)\n",
    "    tweet_tokens.append(tweet_prep)\n",
    "    c += 1\n",
    "    if c > 100000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция поиска 5 ближайших твитов\n",
    "def get_response(tweet, index, model, index_map):\n",
    "    tweet_token = preprocess_txt(tweet)\n",
    "    vector = np.zeros(300)\n",
    "    norm = 0\n",
    "    for word in tweet_token:\n",
    "        if word in model:\n",
    "            vector += model[word]\n",
    "            norm += 1\n",
    "    if norm > 0:\n",
    "        vector = vector / norm\n",
    "    similar_tweet = index.get_nns_by_vector(vector, 5, include_distances=True)\n",
    "    \n",
    "    result = pd.DataFrame({'tweet': [index_map[i]  for i in similar_tweet[0]], \n",
    "                           'distance' : similar_tweet[1]})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Процедура поиска ближайших твитов\n",
    "def get_model_index(model):\n",
    "    model_index = annoy.AnnoyIndex(300 ,'angular') # поиск ближайших соседей\n",
    "\n",
    "    index_map = {}\n",
    "    counter = 0\n",
    "\n",
    "    for tweet in x_train:\n",
    "        n_model = 0\n",
    "        index_map[counter] = tweet\n",
    "        tweet_prep = preprocess_txt(tweet)\n",
    "        \n",
    "        vector_model = np.zeros(300) #вектор твита\n",
    "\n",
    "        for word in tweet_prep:\n",
    "            if word in model:\n",
    "                vector_model += model[word]\n",
    "                n_model += 1\n",
    "        if n_model > 0:\n",
    "            vector_model = vector_model / n_model #нормализуем вектор твита\n",
    "       \n",
    "        model_index.add_item(counter, vector_model) #добавляем элемент для индексации\n",
    "     \n",
    "        counter += 1\n",
    "\n",
    "        if counter > 100000:\n",
    "            break\n",
    "\n",
    "    model_index.build(10) #из индексов создаем лес из 10 деревьев\n",
    "    return model_index, index_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2wec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelW2V = Word2Vec(sentences=tweet_tokens, vector_size=300, window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_index, index_map = get_model_index(modelW2V.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@dailykiselev @pavelsheremet это сердечко из воздушного шарика )))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@meh_joey кстати  фотика не будет :с\\n:(:(:(:(...</td>\n",
       "      <td>0.024587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Сегодня мне снилось что я профессор,хожу по ка...</td>\n",
       "      <td>0.025040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@99Umka99 @UporotyhpieDean @v_vladman_v @Minut...</td>\n",
       "      <td>0.025135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @rockgirlirushka: Первый день зимы....начин...</td>\n",
       "      <td>0.026439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @barbi_risha: большой компанией кататься па...</td>\n",
       "      <td>0.026507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  distance\n",
       "0  @meh_joey кстати  фотика не будет :с\\n:(:(:(:(...  0.024587\n",
       "1  Сегодня мне снилось что я профессор,хожу по ка...  0.025040\n",
       "2  @99Umka99 @UporotyhpieDean @v_vladman_v @Minut...  0.025135\n",
       "3  RT @rockgirlirushka: Первый день зимы....начин...  0.026439\n",
       "4  RT @barbi_risha: большой компанией кататься па...  0.026507"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_test.values[0])\n",
    "get_response(x_test.values[0], w2v_index, modelW2V.wv, index_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelFT = FastText(sentences=tweet_tokens, vector_size=300, min_count=1, window=5, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_index, index_map = get_model_index(modelFT.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@dailykiselev @pavelsheremet это сердечко из воздушного шарика )))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brothers: A tale of two sons классная, только ...</td>\n",
       "      <td>0.015430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@lentaruofficial Это же Святая София Константи...</td>\n",
       "      <td>0.017493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#HappyBirthdayTreCool\\n#HappyBirthdayTreCool\\n...</td>\n",
       "      <td>0.017877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@circumstances_ @ValeriaNefedowa типичная реак...</td>\n",
       "      <td>0.019090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Vafudr бясплатная медыцына гарантавана для ўс...</td>\n",
       "      <td>0.021530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  distance\n",
       "0  Brothers: A tale of two sons классная, только ...  0.015430\n",
       "1  @lentaruofficial Это же Святая София Константи...  0.017493\n",
       "2  #HappyBirthdayTreCool\\n#HappyBirthdayTreCool\\n...  0.017877\n",
       "3  @circumstances_ @ValeriaNefedowa типичная реак...  0.019090\n",
       "4  @Vafudr бясплатная медыцына гарантавана для ўс...  0.021530"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_test.values[0])\n",
    "get_response(x_test.values[0], ft_index, modelFT.wv, index_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding слой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset = tf.data.Dataset.from_tensor_slices([x for l in tweet_tokens for x in l]) #список всех токенов из x_train\n",
    "#создание слоя словаря\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=100000,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=1)\n",
    "#создаем словарь\n",
    "vectorize_layer.adapt(text_dataset)\n",
    "\n",
    "#создаем модель\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.Input(shape=(1,), dtype=tf.string)) #на вход подается строка\n",
    "model.add(vectorize_layer)  #получаем тензор (1, 100), содержащий для каждого слова индексы из словаря \n",
    "model.add(Embedding(100000, 300, input_length=100, mask_zero=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@dailykiselev @pavelsheremet это сердечко из воздушного шарика )))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-4.25471552e-02,  2.71144174e-02, -2.19220519e-02, -3.42767462e-02,\n",
       "         2.67246030e-02,  1.23402588e-02, -1.06560104e-02,  3.93012911e-03,\n",
       "         1.92700699e-03,  4.27275412e-02, -4.34834845e-02,  4.18946631e-02,\n",
       "        -8.29537958e-03,  3.84259261e-02,  3.03425901e-02,  4.26512994e-02,\n",
       "         9.12897661e-03,  3.29642333e-02,  2.08178051e-02, -6.90040737e-03,\n",
       "        -5.12353331e-03,  1.61911510e-02, -3.28320265e-02, -2.93199662e-02,\n",
       "        -3.34843174e-02,  2.55197771e-02, -1.38547309e-02, -5.51538542e-03,\n",
       "         2.35513337e-02,  3.92812230e-02, -3.14476639e-02,  2.77283899e-02,\n",
       "         4.73274253e-02,  4.69980575e-02,  2.18016393e-02, -7.86186382e-03,\n",
       "        -3.70436683e-02,  1.18833557e-02, -4.30220366e-03, -1.52326748e-03,\n",
       "         3.69067080e-02,  1.98737532e-03, -3.53524573e-02,  3.75496037e-02,\n",
       "         3.90128605e-02,  2.12968849e-02,  1.94350630e-03,  4.24803831e-02,\n",
       "        -3.24489474e-02,  2.20609829e-03,  8.69869068e-03,  8.98983330e-03,\n",
       "        -3.95660475e-03, -2.52689365e-02,  4.27207462e-02, -2.63139140e-02,\n",
       "         2.06233598e-02, -3.67519371e-02,  3.44432518e-03,  1.17435455e-02,\n",
       "         1.78124346e-02, -4.36461344e-02, -1.18262880e-02, -3.67443562e-02,\n",
       "         2.57682055e-04,  1.06740966e-02,  2.47277655e-02,  3.78782675e-03,\n",
       "         2.64929570e-02,  3.33357602e-04, -3.49888094e-02, -4.92402799e-02,\n",
       "         2.41666548e-02,  2.31244825e-02,  4.22929600e-03, -6.11864030e-04,\n",
       "         4.60033081e-02,  3.82147729e-04,  3.80571745e-02,  2.38840915e-02,\n",
       "        -2.38649603e-02,  1.26558430e-02,  2.96988599e-02, -2.55982280e-02,\n",
       "         4.37767766e-02,  1.10591576e-03,  3.86027731e-02, -2.09169630e-02,\n",
       "         4.86305468e-02, -2.36213561e-02,  2.51228474e-02,  2.66572572e-02,\n",
       "         4.24597599e-02,  1.68626644e-02,  7.13248178e-03, -3.45182195e-02,\n",
       "        -4.09322754e-02, -1.71527490e-02, -4.76505272e-02, -2.03243028e-02,\n",
       "        -3.62349264e-02,  1.38324536e-02, -1.41656883e-02, -1.63576826e-02,\n",
       "         1.80668719e-02, -1.82440504e-02, -4.89191078e-02,  3.60100344e-03,\n",
       "        -2.91603450e-02, -2.10520513e-02,  2.63792761e-02,  1.79551877e-02,\n",
       "         4.65657227e-02, -2.01854110e-02, -2.28501558e-02, -4.23669815e-04,\n",
       "        -1.06473081e-02, -4.49981354e-02,  3.31156328e-03,  2.44567730e-02,\n",
       "         3.03059556e-02, -3.89642827e-02,  3.76763828e-02,  4.03552316e-02,\n",
       "         1.38860084e-02,  4.78047989e-02, -1.41601674e-02,  1.71588771e-02,\n",
       "         4.16782014e-02, -1.04910359e-02, -4.80859987e-02, -4.33254130e-02,\n",
       "        -3.56840603e-02,  3.93921994e-02, -1.62281841e-03,  3.32618244e-02,\n",
       "        -4.51258570e-03, -1.06226429e-02,  1.25881284e-03, -2.82913931e-02,\n",
       "        -2.78261546e-02,  3.57768685e-03,  9.33412462e-03,  2.81548537e-02,\n",
       "         2.10691802e-02,  4.70957868e-02,  3.46510485e-03, -1.82005279e-02,\n",
       "         4.06580679e-02, -3.50497253e-02,  2.17796601e-02, -1.13373622e-02,\n",
       "        -2.11868640e-02,  2.17298530e-02, -3.76725309e-02, -2.86545157e-02,\n",
       "        -6.67832792e-04,  4.63024639e-02, -2.72222608e-03, -3.69153917e-04,\n",
       "        -3.15732881e-03,  4.16708477e-02, -4.90622409e-02,  1.75183304e-02,\n",
       "        -3.88018377e-02,  4.29603495e-02, -4.98410314e-03,  3.76809947e-02,\n",
       "        -2.04895493e-02, -4.61739078e-02,  6.64752722e-03, -3.21647525e-02,\n",
       "         2.31926776e-02,  7.72848725e-05,  2.07477249e-02,  3.40034254e-02,\n",
       "        -4.42282110e-03, -1.14194378e-02, -3.78985777e-02,  1.82671435e-02,\n",
       "         1.59227364e-02, -3.16236988e-02,  2.03664340e-02,  2.73895152e-02,\n",
       "         8.83167982e-03,  3.16088460e-02,  1.12694874e-02,  1.00686401e-03,\n",
       "         4.30526026e-02, -4.46922891e-02,  3.92050035e-02, -1.94739550e-04,\n",
       "         6.00484759e-03, -4.40450087e-02,  2.05912255e-02, -2.10058931e-02,\n",
       "        -2.79029366e-02, -3.20685729e-02,  4.54018749e-02, -1.49226896e-02,\n",
       "        -4.48703542e-02,  2.53880508e-02, -4.30478714e-02, -2.51591932e-02,\n",
       "        -4.97980975e-02,  4.08432744e-02, -4.65942025e-02, -2.53419522e-02,\n",
       "         4.90810387e-02,  4.74029779e-03,  3.70510481e-02, -2.78400909e-02,\n",
       "         3.82884406e-02,  4.11019959e-02, -2.54594330e-02,  4.72705625e-02,\n",
       "         1.21567361e-02,  3.42530049e-02, -3.95546667e-02,  4.83995415e-02,\n",
       "         2.52361633e-02, -4.92613204e-02, -4.39303517e-02,  1.80981867e-02,\n",
       "         2.42966451e-02,  3.47856395e-02, -2.77876854e-03,  3.97366285e-03,\n",
       "         2.62348987e-02,  1.54333003e-02, -1.07419491e-03, -4.00971882e-02,\n",
       "        -5.24377823e-03, -1.12352856e-02,  2.10637189e-02,  2.22706683e-02,\n",
       "        -3.89182791e-02, -4.11382914e-02, -4.82961535e-02, -4.20866124e-02,\n",
       "        -4.69096676e-02, -4.97730263e-02,  1.16021745e-02,  1.60931423e-03,\n",
       "        -4.86445539e-02, -4.87357378e-03,  1.25760473e-02,  2.78902054e-03,\n",
       "         4.66131903e-02,  1.20174773e-02,  1.85152404e-02, -2.43495107e-02,\n",
       "        -4.33072224e-02, -4.21027541e-02, -2.20195781e-02, -4.50594090e-02,\n",
       "         1.80569179e-02, -2.85876039e-02,  3.43749672e-03,  4.99892719e-02,\n",
       "        -1.99580435e-02,  1.17475912e-03, -2.70864498e-02, -4.33689244e-02,\n",
       "         9.55113024e-03, -1.30483508e-02, -1.94410812e-02, -1.97628867e-02,\n",
       "         4.50936593e-02, -3.79287377e-02,  4.80371751e-02, -4.41564433e-02,\n",
       "        -1.81591138e-02,  1.44456737e-02, -2.23977491e-03,  1.79133527e-02,\n",
       "        -3.05427909e-02, -1.88267827e-02,  4.13262285e-02, -2.44054943e-03,\n",
       "        -1.21245757e-02,  1.20184645e-02,  1.26314871e-02,  3.96371521e-02,\n",
       "         3.71642821e-02,  2.74254009e-03, -2.29723211e-02, -4.99883294e-02,\n",
       "        -3.39941531e-02, -3.57635505e-02, -3.17411311e-02,  2.33952440e-02,\n",
       "         8.26464966e-03,  3.36527713e-02, -3.94067988e-02, -2.06835996e-02,\n",
       "        -2.38536131e-02,  3.98250669e-03, -8.07235390e-03,  2.21762545e-02],\n",
       "       dtype=float32),\n",
       " (300,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_test.values[0])\n",
    "v1 = model.predict([x_test.values[0]])[0][0]\n",
    "v2 = model.predict([x_test.values[1]])[0][0]\n",
    "v1, v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Поиск ближайших твитов\n",
    "emb_index = annoy.AnnoyIndex(300 ,'angular') # поиск ближайших соседей\n",
    "\n",
    "emb_map = {}\n",
    "counter = 0\n",
    "\n",
    "for tweet in x_train:\n",
    "    emb_map[counter] = tweet\n",
    "    vector_emb = model.predict([tweet])[0][0]\n",
    "    emb_index.add_item(counter, vector_emb) # добавляем элемент для индексации\n",
    "    counter += 1\n",
    "        \n",
    "    if counter > 100000:\n",
    "        break\n",
    "\n",
    "emb_index.build(10) # из индексов создаем лес из 10 деревьев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_emb(model, emb_index, tweet, n_similar_tweets):\n",
    "    vector = model.predict([tweet])[0][0]\n",
    "\n",
    "    similar_tweet = emb_index.get_nns_by_vector(vector, n_similar_tweets, include_distances=True)\n",
    "    \n",
    "    result = pd.DataFrame({'tweet': [emb_map[i]  for i in similar_tweet[0]], \n",
    "                           'distance' : similar_tweet[1]\n",
    "                          })\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@dailykiselev @pavelsheremet это сердечко из воздушного шарика )))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@mint_muffin ахахха, ты это зряяя\\nты не предс...</td>\n",
       "      <td>1.290093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@satanishka666 согласна!\\nи да, только подумал...</td>\n",
       "      <td>1.290618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@satanishka666 дада)\\nну, нада постараться!\\nJ...</td>\n",
       "      <td>1.290618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@love__AndyBVB ничего((99((9((9((((9 жизнь пря...</td>\n",
       "      <td>1.298755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@SimbIblis зашибись.. Стагнировали бы еще нало...</td>\n",
       "      <td>1.323780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  distance\n",
       "0  @mint_muffin ахахха, ты это зряяя\\nты не предс...  1.290093\n",
       "1  @satanishka666 согласна!\\nи да, только подумал...  1.290618\n",
       "2  @satanishka666 дада)\\nну, нада постараться!\\nJ...  1.290618\n",
       "3  @love__AndyBVB ничего((99((9((9((((9 жизнь пря...  1.298755\n",
       "4  @SimbIblis зашибись.. Стагнировали бы еще нало...  1.323780"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_test.values[0])\n",
    "get_response_emb(model, emb_index, x_test.values[0], 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
